{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.random as random\n",
    "from torch.utils import data\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import quant_lib.Logit_MLP as mlp\n",
    "import quant_lib.figure as figure\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPLogitNet(bit_width=1):\n",
    "    return mlp.MLP(input_class=10, num_classes=1, bit_width=bit_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(features, labels, batch_size, is_train=True):\n",
    "    dataset = data.TensorDataset(features, labels)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate CUDA\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set hyperparameter\n",
    "bit_width = 1\n",
    "EPOCH = 25\n",
    "ANNEAL_EPOCH_AS = 5\n",
    "ANNEAL_EPOCH_PQ = 5\n",
    "pre_epoch = 0\n",
    "BATCH_SIZE = 50\n",
    "LR = 1\n",
    "# ASkewSGD\n",
    "DECAY_CONST = 0.88\n",
    "alpha = 0.1\n",
    "# ProxQuant\n",
    "reg_lambda = 5e-2\n",
    "\n",
    "# Generate training and testing dataset\n",
    "n_train = 5000\n",
    "n_test = 1000\n",
    "n = n_train + n_test\n",
    "w_star = (torch.rand(10) - 0.5).sign()\n",
    "X = torch.rand(n, 10)\n",
    "probs = 1.0 / (1 + torch.exp(-torch.mv(X, w_star)))\n",
    "Y = torch.bernoulli(probs)\n",
    "X_train, Y_train = X[:n_train], Y[:n_train]\n",
    "X_test, Y_test = X[n_train:], Y[n_train:]\n",
    "trainloader = load_array(X_train, Y_train, batch_size=BATCH_SIZE, is_train=True)\n",
    "testloader = load_array(X_test, Y_test, batch_size=BATCH_SIZE, is_train=False)\n",
    "# Define base net\n",
    "base_net = MLPLogitNet(bit_width=bit_width).to(device)\n",
    "# Define loss\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(project_name, opt_name, batch_size, architecture, dataset_name, lr, alpha=None, reg_lambda=None):\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=project_name,\n",
    "        name=opt_name,\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"batch_size\": batch_size,\n",
    "            \"architecture\": architecture,\n",
    "            \"dataset\": dataset_name,\n",
    "            \"lr\": lr,\n",
    "            \"alpha\": alpha, \n",
    "            \"reg_lambda\": reg_lambda,\n",
    "            \"bit_width\": base_net.bit_width\n",
    "        },\n",
    "    )\n",
    "    net = copy.deepcopy(base_net)\n",
    "    net.to(device)\n",
    "    weights = [p for name, p in net.named_parameters() if 'bias' not in name]\n",
    "    bias = [p for name, p in net.named_parameters() if 'bias' in name]\n",
    "    parameters = [{\"params\": weights, \"tag\": \"weights\"}, {\"params\": bias, \"tag\": \"bias\"}]\n",
    "    optimizer = optim.SGD(parameters, lr=lr)\n",
    "    return net, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.21.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lenovo\\Desktop\\CUHK\\Research-2025\\exp\\wandb\\run-20250711_035309-86ignwg9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/86ignwg9' target=\"_blank\">SGD</a></strong> to <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/86ignwg9' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/86ignwg9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "[Epoch:1, Iter:1] Loss: 0.713 | Acc: 46.000% \n",
      "[Epoch:1, Iter:2] Loss: 0.694 | Acc: 54.000% \n",
      "[Epoch:1, Iter:3] Loss: 0.684 | Acc: 56.000% \n",
      "[Epoch:1, Iter:4] Loss: 0.665 | Acc: 59.500% \n",
      "[Epoch:1, Iter:5] Loss: 0.649 | Acc: 61.600% \n",
      "[Epoch:1, Iter:6] Loss: 0.632 | Acc: 64.000% \n",
      "[Epoch:1, Iter:7] Loss: 0.624 | Acc: 65.143% \n",
      "[Epoch:1, Iter:8] Loss: 0.623 | Acc: 65.250% \n",
      "[Epoch:1, Iter:9] Loss: 0.623 | Acc: 65.111% \n",
      "[Epoch:1, Iter:10] Loss: 0.621 | Acc: 65.200% \n",
      "[Epoch:1, Iter:11] Loss: 0.617 | Acc: 65.636% \n",
      "[Epoch:1, Iter:12] Loss: 0.618 | Acc: 65.500% \n",
      "[Epoch:1, Iter:13] Loss: 0.616 | Acc: 66.000% \n",
      "[Epoch:1, Iter:14] Loss: 0.613 | Acc: 66.286% \n",
      "[Epoch:1, Iter:15] Loss: 0.614 | Acc: 66.133% \n",
      "[Epoch:1, Iter:16] Loss: 0.616 | Acc: 66.125% \n",
      "[Epoch:1, Iter:17] Loss: 0.617 | Acc: 66.000% \n",
      "[Epoch:1, Iter:18] Loss: 0.615 | Acc: 66.111% \n",
      "[Epoch:1, Iter:19] Loss: 0.610 | Acc: 66.526% \n",
      "[Epoch:1, Iter:20] Loss: 0.610 | Acc: 66.400% \n",
      "[Epoch:1, Iter:21] Loss: 0.609 | Acc: 66.857% \n",
      "[Epoch:1, Iter:22] Loss: 0.610 | Acc: 66.636% \n",
      "[Epoch:1, Iter:23] Loss: 0.612 | Acc: 66.609% \n",
      "[Epoch:1, Iter:24] Loss: 0.613 | Acc: 66.500% \n",
      "[Epoch:1, Iter:25] Loss: 0.612 | Acc: 66.640% \n",
      "[Epoch:1, Iter:26] Loss: 0.609 | Acc: 67.000% \n",
      "[Epoch:1, Iter:27] Loss: 0.611 | Acc: 66.815% \n",
      "[Epoch:1, Iter:28] Loss: 0.613 | Acc: 66.500% \n",
      "[Epoch:1, Iter:29] Loss: 0.610 | Acc: 67.103% \n",
      "[Epoch:1, Iter:30] Loss: 0.606 | Acc: 67.467% \n",
      "[Epoch:1, Iter:31] Loss: 0.612 | Acc: 67.226% \n",
      "[Epoch:1, Iter:32] Loss: 0.608 | Acc: 67.812% \n",
      "[Epoch:1, Iter:33] Loss: 0.606 | Acc: 67.939% \n",
      "[Epoch:1, Iter:34] Loss: 0.604 | Acc: 68.235% \n",
      "[Epoch:1, Iter:35] Loss: 0.604 | Acc: 68.229% \n",
      "[Epoch:1, Iter:36] Loss: 0.602 | Acc: 68.333% \n",
      "[Epoch:1, Iter:37] Loss: 0.599 | Acc: 68.649% \n",
      "[Epoch:1, Iter:38] Loss: 0.602 | Acc: 68.368% \n",
      "[Epoch:1, Iter:39] Loss: 0.601 | Acc: 68.308% \n",
      "[Epoch:1, Iter:40] Loss: 0.599 | Acc: 68.600% \n",
      "[Epoch:1, Iter:41] Loss: 0.598 | Acc: 68.585% \n",
      "[Epoch:1, Iter:42] Loss: 0.596 | Acc: 68.762% \n",
      "[Epoch:1, Iter:43] Loss: 0.595 | Acc: 68.791% \n",
      "[Epoch:1, Iter:44] Loss: 0.593 | Acc: 68.818% \n",
      "[Epoch:1, Iter:45] Loss: 0.592 | Acc: 68.889% \n",
      "[Epoch:1, Iter:46] Loss: 0.591 | Acc: 69.000% \n",
      "[Epoch:1, Iter:47] Loss: 0.589 | Acc: 69.021% \n",
      "[Epoch:1, Iter:48] Loss: 0.589 | Acc: 69.000% \n",
      "[Epoch:1, Iter:49] Loss: 0.587 | Acc: 69.265% \n",
      "[Epoch:1, Iter:50] Loss: 0.588 | Acc: 69.320% \n",
      "[Epoch:1, Iter:51] Loss: 0.586 | Acc: 69.490% \n",
      "[Epoch:1, Iter:52] Loss: 0.587 | Acc: 69.423% \n",
      "[Epoch:1, Iter:53] Loss: 0.586 | Acc: 69.509% \n",
      "[Epoch:1, Iter:54] Loss: 0.585 | Acc: 69.481% \n",
      "[Epoch:1, Iter:55] Loss: 0.584 | Acc: 69.527% \n",
      "[Epoch:1, Iter:56] Loss: 0.582 | Acc: 69.643% \n",
      "[Epoch:1, Iter:57] Loss: 0.581 | Acc: 69.719% \n",
      "[Epoch:1, Iter:58] Loss: 0.580 | Acc: 69.793% \n",
      "[Epoch:1, Iter:59] Loss: 0.581 | Acc: 69.763% \n",
      "[Epoch:1, Iter:60] Loss: 0.580 | Acc: 69.900% \n",
      "[Epoch:1, Iter:61] Loss: 0.582 | Acc: 69.836% \n",
      "[Epoch:1, Iter:62] Loss: 0.581 | Acc: 69.935% \n",
      "[Epoch:1, Iter:63] Loss: 0.579 | Acc: 70.159% \n",
      "[Epoch:1, Iter:64] Loss: 0.579 | Acc: 70.156% \n",
      "[Epoch:1, Iter:65] Loss: 0.579 | Acc: 70.154% \n",
      "[Epoch:1, Iter:66] Loss: 0.577 | Acc: 70.303% \n",
      "[Epoch:1, Iter:67] Loss: 0.577 | Acc: 70.299% \n",
      "[Epoch:1, Iter:68] Loss: 0.577 | Acc: 70.324% \n",
      "[Epoch:1, Iter:69] Loss: 0.577 | Acc: 70.261% \n",
      "[Epoch:1, Iter:70] Loss: 0.577 | Acc: 70.343% \n",
      "[Epoch:1, Iter:71] Loss: 0.577 | Acc: 70.366% \n",
      "[Epoch:1, Iter:72] Loss: 0.577 | Acc: 70.389% \n",
      "[Epoch:1, Iter:73] Loss: 0.577 | Acc: 70.301% \n",
      "[Epoch:1, Iter:74] Loss: 0.576 | Acc: 70.405% \n",
      "[Epoch:1, Iter:75] Loss: 0.575 | Acc: 70.427% \n",
      "[Epoch:1, Iter:76] Loss: 0.575 | Acc: 70.342% \n",
      "[Epoch:1, Iter:77] Loss: 0.575 | Acc: 70.260% \n",
      "[Epoch:1, Iter:78] Loss: 0.575 | Acc: 70.359% \n",
      "[Epoch:1, Iter:79] Loss: 0.574 | Acc: 70.430% \n",
      "[Epoch:1, Iter:80] Loss: 0.571 | Acc: 70.650% \n",
      "[Epoch:1, Iter:81] Loss: 0.571 | Acc: 70.593% \n",
      "[Epoch:1, Iter:82] Loss: 0.571 | Acc: 70.610% \n",
      "[Epoch:1, Iter:83] Loss: 0.570 | Acc: 70.651% \n",
      "[Epoch:1, Iter:84] Loss: 0.571 | Acc: 70.595% \n",
      "[Epoch:1, Iter:85] Loss: 0.570 | Acc: 70.612% \n",
      "[Epoch:1, Iter:86] Loss: 0.570 | Acc: 70.651% \n",
      "[Epoch:1, Iter:87] Loss: 0.569 | Acc: 70.759% \n",
      "[Epoch:1, Iter:88] Loss: 0.569 | Acc: 70.727% \n",
      "[Epoch:1, Iter:89] Loss: 0.571 | Acc: 70.607% \n",
      "[Epoch:1, Iter:90] Loss: 0.570 | Acc: 70.667% \n",
      "[Epoch:1, Iter:91] Loss: 0.569 | Acc: 70.769% \n",
      "[Epoch:1, Iter:92] Loss: 0.570 | Acc: 70.696% \n",
      "[Epoch:1, Iter:93] Loss: 0.570 | Acc: 70.731% \n",
      "[Epoch:1, Iter:94] Loss: 0.570 | Acc: 70.660% \n",
      "[Epoch:1, Iter:95] Loss: 0.570 | Acc: 70.653% \n",
      "[Epoch:1, Iter:96] Loss: 0.570 | Acc: 70.667% \n",
      "[Epoch:1, Iter:97] Loss: 0.570 | Acc: 70.763% \n",
      "[Epoch:1, Iter:98] Loss: 0.569 | Acc: 70.837% \n",
      "[Epoch:1, Iter:99] Loss: 0.570 | Acc: 70.828% \n",
      "[Epoch:1, Iter:100] Loss: 0.569 | Acc: 70.860% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.100%\n",
      "Training set's accuracy (after quantization) is: 72.980%\n",
      "Test set's accuracy (before quantization) is: 72.400%\n",
      "Test set's accuracy (after quantization) is: 73.800%\n",
      "Train Loss: 0.545 | Train Acc: 72.100% | Test Loss: 0.547 | Test Acc: 72.400% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.980% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.800% \n",
      "\n",
      "Epoch: 2\n",
      "[Epoch:2, Iter:101] Loss: 0.610 | Acc: 70.000% \n",
      "[Epoch:2, Iter:102] Loss: 0.544 | Acc: 72.000% \n",
      "[Epoch:2, Iter:103] Loss: 0.574 | Acc: 70.000% \n",
      "[Epoch:2, Iter:104] Loss: 0.565 | Acc: 73.000% \n",
      "[Epoch:2, Iter:105] Loss: 0.550 | Acc: 73.200% \n",
      "[Epoch:2, Iter:106] Loss: 0.550 | Acc: 72.667% \n",
      "[Epoch:2, Iter:107] Loss: 0.542 | Acc: 73.143% \n",
      "[Epoch:2, Iter:108] Loss: 0.552 | Acc: 73.000% \n",
      "[Epoch:2, Iter:109] Loss: 0.557 | Acc: 72.000% \n",
      "[Epoch:2, Iter:110] Loss: 0.539 | Acc: 73.600% \n",
      "[Epoch:2, Iter:111] Loss: 0.563 | Acc: 71.636% \n",
      "[Epoch:2, Iter:112] Loss: 0.564 | Acc: 71.333% \n",
      "[Epoch:2, Iter:113] Loss: 0.571 | Acc: 70.769% \n",
      "[Epoch:2, Iter:114] Loss: 0.576 | Acc: 70.714% \n",
      "[Epoch:2, Iter:115] Loss: 0.575 | Acc: 70.800% \n",
      "[Epoch:2, Iter:116] Loss: 0.574 | Acc: 70.750% \n",
      "[Epoch:2, Iter:117] Loss: 0.575 | Acc: 70.471% \n",
      "[Epoch:2, Iter:118] Loss: 0.575 | Acc: 70.556% \n",
      "[Epoch:2, Iter:119] Loss: 0.573 | Acc: 70.737% \n",
      "[Epoch:2, Iter:120] Loss: 0.575 | Acc: 70.000% \n",
      "[Epoch:2, Iter:121] Loss: 0.574 | Acc: 70.381% \n",
      "[Epoch:2, Iter:122] Loss: 0.573 | Acc: 70.273% \n",
      "[Epoch:2, Iter:123] Loss: 0.571 | Acc: 70.609% \n",
      "[Epoch:2, Iter:124] Loss: 0.566 | Acc: 71.083% \n",
      "[Epoch:2, Iter:125] Loss: 0.563 | Acc: 71.200% \n",
      "[Epoch:2, Iter:126] Loss: 0.560 | Acc: 71.385% \n",
      "[Epoch:2, Iter:127] Loss: 0.563 | Acc: 71.111% \n",
      "[Epoch:2, Iter:128] Loss: 0.567 | Acc: 70.714% \n",
      "[Epoch:2, Iter:129] Loss: 0.567 | Acc: 70.483% \n",
      "[Epoch:2, Iter:130] Loss: 0.565 | Acc: 70.800% \n",
      "[Epoch:2, Iter:131] Loss: 0.566 | Acc: 70.581% \n",
      "[Epoch:2, Iter:132] Loss: 0.565 | Acc: 70.750% \n",
      "[Epoch:2, Iter:133] Loss: 0.567 | Acc: 70.606% \n",
      "[Epoch:2, Iter:134] Loss: 0.566 | Acc: 70.353% \n",
      "[Epoch:2, Iter:135] Loss: 0.566 | Acc: 70.571% \n",
      "[Epoch:2, Iter:136] Loss: 0.564 | Acc: 70.444% \n",
      "[Epoch:2, Iter:137] Loss: 0.564 | Acc: 70.595% \n",
      "[Epoch:2, Iter:138] Loss: 0.563 | Acc: 70.737% \n",
      "[Epoch:2, Iter:139] Loss: 0.561 | Acc: 70.872% \n",
      "[Epoch:2, Iter:140] Loss: 0.560 | Acc: 70.850% \n",
      "[Epoch:2, Iter:141] Loss: 0.560 | Acc: 70.780% \n",
      "[Epoch:2, Iter:142] Loss: 0.560 | Acc: 70.762% \n",
      "[Epoch:2, Iter:143] Loss: 0.562 | Acc: 70.698% \n",
      "[Epoch:2, Iter:144] Loss: 0.561 | Acc: 70.727% \n",
      "[Epoch:2, Iter:145] Loss: 0.563 | Acc: 70.711% \n",
      "[Epoch:2, Iter:146] Loss: 0.561 | Acc: 70.870% \n",
      "[Epoch:2, Iter:147] Loss: 0.560 | Acc: 71.021% \n",
      "[Epoch:2, Iter:148] Loss: 0.560 | Acc: 71.000% \n",
      "[Epoch:2, Iter:149] Loss: 0.559 | Acc: 71.184% \n",
      "[Epoch:2, Iter:150] Loss: 0.556 | Acc: 71.400% \n",
      "[Epoch:2, Iter:151] Loss: 0.558 | Acc: 71.294% \n",
      "[Epoch:2, Iter:152] Loss: 0.557 | Acc: 71.308% \n",
      "[Epoch:2, Iter:153] Loss: 0.557 | Acc: 71.283% \n",
      "[Epoch:2, Iter:154] Loss: 0.557 | Acc: 71.185% \n",
      "[Epoch:2, Iter:155] Loss: 0.556 | Acc: 71.200% \n",
      "[Epoch:2, Iter:156] Loss: 0.555 | Acc: 71.357% \n",
      "[Epoch:2, Iter:157] Loss: 0.555 | Acc: 71.298% \n",
      "[Epoch:2, Iter:158] Loss: 0.555 | Acc: 71.310% \n",
      "[Epoch:2, Iter:159] Loss: 0.552 | Acc: 71.559% \n",
      "[Epoch:2, Iter:160] Loss: 0.551 | Acc: 71.633% \n",
      "[Epoch:2, Iter:161] Loss: 0.551 | Acc: 71.574% \n",
      "[Epoch:2, Iter:162] Loss: 0.552 | Acc: 71.516% \n",
      "[Epoch:2, Iter:163] Loss: 0.552 | Acc: 71.524% \n",
      "[Epoch:2, Iter:164] Loss: 0.552 | Acc: 71.562% \n",
      "[Epoch:2, Iter:165] Loss: 0.553 | Acc: 71.354% \n",
      "[Epoch:2, Iter:166] Loss: 0.552 | Acc: 71.333% \n",
      "[Epoch:2, Iter:167] Loss: 0.552 | Acc: 71.403% \n",
      "[Epoch:2, Iter:168] Loss: 0.550 | Acc: 71.618% \n",
      "[Epoch:2, Iter:169] Loss: 0.553 | Acc: 71.536% \n",
      "[Epoch:2, Iter:170] Loss: 0.552 | Acc: 71.543% \n",
      "[Epoch:2, Iter:171] Loss: 0.552 | Acc: 71.521% \n",
      "[Epoch:2, Iter:172] Loss: 0.553 | Acc: 71.556% \n",
      "[Epoch:2, Iter:173] Loss: 0.552 | Acc: 71.699% \n",
      "[Epoch:2, Iter:174] Loss: 0.552 | Acc: 71.703% \n",
      "[Epoch:2, Iter:175] Loss: 0.550 | Acc: 71.760% \n",
      "[Epoch:2, Iter:176] Loss: 0.551 | Acc: 71.763% \n",
      "[Epoch:2, Iter:177] Loss: 0.549 | Acc: 71.818% \n",
      "[Epoch:2, Iter:178] Loss: 0.550 | Acc: 71.744% \n",
      "[Epoch:2, Iter:179] Loss: 0.549 | Acc: 71.823% \n",
      "[Epoch:2, Iter:180] Loss: 0.550 | Acc: 71.775% \n",
      "[Epoch:2, Iter:181] Loss: 0.550 | Acc: 71.753% \n",
      "[Epoch:2, Iter:182] Loss: 0.550 | Acc: 71.707% \n",
      "[Epoch:2, Iter:183] Loss: 0.551 | Acc: 71.614% \n",
      "[Epoch:2, Iter:184] Loss: 0.550 | Acc: 71.690% \n",
      "[Epoch:2, Iter:185] Loss: 0.550 | Acc: 71.718% \n",
      "[Epoch:2, Iter:186] Loss: 0.550 | Acc: 71.698% \n",
      "[Epoch:2, Iter:187] Loss: 0.550 | Acc: 71.563% \n",
      "[Epoch:2, Iter:188] Loss: 0.549 | Acc: 71.705% \n",
      "[Epoch:2, Iter:189] Loss: 0.550 | Acc: 71.663% \n",
      "[Epoch:2, Iter:190] Loss: 0.549 | Acc: 71.778% \n",
      "[Epoch:2, Iter:191] Loss: 0.549 | Acc: 71.802% \n",
      "[Epoch:2, Iter:192] Loss: 0.548 | Acc: 71.891% \n",
      "[Epoch:2, Iter:193] Loss: 0.549 | Acc: 71.892% \n",
      "[Epoch:2, Iter:194] Loss: 0.548 | Acc: 71.979% \n",
      "[Epoch:2, Iter:195] Loss: 0.548 | Acc: 71.979% \n",
      "[Epoch:2, Iter:196] Loss: 0.548 | Acc: 71.958% \n",
      "[Epoch:2, Iter:197] Loss: 0.549 | Acc: 71.918% \n",
      "[Epoch:2, Iter:198] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:2, Iter:199] Loss: 0.549 | Acc: 71.960% \n",
      "[Epoch:2, Iter:200] Loss: 0.548 | Acc: 72.020% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.080%\n",
      "Training set's accuracy (after quantization) is: 72.800%\n",
      "Test set's accuracy (before quantization) is: 72.900%\n",
      "Test set's accuracy (after quantization) is: 73.300%\n",
      "Train Loss: 0.540 | Train Acc: 73.080% | Test Loss: 0.544 | Test Acc: 72.900% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.800% | Quantized Test Loss: 0.544 | Quantized Test Acc: 73.300% \n",
      "\n",
      "Epoch: 3\n",
      "[Epoch:3, Iter:201] Loss: 0.538 | Acc: 76.000% \n",
      "[Epoch:3, Iter:202] Loss: 0.480 | Acc: 79.000% \n",
      "[Epoch:3, Iter:203] Loss: 0.450 | Acc: 80.667% \n",
      "[Epoch:3, Iter:204] Loss: 0.493 | Acc: 77.500% \n",
      "[Epoch:3, Iter:205] Loss: 0.492 | Acc: 76.400% \n",
      "[Epoch:3, Iter:206] Loss: 0.503 | Acc: 76.000% \n",
      "[Epoch:3, Iter:207] Loss: 0.508 | Acc: 74.571% \n",
      "[Epoch:3, Iter:208] Loss: 0.512 | Acc: 75.000% \n",
      "[Epoch:3, Iter:209] Loss: 0.527 | Acc: 74.444% \n",
      "[Epoch:3, Iter:210] Loss: 0.529 | Acc: 74.400% \n",
      "[Epoch:3, Iter:211] Loss: 0.530 | Acc: 74.364% \n",
      "[Epoch:3, Iter:212] Loss: 0.537 | Acc: 73.833% \n",
      "[Epoch:3, Iter:213] Loss: 0.530 | Acc: 74.462% \n",
      "[Epoch:3, Iter:214] Loss: 0.528 | Acc: 74.571% \n",
      "[Epoch:3, Iter:215] Loss: 0.520 | Acc: 75.067% \n",
      "[Epoch:3, Iter:216] Loss: 0.531 | Acc: 74.250% \n",
      "[Epoch:3, Iter:217] Loss: 0.531 | Acc: 74.353% \n",
      "[Epoch:3, Iter:218] Loss: 0.531 | Acc: 74.111% \n",
      "[Epoch:3, Iter:219] Loss: 0.531 | Acc: 73.895% \n",
      "[Epoch:3, Iter:220] Loss: 0.535 | Acc: 73.300% \n",
      "[Epoch:3, Iter:221] Loss: 0.540 | Acc: 72.667% \n",
      "[Epoch:3, Iter:222] Loss: 0.537 | Acc: 73.000% \n",
      "[Epoch:3, Iter:223] Loss: 0.540 | Acc: 72.435% \n",
      "[Epoch:3, Iter:224] Loss: 0.539 | Acc: 72.500% \n",
      "[Epoch:3, Iter:225] Loss: 0.542 | Acc: 72.480% \n",
      "[Epoch:3, Iter:226] Loss: 0.541 | Acc: 72.615% \n",
      "[Epoch:3, Iter:227] Loss: 0.541 | Acc: 72.444% \n",
      "[Epoch:3, Iter:228] Loss: 0.539 | Acc: 72.929% \n",
      "[Epoch:3, Iter:229] Loss: 0.538 | Acc: 72.897% \n",
      "[Epoch:3, Iter:230] Loss: 0.543 | Acc: 72.467% \n",
      "[Epoch:3, Iter:231] Loss: 0.545 | Acc: 72.258% \n",
      "[Epoch:3, Iter:232] Loss: 0.550 | Acc: 71.812% \n",
      "[Epoch:3, Iter:233] Loss: 0.553 | Acc: 71.697% \n",
      "[Epoch:3, Iter:234] Loss: 0.554 | Acc: 71.471% \n",
      "[Epoch:3, Iter:235] Loss: 0.550 | Acc: 71.657% \n",
      "[Epoch:3, Iter:236] Loss: 0.550 | Acc: 71.722% \n",
      "[Epoch:3, Iter:237] Loss: 0.554 | Acc: 71.459% \n",
      "[Epoch:3, Iter:238] Loss: 0.552 | Acc: 71.632% \n",
      "[Epoch:3, Iter:239] Loss: 0.547 | Acc: 72.000% \n",
      "[Epoch:3, Iter:240] Loss: 0.546 | Acc: 72.100% \n",
      "[Epoch:3, Iter:241] Loss: 0.549 | Acc: 71.951% \n",
      "[Epoch:3, Iter:242] Loss: 0.547 | Acc: 72.190% \n",
      "[Epoch:3, Iter:243] Loss: 0.545 | Acc: 72.372% \n",
      "[Epoch:3, Iter:244] Loss: 0.545 | Acc: 72.364% \n",
      "[Epoch:3, Iter:245] Loss: 0.548 | Acc: 72.178% \n",
      "[Epoch:3, Iter:246] Loss: 0.549 | Acc: 72.043% \n",
      "[Epoch:3, Iter:247] Loss: 0.548 | Acc: 72.128% \n",
      "[Epoch:3, Iter:248] Loss: 0.545 | Acc: 72.500% \n",
      "[Epoch:3, Iter:249] Loss: 0.548 | Acc: 72.286% \n",
      "[Epoch:3, Iter:250] Loss: 0.546 | Acc: 72.560% \n",
      "[Epoch:3, Iter:251] Loss: 0.548 | Acc: 72.627% \n",
      "[Epoch:3, Iter:252] Loss: 0.550 | Acc: 72.577% \n",
      "[Epoch:3, Iter:253] Loss: 0.547 | Acc: 72.755% \n",
      "[Epoch:3, Iter:254] Loss: 0.545 | Acc: 72.889% \n",
      "[Epoch:3, Iter:255] Loss: 0.546 | Acc: 72.945% \n",
      "[Epoch:3, Iter:256] Loss: 0.548 | Acc: 72.821% \n",
      "[Epoch:3, Iter:257] Loss: 0.549 | Acc: 72.632% \n",
      "[Epoch:3, Iter:258] Loss: 0.550 | Acc: 72.690% \n",
      "[Epoch:3, Iter:259] Loss: 0.550 | Acc: 72.712% \n",
      "[Epoch:3, Iter:260] Loss: 0.551 | Acc: 72.667% \n",
      "[Epoch:3, Iter:261] Loss: 0.554 | Acc: 72.328% \n",
      "[Epoch:3, Iter:262] Loss: 0.556 | Acc: 72.194% \n",
      "[Epoch:3, Iter:263] Loss: 0.556 | Acc: 72.222% \n",
      "[Epoch:3, Iter:264] Loss: 0.554 | Acc: 72.344% \n",
      "[Epoch:3, Iter:265] Loss: 0.552 | Acc: 72.523% \n",
      "[Epoch:3, Iter:266] Loss: 0.553 | Acc: 72.455% \n",
      "[Epoch:3, Iter:267] Loss: 0.553 | Acc: 72.448% \n",
      "[Epoch:3, Iter:268] Loss: 0.553 | Acc: 72.500% \n",
      "[Epoch:3, Iter:269] Loss: 0.552 | Acc: 72.580% \n",
      "[Epoch:3, Iter:270] Loss: 0.554 | Acc: 72.486% \n",
      "[Epoch:3, Iter:271] Loss: 0.552 | Acc: 72.535% \n",
      "[Epoch:3, Iter:272] Loss: 0.553 | Acc: 72.500% \n",
      "[Epoch:3, Iter:273] Loss: 0.553 | Acc: 72.521% \n",
      "[Epoch:3, Iter:274] Loss: 0.553 | Acc: 72.486% \n",
      "[Epoch:3, Iter:275] Loss: 0.551 | Acc: 72.613% \n",
      "[Epoch:3, Iter:276] Loss: 0.551 | Acc: 72.658% \n",
      "[Epoch:3, Iter:277] Loss: 0.552 | Acc: 72.494% \n",
      "[Epoch:3, Iter:278] Loss: 0.553 | Acc: 72.359% \n",
      "[Epoch:3, Iter:279] Loss: 0.552 | Acc: 72.456% \n",
      "[Epoch:3, Iter:280] Loss: 0.553 | Acc: 72.375% \n",
      "[Epoch:3, Iter:281] Loss: 0.553 | Acc: 72.370% \n",
      "[Epoch:3, Iter:282] Loss: 0.553 | Acc: 72.341% \n",
      "[Epoch:3, Iter:283] Loss: 0.554 | Acc: 72.289% \n",
      "[Epoch:3, Iter:284] Loss: 0.554 | Acc: 72.286% \n",
      "[Epoch:3, Iter:285] Loss: 0.553 | Acc: 72.400% \n",
      "[Epoch:3, Iter:286] Loss: 0.554 | Acc: 72.326% \n",
      "[Epoch:3, Iter:287] Loss: 0.555 | Acc: 72.299% \n",
      "[Epoch:3, Iter:288] Loss: 0.555 | Acc: 72.318% \n",
      "[Epoch:3, Iter:289] Loss: 0.555 | Acc: 72.315% \n",
      "[Epoch:3, Iter:290] Loss: 0.554 | Acc: 72.333% \n",
      "[Epoch:3, Iter:291] Loss: 0.554 | Acc: 72.374% \n",
      "[Epoch:3, Iter:292] Loss: 0.553 | Acc: 72.478% \n",
      "[Epoch:3, Iter:293] Loss: 0.553 | Acc: 72.473% \n",
      "[Epoch:3, Iter:294] Loss: 0.551 | Acc: 72.553% \n",
      "[Epoch:3, Iter:295] Loss: 0.550 | Acc: 72.611% \n",
      "[Epoch:3, Iter:296] Loss: 0.550 | Acc: 72.646% \n",
      "[Epoch:3, Iter:297] Loss: 0.549 | Acc: 72.660% \n",
      "[Epoch:3, Iter:298] Loss: 0.549 | Acc: 72.633% \n",
      "[Epoch:3, Iter:299] Loss: 0.550 | Acc: 72.586% \n",
      "[Epoch:3, Iter:300] Loss: 0.549 | Acc: 72.660% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.100%\n",
      "Training set's accuracy (after quantization) is: 72.900%\n",
      "Test set's accuracy (before quantization) is: 73.200%\n",
      "Test set's accuracy (after quantization) is: 72.700%\n",
      "Train Loss: 0.540 | Train Acc: 73.100% | Test Loss: 0.543 | Test Acc: 73.200% \n",
      "Quantized Train Loss: 0.543 | Quantized Train Acc: 72.900% | Quantized Test Loss: 0.545 | Quantized Test Acc: 72.700% \n",
      "\n",
      "Epoch: 4\n",
      "[Epoch:4, Iter:301] Loss: 0.508 | Acc: 76.000% \n",
      "[Epoch:4, Iter:302] Loss: 0.491 | Acc: 78.000% \n",
      "[Epoch:4, Iter:303] Loss: 0.563 | Acc: 72.000% \n",
      "[Epoch:4, Iter:304] Loss: 0.568 | Acc: 71.000% \n",
      "[Epoch:4, Iter:305] Loss: 0.543 | Acc: 72.400% \n",
      "[Epoch:4, Iter:306] Loss: 0.542 | Acc: 72.333% \n",
      "[Epoch:4, Iter:307] Loss: 0.542 | Acc: 72.286% \n",
      "[Epoch:4, Iter:308] Loss: 0.534 | Acc: 73.250% \n",
      "[Epoch:4, Iter:309] Loss: 0.532 | Acc: 73.778% \n",
      "[Epoch:4, Iter:310] Loss: 0.541 | Acc: 73.800% \n",
      "[Epoch:4, Iter:311] Loss: 0.549 | Acc: 73.273% \n",
      "[Epoch:4, Iter:312] Loss: 0.545 | Acc: 73.667% \n",
      "[Epoch:4, Iter:313] Loss: 0.546 | Acc: 73.231% \n",
      "[Epoch:4, Iter:314] Loss: 0.545 | Acc: 73.000% \n",
      "[Epoch:4, Iter:315] Loss: 0.539 | Acc: 73.467% \n",
      "[Epoch:4, Iter:316] Loss: 0.540 | Acc: 73.500% \n",
      "[Epoch:4, Iter:317] Loss: 0.533 | Acc: 74.000% \n",
      "[Epoch:4, Iter:318] Loss: 0.538 | Acc: 73.667% \n",
      "[Epoch:4, Iter:319] Loss: 0.538 | Acc: 73.684% \n",
      "[Epoch:4, Iter:320] Loss: 0.538 | Acc: 74.100% \n",
      "[Epoch:4, Iter:321] Loss: 0.540 | Acc: 74.095% \n",
      "[Epoch:4, Iter:322] Loss: 0.539 | Acc: 74.091% \n",
      "[Epoch:4, Iter:323] Loss: 0.536 | Acc: 73.913% \n",
      "[Epoch:4, Iter:324] Loss: 0.532 | Acc: 74.083% \n",
      "[Epoch:4, Iter:325] Loss: 0.531 | Acc: 73.840% \n",
      "[Epoch:4, Iter:326] Loss: 0.528 | Acc: 74.000% \n",
      "[Epoch:4, Iter:327] Loss: 0.524 | Acc: 74.370% \n",
      "[Epoch:4, Iter:328] Loss: 0.526 | Acc: 74.071% \n",
      "[Epoch:4, Iter:329] Loss: 0.529 | Acc: 73.586% \n",
      "[Epoch:4, Iter:330] Loss: 0.532 | Acc: 73.267% \n",
      "[Epoch:4, Iter:331] Loss: 0.533 | Acc: 73.226% \n",
      "[Epoch:4, Iter:332] Loss: 0.532 | Acc: 73.312% \n",
      "[Epoch:4, Iter:333] Loss: 0.532 | Acc: 73.394% \n",
      "[Epoch:4, Iter:334] Loss: 0.533 | Acc: 73.412% \n",
      "[Epoch:4, Iter:335] Loss: 0.534 | Acc: 73.429% \n",
      "[Epoch:4, Iter:336] Loss: 0.536 | Acc: 73.389% \n",
      "[Epoch:4, Iter:337] Loss: 0.538 | Acc: 73.135% \n",
      "[Epoch:4, Iter:338] Loss: 0.536 | Acc: 73.105% \n",
      "[Epoch:4, Iter:339] Loss: 0.537 | Acc: 73.077% \n",
      "[Epoch:4, Iter:340] Loss: 0.538 | Acc: 73.150% \n",
      "[Epoch:4, Iter:341] Loss: 0.540 | Acc: 72.976% \n",
      "[Epoch:4, Iter:342] Loss: 0.540 | Acc: 73.000% \n",
      "[Epoch:4, Iter:343] Loss: 0.542 | Acc: 72.884% \n",
      "[Epoch:4, Iter:344] Loss: 0.542 | Acc: 72.818% \n",
      "[Epoch:4, Iter:345] Loss: 0.542 | Acc: 72.844% \n",
      "[Epoch:4, Iter:346] Loss: 0.541 | Acc: 72.913% \n",
      "[Epoch:4, Iter:347] Loss: 0.543 | Acc: 72.851% \n",
      "[Epoch:4, Iter:348] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:4, Iter:349] Loss: 0.544 | Acc: 73.020% \n",
      "[Epoch:4, Iter:350] Loss: 0.544 | Acc: 72.960% \n",
      "[Epoch:4, Iter:351] Loss: 0.543 | Acc: 72.902% \n",
      "[Epoch:4, Iter:352] Loss: 0.546 | Acc: 72.654% \n",
      "[Epoch:4, Iter:353] Loss: 0.546 | Acc: 72.679% \n",
      "[Epoch:4, Iter:354] Loss: 0.546 | Acc: 72.630% \n",
      "[Epoch:4, Iter:355] Loss: 0.547 | Acc: 72.691% \n",
      "[Epoch:4, Iter:356] Loss: 0.548 | Acc: 72.607% \n",
      "[Epoch:4, Iter:357] Loss: 0.548 | Acc: 72.596% \n",
      "[Epoch:4, Iter:358] Loss: 0.548 | Acc: 72.552% \n",
      "[Epoch:4, Iter:359] Loss: 0.548 | Acc: 72.542% \n",
      "[Epoch:4, Iter:360] Loss: 0.548 | Acc: 72.433% \n",
      "[Epoch:4, Iter:361] Loss: 0.547 | Acc: 72.525% \n",
      "[Epoch:4, Iter:362] Loss: 0.545 | Acc: 72.645% \n",
      "[Epoch:4, Iter:363] Loss: 0.545 | Acc: 72.698% \n",
      "[Epoch:4, Iter:364] Loss: 0.546 | Acc: 72.625% \n",
      "[Epoch:4, Iter:365] Loss: 0.546 | Acc: 72.646% \n",
      "[Epoch:4, Iter:366] Loss: 0.546 | Acc: 72.636% \n",
      "[Epoch:4, Iter:367] Loss: 0.546 | Acc: 72.746% \n",
      "[Epoch:4, Iter:368] Loss: 0.544 | Acc: 72.853% \n",
      "[Epoch:4, Iter:369] Loss: 0.544 | Acc: 72.841% \n",
      "[Epoch:4, Iter:370] Loss: 0.544 | Acc: 72.800% \n",
      "[Epoch:4, Iter:371] Loss: 0.544 | Acc: 72.761% \n",
      "[Epoch:4, Iter:372] Loss: 0.545 | Acc: 72.722% \n",
      "[Epoch:4, Iter:373] Loss: 0.545 | Acc: 72.712% \n",
      "[Epoch:4, Iter:374] Loss: 0.546 | Acc: 72.649% \n",
      "[Epoch:4, Iter:375] Loss: 0.546 | Acc: 72.560% \n",
      "[Epoch:4, Iter:376] Loss: 0.546 | Acc: 72.632% \n",
      "[Epoch:4, Iter:377] Loss: 0.546 | Acc: 72.649% \n",
      "[Epoch:4, Iter:378] Loss: 0.546 | Acc: 72.564% \n",
      "[Epoch:4, Iter:379] Loss: 0.544 | Acc: 72.734% \n",
      "[Epoch:4, Iter:380] Loss: 0.544 | Acc: 72.725% \n",
      "[Epoch:4, Iter:381] Loss: 0.544 | Acc: 72.765% \n",
      "[Epoch:4, Iter:382] Loss: 0.543 | Acc: 72.756% \n",
      "[Epoch:4, Iter:383] Loss: 0.543 | Acc: 72.723% \n",
      "[Epoch:4, Iter:384] Loss: 0.545 | Acc: 72.619% \n",
      "[Epoch:4, Iter:385] Loss: 0.546 | Acc: 72.518% \n",
      "[Epoch:4, Iter:386] Loss: 0.547 | Acc: 72.419% \n",
      "[Epoch:4, Iter:387] Loss: 0.547 | Acc: 72.460% \n",
      "[Epoch:4, Iter:388] Loss: 0.546 | Acc: 72.523% \n",
      "[Epoch:4, Iter:389] Loss: 0.547 | Acc: 72.607% \n",
      "[Epoch:4, Iter:390] Loss: 0.546 | Acc: 72.644% \n",
      "[Epoch:4, Iter:391] Loss: 0.546 | Acc: 72.681% \n",
      "[Epoch:4, Iter:392] Loss: 0.546 | Acc: 72.739% \n",
      "[Epoch:4, Iter:393] Loss: 0.545 | Acc: 72.731% \n",
      "[Epoch:4, Iter:394] Loss: 0.545 | Acc: 72.723% \n",
      "[Epoch:4, Iter:395] Loss: 0.545 | Acc: 72.695% \n",
      "[Epoch:4, Iter:396] Loss: 0.545 | Acc: 72.708% \n",
      "[Epoch:4, Iter:397] Loss: 0.545 | Acc: 72.722% \n",
      "[Epoch:4, Iter:398] Loss: 0.544 | Acc: 72.776% \n",
      "[Epoch:4, Iter:399] Loss: 0.545 | Acc: 72.667% \n",
      "[Epoch:4, Iter:400] Loss: 0.546 | Acc: 72.580% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 71.820%\n",
      "Training set's accuracy (after quantization) is: 71.920%\n",
      "Test set's accuracy (before quantization) is: 71.000%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.555 | Train Acc: 71.820% | Test Loss: 0.559 | Test Acc: 71.000% \n",
      "Quantized Train Loss: 0.552 | Quantized Train Acc: 71.920% | Quantized Test Loss: 0.554 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 5\n",
      "[Epoch:5, Iter:401] Loss: 0.568 | Acc: 66.000% \n",
      "[Epoch:5, Iter:402] Loss: 0.551 | Acc: 70.000% \n",
      "[Epoch:5, Iter:403] Loss: 0.525 | Acc: 72.000% \n",
      "[Epoch:5, Iter:404] Loss: 0.560 | Acc: 72.500% \n",
      "[Epoch:5, Iter:405] Loss: 0.572 | Acc: 72.000% \n",
      "[Epoch:5, Iter:406] Loss: 0.571 | Acc: 71.000% \n",
      "[Epoch:5, Iter:407] Loss: 0.552 | Acc: 72.286% \n",
      "[Epoch:5, Iter:408] Loss: 0.565 | Acc: 71.750% \n",
      "[Epoch:5, Iter:409] Loss: 0.574 | Acc: 71.111% \n",
      "[Epoch:5, Iter:410] Loss: 0.571 | Acc: 71.400% \n",
      "[Epoch:5, Iter:411] Loss: 0.573 | Acc: 71.091% \n",
      "[Epoch:5, Iter:412] Loss: 0.568 | Acc: 72.000% \n",
      "[Epoch:5, Iter:413] Loss: 0.570 | Acc: 72.154% \n",
      "[Epoch:5, Iter:414] Loss: 0.571 | Acc: 72.000% \n",
      "[Epoch:5, Iter:415] Loss: 0.569 | Acc: 71.733% \n",
      "[Epoch:5, Iter:416] Loss: 0.567 | Acc: 72.250% \n",
      "[Epoch:5, Iter:417] Loss: 0.574 | Acc: 71.529% \n",
      "[Epoch:5, Iter:418] Loss: 0.574 | Acc: 71.556% \n",
      "[Epoch:5, Iter:419] Loss: 0.571 | Acc: 71.789% \n",
      "[Epoch:5, Iter:420] Loss: 0.570 | Acc: 71.600% \n",
      "[Epoch:5, Iter:421] Loss: 0.569 | Acc: 71.619% \n",
      "[Epoch:5, Iter:422] Loss: 0.564 | Acc: 71.909% \n",
      "[Epoch:5, Iter:423] Loss: 0.564 | Acc: 72.000% \n",
      "[Epoch:5, Iter:424] Loss: 0.568 | Acc: 71.500% \n",
      "[Epoch:5, Iter:425] Loss: 0.565 | Acc: 71.920% \n",
      "[Epoch:5, Iter:426] Loss: 0.564 | Acc: 72.000% \n",
      "[Epoch:5, Iter:427] Loss: 0.561 | Acc: 72.148% \n",
      "[Epoch:5, Iter:428] Loss: 0.559 | Acc: 72.143% \n",
      "[Epoch:5, Iter:429] Loss: 0.561 | Acc: 71.793% \n",
      "[Epoch:5, Iter:430] Loss: 0.564 | Acc: 71.400% \n",
      "[Epoch:5, Iter:431] Loss: 0.563 | Acc: 71.419% \n",
      "[Epoch:5, Iter:432] Loss: 0.563 | Acc: 71.375% \n",
      "[Epoch:5, Iter:433] Loss: 0.562 | Acc: 71.333% \n",
      "[Epoch:5, Iter:434] Loss: 0.564 | Acc: 71.353% \n",
      "[Epoch:5, Iter:435] Loss: 0.564 | Acc: 71.314% \n",
      "[Epoch:5, Iter:436] Loss: 0.562 | Acc: 71.389% \n",
      "[Epoch:5, Iter:437] Loss: 0.560 | Acc: 71.676% \n",
      "[Epoch:5, Iter:438] Loss: 0.556 | Acc: 71.895% \n",
      "[Epoch:5, Iter:439] Loss: 0.557 | Acc: 71.949% \n",
      "[Epoch:5, Iter:440] Loss: 0.556 | Acc: 72.000% \n",
      "[Epoch:5, Iter:441] Loss: 0.555 | Acc: 72.146% \n",
      "[Epoch:5, Iter:442] Loss: 0.555 | Acc: 72.048% \n",
      "[Epoch:5, Iter:443] Loss: 0.552 | Acc: 72.233% \n",
      "[Epoch:5, Iter:444] Loss: 0.552 | Acc: 72.364% \n",
      "[Epoch:5, Iter:445] Loss: 0.550 | Acc: 72.489% \n",
      "[Epoch:5, Iter:446] Loss: 0.548 | Acc: 72.565% \n",
      "[Epoch:5, Iter:447] Loss: 0.549 | Acc: 72.511% \n",
      "[Epoch:5, Iter:448] Loss: 0.549 | Acc: 72.417% \n",
      "[Epoch:5, Iter:449] Loss: 0.548 | Acc: 72.571% \n",
      "[Epoch:5, Iter:450] Loss: 0.546 | Acc: 72.720% \n",
      "[Epoch:5, Iter:451] Loss: 0.547 | Acc: 72.667% \n",
      "[Epoch:5, Iter:452] Loss: 0.547 | Acc: 72.692% \n",
      "[Epoch:5, Iter:453] Loss: 0.547 | Acc: 72.642% \n",
      "[Epoch:5, Iter:454] Loss: 0.549 | Acc: 72.481% \n",
      "[Epoch:5, Iter:455] Loss: 0.548 | Acc: 72.618% \n",
      "[Epoch:5, Iter:456] Loss: 0.547 | Acc: 72.679% \n",
      "[Epoch:5, Iter:457] Loss: 0.547 | Acc: 72.667% \n",
      "[Epoch:5, Iter:458] Loss: 0.547 | Acc: 72.793% \n",
      "[Epoch:5, Iter:459] Loss: 0.548 | Acc: 72.576% \n",
      "[Epoch:5, Iter:460] Loss: 0.549 | Acc: 72.500% \n",
      "[Epoch:5, Iter:461] Loss: 0.549 | Acc: 72.492% \n",
      "[Epoch:5, Iter:462] Loss: 0.548 | Acc: 72.484% \n",
      "[Epoch:5, Iter:463] Loss: 0.549 | Acc: 72.444% \n",
      "[Epoch:5, Iter:464] Loss: 0.549 | Acc: 72.625% \n",
      "[Epoch:5, Iter:465] Loss: 0.549 | Acc: 72.615% \n",
      "[Epoch:5, Iter:466] Loss: 0.548 | Acc: 72.727% \n",
      "[Epoch:5, Iter:467] Loss: 0.547 | Acc: 72.657% \n",
      "[Epoch:5, Iter:468] Loss: 0.549 | Acc: 72.559% \n",
      "[Epoch:5, Iter:469] Loss: 0.548 | Acc: 72.580% \n",
      "[Epoch:5, Iter:470] Loss: 0.548 | Acc: 72.600% \n",
      "[Epoch:5, Iter:471] Loss: 0.547 | Acc: 72.761% \n",
      "[Epoch:5, Iter:472] Loss: 0.547 | Acc: 72.694% \n",
      "[Epoch:5, Iter:473] Loss: 0.546 | Acc: 72.822% \n",
      "[Epoch:5, Iter:474] Loss: 0.546 | Acc: 72.757% \n",
      "[Epoch:5, Iter:475] Loss: 0.547 | Acc: 72.800% \n",
      "[Epoch:5, Iter:476] Loss: 0.548 | Acc: 72.711% \n",
      "[Epoch:5, Iter:477] Loss: 0.547 | Acc: 72.779% \n",
      "[Epoch:5, Iter:478] Loss: 0.546 | Acc: 72.846% \n",
      "[Epoch:5, Iter:479] Loss: 0.546 | Acc: 72.810% \n",
      "[Epoch:5, Iter:480] Loss: 0.545 | Acc: 72.850% \n",
      "[Epoch:5, Iter:481] Loss: 0.547 | Acc: 72.667% \n",
      "[Epoch:5, Iter:482] Loss: 0.548 | Acc: 72.537% \n",
      "[Epoch:5, Iter:483] Loss: 0.549 | Acc: 72.458% \n",
      "[Epoch:5, Iter:484] Loss: 0.549 | Acc: 72.500% \n",
      "[Epoch:5, Iter:485] Loss: 0.548 | Acc: 72.518% \n",
      "[Epoch:5, Iter:486] Loss: 0.547 | Acc: 72.581% \n",
      "[Epoch:5, Iter:487] Loss: 0.547 | Acc: 72.506% \n",
      "[Epoch:5, Iter:488] Loss: 0.546 | Acc: 72.568% \n",
      "[Epoch:5, Iter:489] Loss: 0.546 | Acc: 72.584% \n",
      "[Epoch:5, Iter:490] Loss: 0.547 | Acc: 72.467% \n",
      "[Epoch:5, Iter:491] Loss: 0.547 | Acc: 72.593% \n",
      "[Epoch:5, Iter:492] Loss: 0.545 | Acc: 72.717% \n",
      "[Epoch:5, Iter:493] Loss: 0.546 | Acc: 72.688% \n",
      "[Epoch:5, Iter:494] Loss: 0.546 | Acc: 72.596% \n",
      "[Epoch:5, Iter:495] Loss: 0.546 | Acc: 72.589% \n",
      "[Epoch:5, Iter:496] Loss: 0.546 | Acc: 72.562% \n",
      "[Epoch:5, Iter:497] Loss: 0.547 | Acc: 72.515% \n",
      "[Epoch:5, Iter:498] Loss: 0.545 | Acc: 72.673% \n",
      "[Epoch:5, Iter:499] Loss: 0.544 | Acc: 72.707% \n",
      "[Epoch:5, Iter:500] Loss: 0.545 | Acc: 72.680% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.940%\n",
      "Training set's accuracy (after quantization) is: 72.600%\n",
      "Test set's accuracy (before quantization) is: 72.600%\n",
      "Test set's accuracy (after quantization) is: 71.800%\n",
      "Train Loss: 0.540 | Train Acc: 72.940% | Test Loss: 0.545 | Test Acc: 72.600% \n",
      "Quantized Train Loss: 0.546 | Quantized Train Acc: 72.600% | Quantized Test Loss: 0.549 | Quantized Test Acc: 71.800% \n",
      "\n",
      "Epoch: 6\n",
      "[Epoch:6, Iter:501] Loss: 0.532 | Acc: 76.000% \n",
      "[Epoch:6, Iter:502] Loss: 0.479 | Acc: 78.000% \n",
      "[Epoch:6, Iter:503] Loss: 0.493 | Acc: 78.000% \n",
      "[Epoch:6, Iter:504] Loss: 0.517 | Acc: 76.000% \n",
      "[Epoch:6, Iter:505] Loss: 0.534 | Acc: 73.600% \n",
      "[Epoch:6, Iter:506] Loss: 0.529 | Acc: 73.667% \n",
      "[Epoch:6, Iter:507] Loss: 0.525 | Acc: 74.286% \n",
      "[Epoch:6, Iter:508] Loss: 0.525 | Acc: 74.750% \n",
      "[Epoch:6, Iter:509] Loss: 0.533 | Acc: 74.444% \n",
      "[Epoch:6, Iter:510] Loss: 0.529 | Acc: 74.400% \n",
      "[Epoch:6, Iter:511] Loss: 0.528 | Acc: 74.727% \n",
      "[Epoch:6, Iter:512] Loss: 0.528 | Acc: 75.000% \n",
      "[Epoch:6, Iter:513] Loss: 0.524 | Acc: 75.077% \n",
      "[Epoch:6, Iter:514] Loss: 0.538 | Acc: 73.857% \n",
      "[Epoch:6, Iter:515] Loss: 0.531 | Acc: 74.533% \n",
      "[Epoch:6, Iter:516] Loss: 0.528 | Acc: 74.875% \n",
      "[Epoch:6, Iter:517] Loss: 0.532 | Acc: 74.588% \n",
      "[Epoch:6, Iter:518] Loss: 0.533 | Acc: 74.111% \n",
      "[Epoch:6, Iter:519] Loss: 0.531 | Acc: 74.632% \n",
      "[Epoch:6, Iter:520] Loss: 0.528 | Acc: 74.700% \n",
      "[Epoch:6, Iter:521] Loss: 0.533 | Acc: 74.286% \n",
      "[Epoch:6, Iter:522] Loss: 0.532 | Acc: 74.182% \n",
      "[Epoch:6, Iter:523] Loss: 0.528 | Acc: 74.609% \n",
      "[Epoch:6, Iter:524] Loss: 0.535 | Acc: 74.333% \n",
      "[Epoch:6, Iter:525] Loss: 0.534 | Acc: 74.400% \n",
      "[Epoch:6, Iter:526] Loss: 0.533 | Acc: 74.538% \n",
      "[Epoch:6, Iter:527] Loss: 0.538 | Acc: 73.852% \n",
      "[Epoch:6, Iter:528] Loss: 0.534 | Acc: 74.214% \n",
      "[Epoch:6, Iter:529] Loss: 0.539 | Acc: 73.724% \n",
      "[Epoch:6, Iter:530] Loss: 0.537 | Acc: 73.733% \n",
      "[Epoch:6, Iter:531] Loss: 0.537 | Acc: 73.871% \n",
      "[Epoch:6, Iter:532] Loss: 0.538 | Acc: 73.750% \n",
      "[Epoch:6, Iter:533] Loss: 0.536 | Acc: 73.758% \n",
      "[Epoch:6, Iter:534] Loss: 0.535 | Acc: 73.941% \n",
      "[Epoch:6, Iter:535] Loss: 0.536 | Acc: 73.771% \n",
      "[Epoch:6, Iter:536] Loss: 0.538 | Acc: 73.611% \n",
      "[Epoch:6, Iter:537] Loss: 0.535 | Acc: 73.676% \n",
      "[Epoch:6, Iter:538] Loss: 0.537 | Acc: 73.579% \n",
      "[Epoch:6, Iter:539] Loss: 0.535 | Acc: 73.692% \n",
      "[Epoch:6, Iter:540] Loss: 0.535 | Acc: 73.550% \n",
      "[Epoch:6, Iter:541] Loss: 0.538 | Acc: 73.415% \n",
      "[Epoch:6, Iter:542] Loss: 0.535 | Acc: 73.667% \n",
      "[Epoch:6, Iter:543] Loss: 0.539 | Acc: 73.442% \n",
      "[Epoch:6, Iter:544] Loss: 0.539 | Acc: 73.545% \n",
      "[Epoch:6, Iter:545] Loss: 0.542 | Acc: 73.244% \n",
      "[Epoch:6, Iter:546] Loss: 0.543 | Acc: 73.130% \n",
      "[Epoch:6, Iter:547] Loss: 0.543 | Acc: 73.064% \n",
      "[Epoch:6, Iter:548] Loss: 0.542 | Acc: 73.083% \n",
      "[Epoch:6, Iter:549] Loss: 0.540 | Acc: 73.265% \n",
      "[Epoch:6, Iter:550] Loss: 0.539 | Acc: 73.360% \n",
      "[Epoch:6, Iter:551] Loss: 0.542 | Acc: 73.137% \n",
      "[Epoch:6, Iter:552] Loss: 0.544 | Acc: 72.885% \n",
      "[Epoch:6, Iter:553] Loss: 0.545 | Acc: 72.906% \n",
      "[Epoch:6, Iter:554] Loss: 0.545 | Acc: 72.852% \n",
      "[Epoch:6, Iter:555] Loss: 0.545 | Acc: 72.836% \n",
      "[Epoch:6, Iter:556] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:6, Iter:557] Loss: 0.544 | Acc: 72.877% \n",
      "[Epoch:6, Iter:558] Loss: 0.543 | Acc: 73.034% \n",
      "[Epoch:6, Iter:559] Loss: 0.543 | Acc: 73.085% \n",
      "[Epoch:6, Iter:560] Loss: 0.541 | Acc: 73.333% \n",
      "[Epoch:6, Iter:561] Loss: 0.541 | Acc: 73.410% \n",
      "[Epoch:6, Iter:562] Loss: 0.540 | Acc: 73.419% \n",
      "[Epoch:6, Iter:563] Loss: 0.539 | Acc: 73.429% \n",
      "[Epoch:6, Iter:564] Loss: 0.539 | Acc: 73.438% \n",
      "[Epoch:6, Iter:565] Loss: 0.540 | Acc: 73.415% \n",
      "[Epoch:6, Iter:566] Loss: 0.541 | Acc: 73.303% \n",
      "[Epoch:6, Iter:567] Loss: 0.541 | Acc: 73.313% \n",
      "[Epoch:6, Iter:568] Loss: 0.542 | Acc: 73.294% \n",
      "[Epoch:6, Iter:569] Loss: 0.541 | Acc: 73.304% \n",
      "[Epoch:6, Iter:570] Loss: 0.541 | Acc: 73.314% \n",
      "[Epoch:6, Iter:571] Loss: 0.541 | Acc: 73.268% \n",
      "[Epoch:6, Iter:572] Loss: 0.540 | Acc: 73.333% \n",
      "[Epoch:6, Iter:573] Loss: 0.540 | Acc: 73.397% \n",
      "[Epoch:6, Iter:574] Loss: 0.541 | Acc: 73.324% \n",
      "[Epoch:6, Iter:575] Loss: 0.539 | Acc: 73.413% \n",
      "[Epoch:6, Iter:576] Loss: 0.539 | Acc: 73.421% \n",
      "[Epoch:6, Iter:577] Loss: 0.539 | Acc: 73.403% \n",
      "[Epoch:6, Iter:578] Loss: 0.540 | Acc: 73.333% \n",
      "[Epoch:6, Iter:579] Loss: 0.541 | Acc: 73.165% \n",
      "[Epoch:6, Iter:580] Loss: 0.542 | Acc: 73.125% \n",
      "[Epoch:6, Iter:581] Loss: 0.541 | Acc: 73.160% \n",
      "[Epoch:6, Iter:582] Loss: 0.543 | Acc: 72.951% \n",
      "[Epoch:6, Iter:583] Loss: 0.543 | Acc: 72.964% \n",
      "[Epoch:6, Iter:584] Loss: 0.544 | Acc: 72.929% \n",
      "[Epoch:6, Iter:585] Loss: 0.544 | Acc: 72.941% \n",
      "[Epoch:6, Iter:586] Loss: 0.544 | Acc: 72.907% \n",
      "[Epoch:6, Iter:587] Loss: 0.544 | Acc: 72.943% \n",
      "[Epoch:6, Iter:588] Loss: 0.544 | Acc: 72.932% \n",
      "[Epoch:6, Iter:589] Loss: 0.545 | Acc: 72.899% \n",
      "[Epoch:6, Iter:590] Loss: 0.544 | Acc: 72.933% \n",
      "[Epoch:6, Iter:591] Loss: 0.544 | Acc: 72.901% \n",
      "[Epoch:6, Iter:592] Loss: 0.544 | Acc: 72.913% \n",
      "[Epoch:6, Iter:593] Loss: 0.545 | Acc: 72.882% \n",
      "[Epoch:6, Iter:594] Loss: 0.546 | Acc: 72.830% \n",
      "[Epoch:6, Iter:595] Loss: 0.546 | Acc: 72.779% \n",
      "[Epoch:6, Iter:596] Loss: 0.544 | Acc: 72.896% \n",
      "[Epoch:6, Iter:597] Loss: 0.544 | Acc: 72.866% \n",
      "[Epoch:6, Iter:598] Loss: 0.545 | Acc: 72.796% \n",
      "[Epoch:6, Iter:599] Loss: 0.546 | Acc: 72.727% \n",
      "[Epoch:6, Iter:600] Loss: 0.546 | Acc: 72.660% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.840%\n",
      "Training set's accuracy (after quantization) is: 71.860%\n",
      "Test set's accuracy (before quantization) is: 72.100%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.542 | Train Acc: 72.840% | Test Loss: 0.546 | Test Acc: 72.100% \n",
      "Quantized Train Loss: 0.551 | Quantized Train Acc: 71.860% | Quantized Test Loss: 0.554 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 7\n",
      "[Epoch:7, Iter:601] Loss: 0.491 | Acc: 76.000% \n",
      "[Epoch:7, Iter:602] Loss: 0.509 | Acc: 76.000% \n",
      "[Epoch:7, Iter:603] Loss: 0.562 | Acc: 71.333% \n",
      "[Epoch:7, Iter:604] Loss: 0.584 | Acc: 71.000% \n",
      "[Epoch:7, Iter:605] Loss: 0.595 | Acc: 69.200% \n",
      "[Epoch:7, Iter:606] Loss: 0.587 | Acc: 70.000% \n",
      "[Epoch:7, Iter:607] Loss: 0.581 | Acc: 70.571% \n",
      "[Epoch:7, Iter:608] Loss: 0.591 | Acc: 70.250% \n",
      "[Epoch:7, Iter:609] Loss: 0.585 | Acc: 70.000% \n",
      "[Epoch:7, Iter:610] Loss: 0.584 | Acc: 70.000% \n",
      "[Epoch:7, Iter:611] Loss: 0.581 | Acc: 70.364% \n",
      "[Epoch:7, Iter:612] Loss: 0.581 | Acc: 70.167% \n",
      "[Epoch:7, Iter:613] Loss: 0.572 | Acc: 70.769% \n",
      "[Epoch:7, Iter:614] Loss: 0.571 | Acc: 70.857% \n",
      "[Epoch:7, Iter:615] Loss: 0.570 | Acc: 70.933% \n",
      "[Epoch:7, Iter:616] Loss: 0.566 | Acc: 71.000% \n",
      "[Epoch:7, Iter:617] Loss: 0.566 | Acc: 70.941% \n",
      "[Epoch:7, Iter:618] Loss: 0.562 | Acc: 70.889% \n",
      "[Epoch:7, Iter:619] Loss: 0.561 | Acc: 70.947% \n",
      "[Epoch:7, Iter:620] Loss: 0.560 | Acc: 70.900% \n",
      "[Epoch:7, Iter:621] Loss: 0.560 | Acc: 70.857% \n",
      "[Epoch:7, Iter:622] Loss: 0.559 | Acc: 71.000% \n",
      "[Epoch:7, Iter:623] Loss: 0.557 | Acc: 71.304% \n",
      "[Epoch:7, Iter:624] Loss: 0.559 | Acc: 71.167% \n",
      "[Epoch:7, Iter:625] Loss: 0.561 | Acc: 71.040% \n",
      "[Epoch:7, Iter:626] Loss: 0.562 | Acc: 70.923% \n",
      "[Epoch:7, Iter:627] Loss: 0.562 | Acc: 71.111% \n",
      "[Epoch:7, Iter:628] Loss: 0.561 | Acc: 71.143% \n",
      "[Epoch:7, Iter:629] Loss: 0.559 | Acc: 71.241% \n",
      "[Epoch:7, Iter:630] Loss: 0.556 | Acc: 71.333% \n",
      "[Epoch:7, Iter:631] Loss: 0.554 | Acc: 71.419% \n",
      "[Epoch:7, Iter:632] Loss: 0.553 | Acc: 71.500% \n",
      "[Epoch:7, Iter:633] Loss: 0.553 | Acc: 71.333% \n",
      "[Epoch:7, Iter:634] Loss: 0.554 | Acc: 71.059% \n",
      "[Epoch:7, Iter:635] Loss: 0.554 | Acc: 71.143% \n",
      "[Epoch:7, Iter:636] Loss: 0.556 | Acc: 71.167% \n",
      "[Epoch:7, Iter:637] Loss: 0.555 | Acc: 71.027% \n",
      "[Epoch:7, Iter:638] Loss: 0.556 | Acc: 70.789% \n",
      "[Epoch:7, Iter:639] Loss: 0.553 | Acc: 70.974% \n",
      "[Epoch:7, Iter:640] Loss: 0.552 | Acc: 71.300% \n",
      "[Epoch:7, Iter:641] Loss: 0.551 | Acc: 71.366% \n",
      "[Epoch:7, Iter:642] Loss: 0.554 | Acc: 71.143% \n",
      "[Epoch:7, Iter:643] Loss: 0.554 | Acc: 71.349% \n",
      "[Epoch:7, Iter:644] Loss: 0.552 | Acc: 71.545% \n",
      "[Epoch:7, Iter:645] Loss: 0.552 | Acc: 71.511% \n",
      "[Epoch:7, Iter:646] Loss: 0.551 | Acc: 71.609% \n",
      "[Epoch:7, Iter:647] Loss: 0.550 | Acc: 71.702% \n",
      "[Epoch:7, Iter:648] Loss: 0.549 | Acc: 71.750% \n",
      "[Epoch:7, Iter:649] Loss: 0.550 | Acc: 71.714% \n",
      "[Epoch:7, Iter:650] Loss: 0.551 | Acc: 71.640% \n",
      "[Epoch:7, Iter:651] Loss: 0.551 | Acc: 71.647% \n",
      "[Epoch:7, Iter:652] Loss: 0.554 | Acc: 71.346% \n",
      "[Epoch:7, Iter:653] Loss: 0.553 | Acc: 71.434% \n",
      "[Epoch:7, Iter:654] Loss: 0.552 | Acc: 71.556% \n",
      "[Epoch:7, Iter:655] Loss: 0.549 | Acc: 71.782% \n",
      "[Epoch:7, Iter:656] Loss: 0.547 | Acc: 71.964% \n",
      "[Epoch:7, Iter:657] Loss: 0.547 | Acc: 72.035% \n",
      "[Epoch:7, Iter:658] Loss: 0.546 | Acc: 72.138% \n",
      "[Epoch:7, Iter:659] Loss: 0.545 | Acc: 72.203% \n",
      "[Epoch:7, Iter:660] Loss: 0.544 | Acc: 72.233% \n",
      "[Epoch:7, Iter:661] Loss: 0.544 | Acc: 72.230% \n",
      "[Epoch:7, Iter:662] Loss: 0.543 | Acc: 72.258% \n",
      "[Epoch:7, Iter:663] Loss: 0.544 | Acc: 72.222% \n",
      "[Epoch:7, Iter:664] Loss: 0.543 | Acc: 72.250% \n",
      "[Epoch:7, Iter:665] Loss: 0.542 | Acc: 72.308% \n",
      "[Epoch:7, Iter:666] Loss: 0.543 | Acc: 72.394% \n",
      "[Epoch:7, Iter:667] Loss: 0.544 | Acc: 72.358% \n",
      "[Epoch:7, Iter:668] Loss: 0.544 | Acc: 72.265% \n",
      "[Epoch:7, Iter:669] Loss: 0.543 | Acc: 72.319% \n",
      "[Epoch:7, Iter:670] Loss: 0.543 | Acc: 72.314% \n",
      "[Epoch:7, Iter:671] Loss: 0.543 | Acc: 72.366% \n",
      "[Epoch:7, Iter:672] Loss: 0.543 | Acc: 72.278% \n",
      "[Epoch:7, Iter:673] Loss: 0.543 | Acc: 72.329% \n",
      "[Epoch:7, Iter:674] Loss: 0.542 | Acc: 72.486% \n",
      "[Epoch:7, Iter:675] Loss: 0.543 | Acc: 72.480% \n",
      "[Epoch:7, Iter:676] Loss: 0.544 | Acc: 72.526% \n",
      "[Epoch:7, Iter:677] Loss: 0.543 | Acc: 72.597% \n",
      "[Epoch:7, Iter:678] Loss: 0.542 | Acc: 72.615% \n",
      "[Epoch:7, Iter:679] Loss: 0.544 | Acc: 72.633% \n",
      "[Epoch:7, Iter:680] Loss: 0.543 | Acc: 72.750% \n",
      "[Epoch:7, Iter:681] Loss: 0.544 | Acc: 72.691% \n",
      "[Epoch:7, Iter:682] Loss: 0.544 | Acc: 72.634% \n",
      "[Epoch:7, Iter:683] Loss: 0.543 | Acc: 72.771% \n",
      "[Epoch:7, Iter:684] Loss: 0.544 | Acc: 72.738% \n",
      "[Epoch:7, Iter:685] Loss: 0.544 | Acc: 72.682% \n",
      "[Epoch:7, Iter:686] Loss: 0.544 | Acc: 72.814% \n",
      "[Epoch:7, Iter:687] Loss: 0.544 | Acc: 72.805% \n",
      "[Epoch:7, Iter:688] Loss: 0.544 | Acc: 72.818% \n",
      "[Epoch:7, Iter:689] Loss: 0.544 | Acc: 72.697% \n",
      "[Epoch:7, Iter:690] Loss: 0.545 | Acc: 72.733% \n",
      "[Epoch:7, Iter:691] Loss: 0.546 | Acc: 72.725% \n",
      "[Epoch:7, Iter:692] Loss: 0.545 | Acc: 72.739% \n",
      "[Epoch:7, Iter:693] Loss: 0.545 | Acc: 72.710% \n",
      "[Epoch:7, Iter:694] Loss: 0.544 | Acc: 72.851% \n",
      "[Epoch:7, Iter:695] Loss: 0.543 | Acc: 72.926% \n",
      "[Epoch:7, Iter:696] Loss: 0.543 | Acc: 72.917% \n",
      "[Epoch:7, Iter:697] Loss: 0.543 | Acc: 72.907% \n",
      "[Epoch:7, Iter:698] Loss: 0.543 | Acc: 72.796% \n",
      "[Epoch:7, Iter:699] Loss: 0.544 | Acc: 72.768% \n",
      "[Epoch:7, Iter:700] Loss: 0.545 | Acc: 72.720% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.720%\n",
      "Training set's accuracy (after quantization) is: 71.820%\n",
      "Test set's accuracy (before quantization) is: 71.600%\n",
      "Test set's accuracy (after quantization) is: 71.100%\n",
      "Train Loss: 0.544 | Train Acc: 72.720% | Test Loss: 0.548 | Test Acc: 71.600% \n",
      "Quantized Train Loss: 0.553 | Quantized Train Acc: 71.820% | Quantized Test Loss: 0.556 | Quantized Test Acc: 71.100% \n",
      "\n",
      "Epoch: 8\n",
      "[Epoch:8, Iter:701] Loss: 0.506 | Acc: 80.000% \n",
      "[Epoch:8, Iter:702] Loss: 0.512 | Acc: 77.000% \n",
      "[Epoch:8, Iter:703] Loss: 0.502 | Acc: 78.000% \n",
      "[Epoch:8, Iter:704] Loss: 0.497 | Acc: 77.000% \n",
      "[Epoch:8, Iter:705] Loss: 0.515 | Acc: 75.200% \n",
      "[Epoch:8, Iter:706] Loss: 0.520 | Acc: 74.333% \n",
      "[Epoch:8, Iter:707] Loss: 0.527 | Acc: 73.429% \n",
      "[Epoch:8, Iter:708] Loss: 0.519 | Acc: 74.000% \n",
      "[Epoch:8, Iter:709] Loss: 0.514 | Acc: 74.222% \n",
      "[Epoch:8, Iter:710] Loss: 0.521 | Acc: 74.000% \n",
      "[Epoch:8, Iter:711] Loss: 0.523 | Acc: 73.818% \n",
      "[Epoch:8, Iter:712] Loss: 0.531 | Acc: 73.833% \n",
      "[Epoch:8, Iter:713] Loss: 0.536 | Acc: 73.692% \n",
      "[Epoch:8, Iter:714] Loss: 0.542 | Acc: 73.286% \n",
      "[Epoch:8, Iter:715] Loss: 0.540 | Acc: 73.200% \n",
      "[Epoch:8, Iter:716] Loss: 0.542 | Acc: 72.750% \n",
      "[Epoch:8, Iter:717] Loss: 0.543 | Acc: 72.706% \n",
      "[Epoch:8, Iter:718] Loss: 0.545 | Acc: 72.667% \n",
      "[Epoch:8, Iter:719] Loss: 0.546 | Acc: 72.316% \n",
      "[Epoch:8, Iter:720] Loss: 0.544 | Acc: 72.400% \n",
      "[Epoch:8, Iter:721] Loss: 0.539 | Acc: 72.571% \n",
      "[Epoch:8, Iter:722] Loss: 0.542 | Acc: 72.455% \n",
      "[Epoch:8, Iter:723] Loss: 0.543 | Acc: 72.261% \n",
      "[Epoch:8, Iter:724] Loss: 0.540 | Acc: 72.667% \n",
      "[Epoch:8, Iter:725] Loss: 0.541 | Acc: 72.800% \n",
      "[Epoch:8, Iter:726] Loss: 0.543 | Acc: 72.846% \n",
      "[Epoch:8, Iter:727] Loss: 0.539 | Acc: 73.185% \n",
      "[Epoch:8, Iter:728] Loss: 0.541 | Acc: 73.000% \n",
      "[Epoch:8, Iter:729] Loss: 0.541 | Acc: 73.034% \n",
      "[Epoch:8, Iter:730] Loss: 0.539 | Acc: 73.133% \n",
      "[Epoch:8, Iter:731] Loss: 0.535 | Acc: 73.484% \n",
      "[Epoch:8, Iter:732] Loss: 0.535 | Acc: 73.500% \n",
      "[Epoch:8, Iter:733] Loss: 0.540 | Acc: 73.091% \n",
      "[Epoch:8, Iter:734] Loss: 0.536 | Acc: 73.529% \n",
      "[Epoch:8, Iter:735] Loss: 0.537 | Acc: 73.371% \n",
      "[Epoch:8, Iter:736] Loss: 0.537 | Acc: 73.444% \n",
      "[Epoch:8, Iter:737] Loss: 0.538 | Acc: 73.297% \n",
      "[Epoch:8, Iter:738] Loss: 0.535 | Acc: 73.421% \n",
      "[Epoch:8, Iter:739] Loss: 0.536 | Acc: 73.385% \n",
      "[Epoch:8, Iter:740] Loss: 0.537 | Acc: 73.300% \n",
      "[Epoch:8, Iter:741] Loss: 0.536 | Acc: 73.268% \n",
      "[Epoch:8, Iter:742] Loss: 0.538 | Acc: 73.000% \n",
      "[Epoch:8, Iter:743] Loss: 0.539 | Acc: 72.791% \n",
      "[Epoch:8, Iter:744] Loss: 0.538 | Acc: 72.773% \n",
      "[Epoch:8, Iter:745] Loss: 0.539 | Acc: 72.667% \n",
      "[Epoch:8, Iter:746] Loss: 0.540 | Acc: 72.609% \n",
      "[Epoch:8, Iter:747] Loss: 0.541 | Acc: 72.681% \n",
      "[Epoch:8, Iter:748] Loss: 0.540 | Acc: 72.667% \n",
      "[Epoch:8, Iter:749] Loss: 0.543 | Acc: 72.408% \n",
      "[Epoch:8, Iter:750] Loss: 0.544 | Acc: 72.400% \n",
      "[Epoch:8, Iter:751] Loss: 0.544 | Acc: 72.471% \n",
      "[Epoch:8, Iter:752] Loss: 0.547 | Acc: 72.269% \n",
      "[Epoch:8, Iter:753] Loss: 0.547 | Acc: 72.264% \n",
      "[Epoch:8, Iter:754] Loss: 0.548 | Acc: 72.148% \n",
      "[Epoch:8, Iter:755] Loss: 0.547 | Acc: 72.145% \n",
      "[Epoch:8, Iter:756] Loss: 0.548 | Acc: 72.250% \n",
      "[Epoch:8, Iter:757] Loss: 0.547 | Acc: 72.281% \n",
      "[Epoch:8, Iter:758] Loss: 0.548 | Acc: 72.138% \n",
      "[Epoch:8, Iter:759] Loss: 0.549 | Acc: 72.068% \n",
      "[Epoch:8, Iter:760] Loss: 0.549 | Acc: 72.033% \n",
      "[Epoch:8, Iter:761] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:8, Iter:762] Loss: 0.548 | Acc: 72.129% \n",
      "[Epoch:8, Iter:763] Loss: 0.547 | Acc: 72.159% \n",
      "[Epoch:8, Iter:764] Loss: 0.545 | Acc: 72.312% \n",
      "[Epoch:8, Iter:765] Loss: 0.545 | Acc: 72.369% \n",
      "[Epoch:8, Iter:766] Loss: 0.546 | Acc: 72.303% \n",
      "[Epoch:8, Iter:767] Loss: 0.544 | Acc: 72.507% \n",
      "[Epoch:8, Iter:768] Loss: 0.544 | Acc: 72.412% \n",
      "[Epoch:8, Iter:769] Loss: 0.546 | Acc: 72.319% \n",
      "[Epoch:8, Iter:770] Loss: 0.546 | Acc: 72.257% \n",
      "[Epoch:8, Iter:771] Loss: 0.547 | Acc: 72.282% \n",
      "[Epoch:8, Iter:772] Loss: 0.547 | Acc: 72.222% \n",
      "[Epoch:8, Iter:773] Loss: 0.546 | Acc: 72.274% \n",
      "[Epoch:8, Iter:774] Loss: 0.547 | Acc: 72.270% \n",
      "[Epoch:8, Iter:775] Loss: 0.547 | Acc: 72.293% \n",
      "[Epoch:8, Iter:776] Loss: 0.547 | Acc: 72.289% \n",
      "[Epoch:8, Iter:777] Loss: 0.548 | Acc: 72.234% \n",
      "[Epoch:8, Iter:778] Loss: 0.549 | Acc: 72.103% \n",
      "[Epoch:8, Iter:779] Loss: 0.549 | Acc: 72.127% \n",
      "[Epoch:8, Iter:780] Loss: 0.548 | Acc: 72.225% \n",
      "[Epoch:8, Iter:781] Loss: 0.547 | Acc: 72.321% \n",
      "[Epoch:8, Iter:782] Loss: 0.548 | Acc: 72.244% \n",
      "[Epoch:8, Iter:783] Loss: 0.548 | Acc: 72.289% \n",
      "[Epoch:8, Iter:784] Loss: 0.550 | Acc: 72.167% \n",
      "[Epoch:8, Iter:785] Loss: 0.551 | Acc: 72.071% \n",
      "[Epoch:8, Iter:786] Loss: 0.551 | Acc: 72.070% \n",
      "[Epoch:8, Iter:787] Loss: 0.551 | Acc: 72.023% \n",
      "[Epoch:8, Iter:788] Loss: 0.550 | Acc: 72.068% \n",
      "[Epoch:8, Iter:789] Loss: 0.548 | Acc: 72.180% \n",
      "[Epoch:8, Iter:790] Loss: 0.549 | Acc: 72.178% \n",
      "[Epoch:8, Iter:791] Loss: 0.548 | Acc: 72.154% \n",
      "[Epoch:8, Iter:792] Loss: 0.548 | Acc: 72.109% \n",
      "[Epoch:8, Iter:793] Loss: 0.548 | Acc: 72.086% \n",
      "[Epoch:8, Iter:794] Loss: 0.548 | Acc: 72.128% \n",
      "[Epoch:8, Iter:795] Loss: 0.548 | Acc: 72.105% \n",
      "[Epoch:8, Iter:796] Loss: 0.548 | Acc: 72.146% \n",
      "[Epoch:8, Iter:797] Loss: 0.548 | Acc: 72.124% \n",
      "[Epoch:8, Iter:798] Loss: 0.548 | Acc: 72.184% \n",
      "[Epoch:8, Iter:799] Loss: 0.547 | Acc: 72.242% \n",
      "[Epoch:8, Iter:800] Loss: 0.546 | Acc: 72.360% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.940%\n",
      "Training set's accuracy (after quantization) is: 72.120%\n",
      "Test set's accuracy (before quantization) is: 73.100%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.540 | Train Acc: 72.940% | Test Loss: 0.544 | Test Acc: 73.100% \n",
      "Quantized Train Loss: 0.549 | Quantized Train Acc: 72.120% | Quantized Test Loss: 0.551 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 9\n",
      "[Epoch:9, Iter:801] Loss: 0.549 | Acc: 70.000% \n",
      "[Epoch:9, Iter:802] Loss: 0.514 | Acc: 72.000% \n",
      "[Epoch:9, Iter:803] Loss: 0.563 | Acc: 67.333% \n",
      "[Epoch:9, Iter:804] Loss: 0.552 | Acc: 69.500% \n",
      "[Epoch:9, Iter:805] Loss: 0.550 | Acc: 70.800% \n",
      "[Epoch:9, Iter:806] Loss: 0.555 | Acc: 70.000% \n",
      "[Epoch:9, Iter:807] Loss: 0.553 | Acc: 71.143% \n",
      "[Epoch:9, Iter:808] Loss: 0.551 | Acc: 71.500% \n",
      "[Epoch:9, Iter:809] Loss: 0.548 | Acc: 70.889% \n",
      "[Epoch:9, Iter:810] Loss: 0.557 | Acc: 70.400% \n",
      "[Epoch:9, Iter:811] Loss: 0.552 | Acc: 70.909% \n",
      "[Epoch:9, Iter:812] Loss: 0.556 | Acc: 70.667% \n",
      "[Epoch:9, Iter:813] Loss: 0.562 | Acc: 70.000% \n",
      "[Epoch:9, Iter:814] Loss: 0.562 | Acc: 70.143% \n",
      "[Epoch:9, Iter:815] Loss: 0.571 | Acc: 69.733% \n",
      "[Epoch:9, Iter:816] Loss: 0.567 | Acc: 69.750% \n",
      "[Epoch:9, Iter:817] Loss: 0.566 | Acc: 69.765% \n",
      "[Epoch:9, Iter:818] Loss: 0.563 | Acc: 70.222% \n",
      "[Epoch:9, Iter:819] Loss: 0.561 | Acc: 70.526% \n",
      "[Epoch:9, Iter:820] Loss: 0.567 | Acc: 70.100% \n",
      "[Epoch:9, Iter:821] Loss: 0.570 | Acc: 70.000% \n",
      "[Epoch:9, Iter:822] Loss: 0.568 | Acc: 70.273% \n",
      "[Epoch:9, Iter:823] Loss: 0.573 | Acc: 69.913% \n",
      "[Epoch:9, Iter:824] Loss: 0.573 | Acc: 69.750% \n",
      "[Epoch:9, Iter:825] Loss: 0.571 | Acc: 70.080% \n",
      "[Epoch:9, Iter:826] Loss: 0.571 | Acc: 70.077% \n",
      "[Epoch:9, Iter:827] Loss: 0.575 | Acc: 69.630% \n",
      "[Epoch:9, Iter:828] Loss: 0.570 | Acc: 70.143% \n",
      "[Epoch:9, Iter:829] Loss: 0.572 | Acc: 70.138% \n",
      "[Epoch:9, Iter:830] Loss: 0.569 | Acc: 70.400% \n",
      "[Epoch:9, Iter:831] Loss: 0.569 | Acc: 70.452% \n",
      "[Epoch:9, Iter:832] Loss: 0.573 | Acc: 70.250% \n",
      "[Epoch:9, Iter:833] Loss: 0.574 | Acc: 70.121% \n",
      "[Epoch:9, Iter:834] Loss: 0.573 | Acc: 70.353% \n",
      "[Epoch:9, Iter:835] Loss: 0.570 | Acc: 70.514% \n",
      "[Epoch:9, Iter:836] Loss: 0.567 | Acc: 70.611% \n",
      "[Epoch:9, Iter:837] Loss: 0.569 | Acc: 70.541% \n",
      "[Epoch:9, Iter:838] Loss: 0.568 | Acc: 70.789% \n",
      "[Epoch:9, Iter:839] Loss: 0.567 | Acc: 70.974% \n",
      "[Epoch:9, Iter:840] Loss: 0.565 | Acc: 71.150% \n",
      "[Epoch:9, Iter:841] Loss: 0.561 | Acc: 71.366% \n",
      "[Epoch:9, Iter:842] Loss: 0.561 | Acc: 71.333% \n",
      "[Epoch:9, Iter:843] Loss: 0.560 | Acc: 71.488% \n",
      "[Epoch:9, Iter:844] Loss: 0.560 | Acc: 71.455% \n",
      "[Epoch:9, Iter:845] Loss: 0.563 | Acc: 71.289% \n",
      "[Epoch:9, Iter:846] Loss: 0.561 | Acc: 71.609% \n",
      "[Epoch:9, Iter:847] Loss: 0.559 | Acc: 71.660% \n",
      "[Epoch:9, Iter:848] Loss: 0.559 | Acc: 71.625% \n",
      "[Epoch:9, Iter:849] Loss: 0.560 | Acc: 71.673% \n",
      "[Epoch:9, Iter:850] Loss: 0.559 | Acc: 71.720% \n",
      "[Epoch:9, Iter:851] Loss: 0.559 | Acc: 71.608% \n",
      "[Epoch:9, Iter:852] Loss: 0.557 | Acc: 71.769% \n",
      "[Epoch:9, Iter:853] Loss: 0.558 | Acc: 71.736% \n",
      "[Epoch:9, Iter:854] Loss: 0.557 | Acc: 71.889% \n",
      "[Epoch:9, Iter:855] Loss: 0.556 | Acc: 71.891% \n",
      "[Epoch:9, Iter:856] Loss: 0.555 | Acc: 72.071% \n",
      "[Epoch:9, Iter:857] Loss: 0.554 | Acc: 72.140% \n",
      "[Epoch:9, Iter:858] Loss: 0.553 | Acc: 72.207% \n",
      "[Epoch:9, Iter:859] Loss: 0.553 | Acc: 72.339% \n",
      "[Epoch:9, Iter:860] Loss: 0.553 | Acc: 72.433% \n",
      "[Epoch:9, Iter:861] Loss: 0.553 | Acc: 72.492% \n",
      "[Epoch:9, Iter:862] Loss: 0.552 | Acc: 72.581% \n",
      "[Epoch:9, Iter:863] Loss: 0.551 | Acc: 72.635% \n",
      "[Epoch:9, Iter:864] Loss: 0.551 | Acc: 72.656% \n",
      "[Epoch:9, Iter:865] Loss: 0.549 | Acc: 72.800% \n",
      "[Epoch:9, Iter:866] Loss: 0.550 | Acc: 72.727% \n",
      "[Epoch:9, Iter:867] Loss: 0.549 | Acc: 72.746% \n",
      "[Epoch:9, Iter:868] Loss: 0.550 | Acc: 72.706% \n",
      "[Epoch:9, Iter:869] Loss: 0.549 | Acc: 72.812% \n",
      "[Epoch:9, Iter:870] Loss: 0.550 | Acc: 72.743% \n",
      "[Epoch:9, Iter:871] Loss: 0.551 | Acc: 72.676% \n",
      "[Epoch:9, Iter:872] Loss: 0.551 | Acc: 72.694% \n",
      "[Epoch:9, Iter:873] Loss: 0.551 | Acc: 72.658% \n",
      "[Epoch:9, Iter:874] Loss: 0.551 | Acc: 72.676% \n",
      "[Epoch:9, Iter:875] Loss: 0.550 | Acc: 72.720% \n",
      "[Epoch:9, Iter:876] Loss: 0.551 | Acc: 72.658% \n",
      "[Epoch:9, Iter:877] Loss: 0.549 | Acc: 72.727% \n",
      "[Epoch:9, Iter:878] Loss: 0.549 | Acc: 72.718% \n",
      "[Epoch:9, Iter:879] Loss: 0.548 | Acc: 72.684% \n",
      "[Epoch:9, Iter:880] Loss: 0.548 | Acc: 72.600% \n",
      "[Epoch:9, Iter:881] Loss: 0.548 | Acc: 72.593% \n",
      "[Epoch:9, Iter:882] Loss: 0.547 | Acc: 72.683% \n",
      "[Epoch:9, Iter:883] Loss: 0.546 | Acc: 72.723% \n",
      "[Epoch:9, Iter:884] Loss: 0.547 | Acc: 72.643% \n",
      "[Epoch:9, Iter:885] Loss: 0.545 | Acc: 72.776% \n",
      "[Epoch:9, Iter:886] Loss: 0.543 | Acc: 72.884% \n",
      "[Epoch:9, Iter:887] Loss: 0.544 | Acc: 72.805% \n",
      "[Epoch:9, Iter:888] Loss: 0.545 | Acc: 72.705% \n",
      "[Epoch:9, Iter:889] Loss: 0.545 | Acc: 72.719% \n",
      "[Epoch:9, Iter:890] Loss: 0.545 | Acc: 72.733% \n",
      "[Epoch:9, Iter:891] Loss: 0.546 | Acc: 72.681% \n",
      "[Epoch:9, Iter:892] Loss: 0.545 | Acc: 72.804% \n",
      "[Epoch:9, Iter:893] Loss: 0.546 | Acc: 72.688% \n",
      "[Epoch:9, Iter:894] Loss: 0.546 | Acc: 72.596% \n",
      "[Epoch:9, Iter:895] Loss: 0.546 | Acc: 72.611% \n",
      "[Epoch:9, Iter:896] Loss: 0.546 | Acc: 72.646% \n",
      "[Epoch:9, Iter:897] Loss: 0.547 | Acc: 72.577% \n",
      "[Epoch:9, Iter:898] Loss: 0.547 | Acc: 72.531% \n",
      "[Epoch:9, Iter:899] Loss: 0.547 | Acc: 72.465% \n",
      "[Epoch:9, Iter:900] Loss: 0.546 | Acc: 72.600% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.080%\n",
      "Training set's accuracy (after quantization) is: 71.780%\n",
      "Test set's accuracy (before quantization) is: 72.600%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.540 | Train Acc: 73.080% | Test Loss: 0.545 | Test Acc: 72.600% \n",
      "Quantized Train Loss: 0.551 | Quantized Train Acc: 71.780% | Quantized Test Loss: 0.554 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 10\n",
      "[Epoch:10, Iter:901] Loss: 0.653 | Acc: 66.000% \n",
      "[Epoch:10, Iter:902] Loss: 0.551 | Acc: 73.000% \n",
      "[Epoch:10, Iter:903] Loss: 0.550 | Acc: 74.000% \n",
      "[Epoch:10, Iter:904] Loss: 0.546 | Acc: 74.000% \n",
      "[Epoch:10, Iter:905] Loss: 0.546 | Acc: 74.000% \n",
      "[Epoch:10, Iter:906] Loss: 0.533 | Acc: 74.000% \n",
      "[Epoch:10, Iter:907] Loss: 0.548 | Acc: 72.571% \n",
      "[Epoch:10, Iter:908] Loss: 0.544 | Acc: 72.500% \n",
      "[Epoch:10, Iter:909] Loss: 0.543 | Acc: 72.000% \n",
      "[Epoch:10, Iter:910] Loss: 0.551 | Acc: 71.400% \n",
      "[Epoch:10, Iter:911] Loss: 0.552 | Acc: 71.636% \n",
      "[Epoch:10, Iter:912] Loss: 0.554 | Acc: 71.500% \n",
      "[Epoch:10, Iter:913] Loss: 0.555 | Acc: 71.538% \n",
      "[Epoch:10, Iter:914] Loss: 0.557 | Acc: 71.571% \n",
      "[Epoch:10, Iter:915] Loss: 0.559 | Acc: 71.067% \n",
      "[Epoch:10, Iter:916] Loss: 0.554 | Acc: 71.250% \n",
      "[Epoch:10, Iter:917] Loss: 0.548 | Acc: 71.765% \n",
      "[Epoch:10, Iter:918] Loss: 0.548 | Acc: 71.556% \n",
      "[Epoch:10, Iter:919] Loss: 0.553 | Acc: 71.368% \n",
      "[Epoch:10, Iter:920] Loss: 0.553 | Acc: 71.400% \n",
      "[Epoch:10, Iter:921] Loss: 0.552 | Acc: 71.429% \n",
      "[Epoch:10, Iter:922] Loss: 0.544 | Acc: 72.091% \n",
      "[Epoch:10, Iter:923] Loss: 0.546 | Acc: 71.826% \n",
      "[Epoch:10, Iter:924] Loss: 0.545 | Acc: 71.667% \n",
      "[Epoch:10, Iter:925] Loss: 0.546 | Acc: 71.440% \n",
      "[Epoch:10, Iter:926] Loss: 0.547 | Acc: 71.462% \n",
      "[Epoch:10, Iter:927] Loss: 0.549 | Acc: 71.111% \n",
      "[Epoch:10, Iter:928] Loss: 0.551 | Acc: 70.857% \n",
      "[Epoch:10, Iter:929] Loss: 0.550 | Acc: 70.828% \n",
      "[Epoch:10, Iter:930] Loss: 0.550 | Acc: 70.867% \n",
      "[Epoch:10, Iter:931] Loss: 0.547 | Acc: 71.290% \n",
      "[Epoch:10, Iter:932] Loss: 0.545 | Acc: 71.562% \n",
      "[Epoch:10, Iter:933] Loss: 0.544 | Acc: 71.636% \n",
      "[Epoch:10, Iter:934] Loss: 0.546 | Acc: 71.529% \n",
      "[Epoch:10, Iter:935] Loss: 0.546 | Acc: 71.486% \n",
      "[Epoch:10, Iter:936] Loss: 0.547 | Acc: 71.278% \n",
      "[Epoch:10, Iter:937] Loss: 0.546 | Acc: 71.459% \n",
      "[Epoch:10, Iter:938] Loss: 0.549 | Acc: 71.263% \n",
      "[Epoch:10, Iter:939] Loss: 0.548 | Acc: 71.231% \n",
      "[Epoch:10, Iter:940] Loss: 0.547 | Acc: 71.400% \n",
      "[Epoch:10, Iter:941] Loss: 0.546 | Acc: 71.512% \n",
      "[Epoch:10, Iter:942] Loss: 0.546 | Acc: 71.667% \n",
      "[Epoch:10, Iter:943] Loss: 0.547 | Acc: 71.628% \n",
      "[Epoch:10, Iter:944] Loss: 0.549 | Acc: 71.545% \n",
      "[Epoch:10, Iter:945] Loss: 0.549 | Acc: 71.556% \n",
      "[Epoch:10, Iter:946] Loss: 0.546 | Acc: 71.826% \n",
      "[Epoch:10, Iter:947] Loss: 0.550 | Acc: 71.617% \n",
      "[Epoch:10, Iter:948] Loss: 0.551 | Acc: 71.375% \n",
      "[Epoch:10, Iter:949] Loss: 0.551 | Acc: 71.388% \n",
      "[Epoch:10, Iter:950] Loss: 0.551 | Acc: 71.360% \n",
      "[Epoch:10, Iter:951] Loss: 0.551 | Acc: 71.451% \n",
      "[Epoch:10, Iter:952] Loss: 0.551 | Acc: 71.462% \n",
      "[Epoch:10, Iter:953] Loss: 0.551 | Acc: 71.547% \n",
      "[Epoch:10, Iter:954] Loss: 0.550 | Acc: 71.593% \n",
      "[Epoch:10, Iter:955] Loss: 0.548 | Acc: 71.818% \n",
      "[Epoch:10, Iter:956] Loss: 0.549 | Acc: 71.750% \n",
      "[Epoch:10, Iter:957] Loss: 0.551 | Acc: 71.579% \n",
      "[Epoch:10, Iter:958] Loss: 0.551 | Acc: 71.621% \n",
      "[Epoch:10, Iter:959] Loss: 0.548 | Acc: 71.831% \n",
      "[Epoch:10, Iter:960] Loss: 0.549 | Acc: 71.733% \n",
      "[Epoch:10, Iter:961] Loss: 0.548 | Acc: 71.869% \n",
      "[Epoch:10, Iter:962] Loss: 0.549 | Acc: 71.806% \n",
      "[Epoch:10, Iter:963] Loss: 0.549 | Acc: 71.873% \n",
      "[Epoch:10, Iter:964] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:10, Iter:965] Loss: 0.548 | Acc: 72.062% \n",
      "[Epoch:10, Iter:966] Loss: 0.550 | Acc: 71.879% \n",
      "[Epoch:10, Iter:967] Loss: 0.550 | Acc: 72.000% \n",
      "[Epoch:10, Iter:968] Loss: 0.551 | Acc: 71.941% \n",
      "[Epoch:10, Iter:969] Loss: 0.551 | Acc: 71.942% \n",
      "[Epoch:10, Iter:970] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:10, Iter:971] Loss: 0.550 | Acc: 72.028% \n",
      "[Epoch:10, Iter:972] Loss: 0.549 | Acc: 72.028% \n",
      "[Epoch:10, Iter:973] Loss: 0.551 | Acc: 71.973% \n",
      "[Epoch:10, Iter:974] Loss: 0.550 | Acc: 72.081% \n",
      "[Epoch:10, Iter:975] Loss: 0.550 | Acc: 72.053% \n",
      "[Epoch:10, Iter:976] Loss: 0.548 | Acc: 72.184% \n",
      "[Epoch:10, Iter:977] Loss: 0.549 | Acc: 72.078% \n",
      "[Epoch:10, Iter:978] Loss: 0.549 | Acc: 72.077% \n",
      "[Epoch:10, Iter:979] Loss: 0.547 | Acc: 72.203% \n",
      "[Epoch:10, Iter:980] Loss: 0.548 | Acc: 72.125% \n",
      "[Epoch:10, Iter:981] Loss: 0.547 | Acc: 72.173% \n",
      "[Epoch:10, Iter:982] Loss: 0.547 | Acc: 72.244% \n",
      "[Epoch:10, Iter:983] Loss: 0.547 | Acc: 72.145% \n",
      "[Epoch:10, Iter:984] Loss: 0.546 | Acc: 72.214% \n",
      "[Epoch:10, Iter:985] Loss: 0.547 | Acc: 72.282% \n",
      "[Epoch:10, Iter:986] Loss: 0.547 | Acc: 72.279% \n",
      "[Epoch:10, Iter:987] Loss: 0.546 | Acc: 72.391% \n",
      "[Epoch:10, Iter:988] Loss: 0.546 | Acc: 72.386% \n",
      "[Epoch:10, Iter:989] Loss: 0.546 | Acc: 72.449% \n",
      "[Epoch:10, Iter:990] Loss: 0.547 | Acc: 72.356% \n",
      "[Epoch:10, Iter:991] Loss: 0.546 | Acc: 72.440% \n",
      "[Epoch:10, Iter:992] Loss: 0.546 | Acc: 72.522% \n",
      "[Epoch:10, Iter:993] Loss: 0.546 | Acc: 72.581% \n",
      "[Epoch:10, Iter:994] Loss: 0.546 | Acc: 72.553% \n",
      "[Epoch:10, Iter:995] Loss: 0.545 | Acc: 72.568% \n",
      "[Epoch:10, Iter:996] Loss: 0.544 | Acc: 72.667% \n",
      "[Epoch:10, Iter:997] Loss: 0.544 | Acc: 72.722% \n",
      "[Epoch:10, Iter:998] Loss: 0.543 | Acc: 72.735% \n",
      "[Epoch:10, Iter:999] Loss: 0.542 | Acc: 72.848% \n",
      "[Epoch:10, Iter:1000] Loss: 0.543 | Acc: 72.780% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.900%\n",
      "Training set's accuracy (after quantization) is: 71.960%\n",
      "Test set's accuracy (before quantization) is: 72.400%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.540 | Train Acc: 72.900% | Test Loss: 0.545 | Test Acc: 72.400% \n",
      "Quantized Train Loss: 0.550 | Quantized Train Acc: 71.960% | Quantized Test Loss: 0.552 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 11\n",
      "[Epoch:11, Iter:1001] Loss: 0.449 | Acc: 80.000% \n",
      "[Epoch:11, Iter:1002] Loss: 0.496 | Acc: 76.000% \n",
      "[Epoch:11, Iter:1003] Loss: 0.532 | Acc: 71.333% \n",
      "[Epoch:11, Iter:1004] Loss: 0.541 | Acc: 71.000% \n",
      "[Epoch:11, Iter:1005] Loss: 0.548 | Acc: 72.400% \n",
      "[Epoch:11, Iter:1006] Loss: 0.544 | Acc: 71.667% \n",
      "[Epoch:11, Iter:1007] Loss: 0.555 | Acc: 71.429% \n",
      "[Epoch:11, Iter:1008] Loss: 0.565 | Acc: 71.500% \n",
      "[Epoch:11, Iter:1009] Loss: 0.553 | Acc: 72.444% \n",
      "[Epoch:11, Iter:1010] Loss: 0.547 | Acc: 72.800% \n",
      "[Epoch:11, Iter:1011] Loss: 0.543 | Acc: 72.909% \n",
      "[Epoch:11, Iter:1012] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:11, Iter:1013] Loss: 0.545 | Acc: 73.077% \n",
      "[Epoch:11, Iter:1014] Loss: 0.545 | Acc: 73.000% \n",
      "[Epoch:11, Iter:1015] Loss: 0.538 | Acc: 73.333% \n",
      "[Epoch:11, Iter:1016] Loss: 0.544 | Acc: 73.000% \n",
      "[Epoch:11, Iter:1017] Loss: 0.542 | Acc: 73.059% \n",
      "[Epoch:11, Iter:1018] Loss: 0.542 | Acc: 72.889% \n",
      "[Epoch:11, Iter:1019] Loss: 0.539 | Acc: 73.053% \n",
      "[Epoch:11, Iter:1020] Loss: 0.541 | Acc: 73.000% \n",
      "[Epoch:11, Iter:1021] Loss: 0.539 | Acc: 73.048% \n",
      "[Epoch:11, Iter:1022] Loss: 0.533 | Acc: 73.273% \n",
      "[Epoch:11, Iter:1023] Loss: 0.538 | Acc: 73.043% \n",
      "[Epoch:11, Iter:1024] Loss: 0.535 | Acc: 73.083% \n",
      "[Epoch:11, Iter:1025] Loss: 0.538 | Acc: 72.960% \n",
      "[Epoch:11, Iter:1026] Loss: 0.536 | Acc: 73.077% \n",
      "[Epoch:11, Iter:1027] Loss: 0.534 | Acc: 73.185% \n",
      "[Epoch:11, Iter:1028] Loss: 0.532 | Acc: 73.429% \n",
      "[Epoch:11, Iter:1029] Loss: 0.532 | Acc: 73.310% \n",
      "[Epoch:11, Iter:1030] Loss: 0.536 | Acc: 72.867% \n",
      "[Epoch:11, Iter:1031] Loss: 0.540 | Acc: 72.387% \n",
      "[Epoch:11, Iter:1032] Loss: 0.540 | Acc: 72.312% \n",
      "[Epoch:11, Iter:1033] Loss: 0.541 | Acc: 72.606% \n",
      "[Epoch:11, Iter:1034] Loss: 0.542 | Acc: 72.647% \n",
      "[Epoch:11, Iter:1035] Loss: 0.543 | Acc: 72.400% \n",
      "[Epoch:11, Iter:1036] Loss: 0.544 | Acc: 72.222% \n",
      "[Epoch:11, Iter:1037] Loss: 0.544 | Acc: 72.216% \n",
      "[Epoch:11, Iter:1038] Loss: 0.543 | Acc: 72.316% \n",
      "[Epoch:11, Iter:1039] Loss: 0.544 | Acc: 72.205% \n",
      "[Epoch:11, Iter:1040] Loss: 0.543 | Acc: 72.250% \n",
      "[Epoch:11, Iter:1041] Loss: 0.543 | Acc: 72.195% \n",
      "[Epoch:11, Iter:1042] Loss: 0.542 | Acc: 72.286% \n",
      "[Epoch:11, Iter:1043] Loss: 0.544 | Acc: 72.326% \n",
      "[Epoch:11, Iter:1044] Loss: 0.547 | Acc: 72.136% \n",
      "[Epoch:11, Iter:1045] Loss: 0.545 | Acc: 72.356% \n",
      "[Epoch:11, Iter:1046] Loss: 0.545 | Acc: 72.391% \n",
      "[Epoch:11, Iter:1047] Loss: 0.546 | Acc: 72.383% \n",
      "[Epoch:11, Iter:1048] Loss: 0.545 | Acc: 72.583% \n",
      "[Epoch:11, Iter:1049] Loss: 0.545 | Acc: 72.612% \n",
      "[Epoch:11, Iter:1050] Loss: 0.545 | Acc: 72.680% \n",
      "[Epoch:11, Iter:1051] Loss: 0.545 | Acc: 72.627% \n",
      "[Epoch:11, Iter:1052] Loss: 0.547 | Acc: 72.500% \n",
      "[Epoch:11, Iter:1053] Loss: 0.548 | Acc: 72.491% \n",
      "[Epoch:11, Iter:1054] Loss: 0.548 | Acc: 72.556% \n",
      "[Epoch:11, Iter:1055] Loss: 0.548 | Acc: 72.655% \n",
      "[Epoch:11, Iter:1056] Loss: 0.547 | Acc: 72.750% \n",
      "[Epoch:11, Iter:1057] Loss: 0.547 | Acc: 72.772% \n",
      "[Epoch:11, Iter:1058] Loss: 0.547 | Acc: 72.793% \n",
      "[Epoch:11, Iter:1059] Loss: 0.545 | Acc: 72.949% \n",
      "[Epoch:11, Iter:1060] Loss: 0.543 | Acc: 73.133% \n",
      "[Epoch:11, Iter:1061] Loss: 0.541 | Acc: 73.246% \n",
      "[Epoch:11, Iter:1062] Loss: 0.541 | Acc: 73.129% \n",
      "[Epoch:11, Iter:1063] Loss: 0.540 | Acc: 73.079% \n",
      "[Epoch:11, Iter:1064] Loss: 0.540 | Acc: 73.094% \n",
      "[Epoch:11, Iter:1065] Loss: 0.540 | Acc: 73.015% \n",
      "[Epoch:11, Iter:1066] Loss: 0.540 | Acc: 73.030% \n",
      "[Epoch:11, Iter:1067] Loss: 0.540 | Acc: 73.015% \n",
      "[Epoch:11, Iter:1068] Loss: 0.539 | Acc: 73.000% \n",
      "[Epoch:11, Iter:1069] Loss: 0.539 | Acc: 73.072% \n",
      "[Epoch:11, Iter:1070] Loss: 0.538 | Acc: 73.114% \n",
      "[Epoch:11, Iter:1071] Loss: 0.538 | Acc: 73.099% \n",
      "[Epoch:11, Iter:1072] Loss: 0.537 | Acc: 73.000% \n",
      "[Epoch:11, Iter:1073] Loss: 0.537 | Acc: 73.123% \n",
      "[Epoch:11, Iter:1074] Loss: 0.536 | Acc: 73.135% \n",
      "[Epoch:11, Iter:1075] Loss: 0.536 | Acc: 73.227% \n",
      "[Epoch:11, Iter:1076] Loss: 0.537 | Acc: 73.184% \n",
      "[Epoch:11, Iter:1077] Loss: 0.538 | Acc: 73.065% \n",
      "[Epoch:11, Iter:1078] Loss: 0.538 | Acc: 73.077% \n",
      "[Epoch:11, Iter:1079] Loss: 0.540 | Acc: 73.089% \n",
      "[Epoch:11, Iter:1080] Loss: 0.540 | Acc: 73.000% \n",
      "[Epoch:11, Iter:1081] Loss: 0.542 | Acc: 72.741% \n",
      "[Epoch:11, Iter:1082] Loss: 0.541 | Acc: 72.805% \n",
      "[Epoch:11, Iter:1083] Loss: 0.542 | Acc: 72.651% \n",
      "[Epoch:11, Iter:1084] Loss: 0.543 | Acc: 72.595% \n",
      "[Epoch:11, Iter:1085] Loss: 0.543 | Acc: 72.518% \n",
      "[Epoch:11, Iter:1086] Loss: 0.544 | Acc: 72.326% \n",
      "[Epoch:11, Iter:1087] Loss: 0.544 | Acc: 72.207% \n",
      "[Epoch:11, Iter:1088] Loss: 0.544 | Acc: 72.318% \n",
      "[Epoch:11, Iter:1089] Loss: 0.544 | Acc: 72.292% \n",
      "[Epoch:11, Iter:1090] Loss: 0.544 | Acc: 72.244% \n",
      "[Epoch:11, Iter:1091] Loss: 0.544 | Acc: 72.242% \n",
      "[Epoch:11, Iter:1092] Loss: 0.545 | Acc: 72.196% \n",
      "[Epoch:11, Iter:1093] Loss: 0.544 | Acc: 72.301% \n",
      "[Epoch:11, Iter:1094] Loss: 0.545 | Acc: 72.213% \n",
      "[Epoch:11, Iter:1095] Loss: 0.545 | Acc: 72.232% \n",
      "[Epoch:11, Iter:1096] Loss: 0.544 | Acc: 72.271% \n",
      "[Epoch:11, Iter:1097] Loss: 0.544 | Acc: 72.227% \n",
      "[Epoch:11, Iter:1098] Loss: 0.545 | Acc: 72.224% \n",
      "[Epoch:11, Iter:1099] Loss: 0.545 | Acc: 72.263% \n",
      "[Epoch:11, Iter:1100] Loss: 0.544 | Acc: 72.300% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.100%\n",
      "Training set's accuracy (after quantization) is: 72.140%\n",
      "Test set's accuracy (before quantization) is: 72.600%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.541 | Train Acc: 73.100% | Test Loss: 0.547 | Test Acc: 72.600% \n",
      "Quantized Train Loss: 0.549 | Quantized Train Acc: 72.140% | Quantized Test Loss: 0.551 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 12\n",
      "[Epoch:12, Iter:1101] Loss: 0.476 | Acc: 80.000% \n",
      "[Epoch:12, Iter:1102] Loss: 0.498 | Acc: 73.000% \n",
      "[Epoch:12, Iter:1103] Loss: 0.496 | Acc: 74.000% \n",
      "[Epoch:12, Iter:1104] Loss: 0.470 | Acc: 76.000% \n",
      "[Epoch:12, Iter:1105] Loss: 0.488 | Acc: 75.600% \n",
      "[Epoch:12, Iter:1106] Loss: 0.527 | Acc: 73.333% \n",
      "[Epoch:12, Iter:1107] Loss: 0.537 | Acc: 72.571% \n",
      "[Epoch:12, Iter:1108] Loss: 0.542 | Acc: 71.250% \n",
      "[Epoch:12, Iter:1109] Loss: 0.558 | Acc: 69.556% \n",
      "[Epoch:12, Iter:1110] Loss: 0.556 | Acc: 69.800% \n",
      "[Epoch:12, Iter:1111] Loss: 0.561 | Acc: 69.091% \n",
      "[Epoch:12, Iter:1112] Loss: 0.553 | Acc: 69.667% \n",
      "[Epoch:12, Iter:1113] Loss: 0.552 | Acc: 69.692% \n",
      "[Epoch:12, Iter:1114] Loss: 0.544 | Acc: 70.286% \n",
      "[Epoch:12, Iter:1115] Loss: 0.541 | Acc: 70.667% \n",
      "[Epoch:12, Iter:1116] Loss: 0.542 | Acc: 70.750% \n",
      "[Epoch:12, Iter:1117] Loss: 0.540 | Acc: 71.059% \n",
      "[Epoch:12, Iter:1118] Loss: 0.550 | Acc: 70.556% \n",
      "[Epoch:12, Iter:1119] Loss: 0.554 | Acc: 70.632% \n",
      "[Epoch:12, Iter:1120] Loss: 0.552 | Acc: 70.800% \n",
      "[Epoch:12, Iter:1121] Loss: 0.551 | Acc: 70.762% \n",
      "[Epoch:12, Iter:1122] Loss: 0.547 | Acc: 71.182% \n",
      "[Epoch:12, Iter:1123] Loss: 0.547 | Acc: 71.130% \n",
      "[Epoch:12, Iter:1124] Loss: 0.548 | Acc: 71.083% \n",
      "[Epoch:12, Iter:1125] Loss: 0.548 | Acc: 71.120% \n",
      "[Epoch:12, Iter:1126] Loss: 0.545 | Acc: 71.385% \n",
      "[Epoch:12, Iter:1127] Loss: 0.547 | Acc: 71.407% \n",
      "[Epoch:12, Iter:1128] Loss: 0.550 | Acc: 71.000% \n",
      "[Epoch:12, Iter:1129] Loss: 0.550 | Acc: 71.310% \n",
      "[Epoch:12, Iter:1130] Loss: 0.547 | Acc: 71.667% \n",
      "[Epoch:12, Iter:1131] Loss: 0.545 | Acc: 71.806% \n",
      "[Epoch:12, Iter:1132] Loss: 0.543 | Acc: 71.875% \n",
      "[Epoch:12, Iter:1133] Loss: 0.541 | Acc: 72.061% \n",
      "[Epoch:12, Iter:1134] Loss: 0.542 | Acc: 72.176% \n",
      "[Epoch:12, Iter:1135] Loss: 0.541 | Acc: 72.229% \n",
      "[Epoch:12, Iter:1136] Loss: 0.540 | Acc: 72.278% \n",
      "[Epoch:12, Iter:1137] Loss: 0.540 | Acc: 72.324% \n",
      "[Epoch:12, Iter:1138] Loss: 0.541 | Acc: 72.368% \n",
      "[Epoch:12, Iter:1139] Loss: 0.542 | Acc: 72.462% \n",
      "[Epoch:12, Iter:1140] Loss: 0.544 | Acc: 72.400% \n",
      "[Epoch:12, Iter:1141] Loss: 0.546 | Acc: 72.195% \n",
      "[Epoch:12, Iter:1142] Loss: 0.546 | Acc: 72.095% \n",
      "[Epoch:12, Iter:1143] Loss: 0.546 | Acc: 72.140% \n",
      "[Epoch:12, Iter:1144] Loss: 0.547 | Acc: 72.000% \n",
      "[Epoch:12, Iter:1145] Loss: 0.547 | Acc: 72.089% \n",
      "[Epoch:12, Iter:1146] Loss: 0.546 | Acc: 72.130% \n",
      "[Epoch:12, Iter:1147] Loss: 0.547 | Acc: 72.043% \n",
      "[Epoch:12, Iter:1148] Loss: 0.546 | Acc: 72.083% \n",
      "[Epoch:12, Iter:1149] Loss: 0.546 | Acc: 72.204% \n",
      "[Epoch:12, Iter:1150] Loss: 0.544 | Acc: 72.320% \n",
      "[Epoch:12, Iter:1151] Loss: 0.544 | Acc: 72.431% \n",
      "[Epoch:12, Iter:1152] Loss: 0.543 | Acc: 72.615% \n",
      "[Epoch:12, Iter:1153] Loss: 0.542 | Acc: 72.642% \n",
      "[Epoch:12, Iter:1154] Loss: 0.544 | Acc: 72.593% \n",
      "[Epoch:12, Iter:1155] Loss: 0.544 | Acc: 72.618% \n",
      "[Epoch:12, Iter:1156] Loss: 0.543 | Acc: 72.679% \n",
      "[Epoch:12, Iter:1157] Loss: 0.542 | Acc: 72.667% \n",
      "[Epoch:12, Iter:1158] Loss: 0.544 | Acc: 72.586% \n",
      "[Epoch:12, Iter:1159] Loss: 0.544 | Acc: 72.475% \n",
      "[Epoch:12, Iter:1160] Loss: 0.544 | Acc: 72.400% \n",
      "[Epoch:12, Iter:1161] Loss: 0.542 | Acc: 72.492% \n",
      "[Epoch:12, Iter:1162] Loss: 0.541 | Acc: 72.613% \n",
      "[Epoch:12, Iter:1163] Loss: 0.539 | Acc: 72.730% \n",
      "[Epoch:12, Iter:1164] Loss: 0.538 | Acc: 72.781% \n",
      "[Epoch:12, Iter:1165] Loss: 0.540 | Acc: 72.615% \n",
      "[Epoch:12, Iter:1166] Loss: 0.541 | Acc: 72.515% \n",
      "[Epoch:12, Iter:1167] Loss: 0.540 | Acc: 72.597% \n",
      "[Epoch:12, Iter:1168] Loss: 0.540 | Acc: 72.588% \n",
      "[Epoch:12, Iter:1169] Loss: 0.540 | Acc: 72.580% \n",
      "[Epoch:12, Iter:1170] Loss: 0.538 | Acc: 72.657% \n",
      "[Epoch:12, Iter:1171] Loss: 0.539 | Acc: 72.563% \n",
      "[Epoch:12, Iter:1172] Loss: 0.540 | Acc: 72.444% \n",
      "[Epoch:12, Iter:1173] Loss: 0.541 | Acc: 72.384% \n",
      "[Epoch:12, Iter:1174] Loss: 0.540 | Acc: 72.432% \n",
      "[Epoch:12, Iter:1175] Loss: 0.541 | Acc: 72.373% \n",
      "[Epoch:12, Iter:1176] Loss: 0.541 | Acc: 72.342% \n",
      "[Epoch:12, Iter:1177] Loss: 0.540 | Acc: 72.442% \n",
      "[Epoch:12, Iter:1178] Loss: 0.540 | Acc: 72.462% \n",
      "[Epoch:12, Iter:1179] Loss: 0.540 | Acc: 72.506% \n",
      "[Epoch:12, Iter:1180] Loss: 0.540 | Acc: 72.525% \n",
      "[Epoch:12, Iter:1181] Loss: 0.539 | Acc: 72.642% \n",
      "[Epoch:12, Iter:1182] Loss: 0.538 | Acc: 72.780% \n",
      "[Epoch:12, Iter:1183] Loss: 0.539 | Acc: 72.723% \n",
      "[Epoch:12, Iter:1184] Loss: 0.541 | Acc: 72.548% \n",
      "[Epoch:12, Iter:1185] Loss: 0.543 | Acc: 72.376% \n",
      "[Epoch:12, Iter:1186] Loss: 0.544 | Acc: 72.209% \n",
      "[Epoch:12, Iter:1187] Loss: 0.543 | Acc: 72.322% \n",
      "[Epoch:12, Iter:1188] Loss: 0.544 | Acc: 72.273% \n",
      "[Epoch:12, Iter:1189] Loss: 0.545 | Acc: 72.202% \n",
      "[Epoch:12, Iter:1190] Loss: 0.545 | Acc: 72.200% \n",
      "[Epoch:12, Iter:1191] Loss: 0.544 | Acc: 72.242% \n",
      "[Epoch:12, Iter:1192] Loss: 0.543 | Acc: 72.283% \n",
      "[Epoch:12, Iter:1193] Loss: 0.543 | Acc: 72.323% \n",
      "[Epoch:12, Iter:1194] Loss: 0.544 | Acc: 72.234% \n",
      "[Epoch:12, Iter:1195] Loss: 0.544 | Acc: 72.358% \n",
      "[Epoch:12, Iter:1196] Loss: 0.543 | Acc: 72.396% \n",
      "[Epoch:12, Iter:1197] Loss: 0.544 | Acc: 72.371% \n",
      "[Epoch:12, Iter:1198] Loss: 0.544 | Acc: 72.367% \n",
      "[Epoch:12, Iter:1199] Loss: 0.544 | Acc: 72.343% \n",
      "[Epoch:12, Iter:1200] Loss: 0.545 | Acc: 72.260% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 71.000%\n",
      "Training set's accuracy (after quantization) is: 71.740%\n",
      "Test set's accuracy (before quantization) is: 70.200%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.561 | Train Acc: 71.000% | Test Loss: 0.564 | Test Acc: 70.200% \n",
      "Quantized Train Loss: 0.559 | Quantized Train Acc: 71.740% | Quantized Test Loss: 0.562 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 13\n",
      "[Epoch:13, Iter:1201] Loss: 0.567 | Acc: 70.000% \n",
      "[Epoch:13, Iter:1202] Loss: 0.564 | Acc: 70.000% \n",
      "[Epoch:13, Iter:1203] Loss: 0.558 | Acc: 69.333% \n",
      "[Epoch:13, Iter:1204] Loss: 0.564 | Acc: 70.000% \n",
      "[Epoch:13, Iter:1205] Loss: 0.562 | Acc: 71.200% \n",
      "[Epoch:13, Iter:1206] Loss: 0.574 | Acc: 71.000% \n",
      "[Epoch:13, Iter:1207] Loss: 0.555 | Acc: 72.857% \n",
      "[Epoch:13, Iter:1208] Loss: 0.584 | Acc: 70.500% \n",
      "[Epoch:13, Iter:1209] Loss: 0.575 | Acc: 70.222% \n",
      "[Epoch:13, Iter:1210] Loss: 0.561 | Acc: 71.200% \n",
      "[Epoch:13, Iter:1211] Loss: 0.565 | Acc: 70.727% \n",
      "[Epoch:13, Iter:1212] Loss: 0.572 | Acc: 70.000% \n",
      "[Epoch:13, Iter:1213] Loss: 0.569 | Acc: 70.154% \n",
      "[Epoch:13, Iter:1214] Loss: 0.563 | Acc: 70.571% \n",
      "[Epoch:13, Iter:1215] Loss: 0.563 | Acc: 70.533% \n",
      "[Epoch:13, Iter:1216] Loss: 0.561 | Acc: 70.875% \n",
      "[Epoch:13, Iter:1217] Loss: 0.565 | Acc: 70.235% \n",
      "[Epoch:13, Iter:1218] Loss: 0.568 | Acc: 70.000% \n",
      "[Epoch:13, Iter:1219] Loss: 0.569 | Acc: 69.895% \n",
      "[Epoch:13, Iter:1220] Loss: 0.564 | Acc: 70.500% \n",
      "[Epoch:13, Iter:1221] Loss: 0.562 | Acc: 70.667% \n",
      "[Epoch:13, Iter:1222] Loss: 0.564 | Acc: 70.273% \n",
      "[Epoch:13, Iter:1223] Loss: 0.561 | Acc: 70.522% \n",
      "[Epoch:13, Iter:1224] Loss: 0.558 | Acc: 70.750% \n",
      "[Epoch:13, Iter:1225] Loss: 0.557 | Acc: 70.800% \n",
      "[Epoch:13, Iter:1226] Loss: 0.554 | Acc: 71.077% \n",
      "[Epoch:13, Iter:1227] Loss: 0.555 | Acc: 70.963% \n",
      "[Epoch:13, Iter:1228] Loss: 0.557 | Acc: 70.786% \n",
      "[Epoch:13, Iter:1229] Loss: 0.554 | Acc: 70.966% \n",
      "[Epoch:13, Iter:1230] Loss: 0.553 | Acc: 71.000% \n",
      "[Epoch:13, Iter:1231] Loss: 0.550 | Acc: 71.161% \n",
      "[Epoch:13, Iter:1232] Loss: 0.550 | Acc: 71.062% \n",
      "[Epoch:13, Iter:1233] Loss: 0.551 | Acc: 71.030% \n",
      "[Epoch:13, Iter:1234] Loss: 0.554 | Acc: 70.647% \n",
      "[Epoch:13, Iter:1235] Loss: 0.554 | Acc: 70.571% \n",
      "[Epoch:13, Iter:1236] Loss: 0.552 | Acc: 70.611% \n",
      "[Epoch:13, Iter:1237] Loss: 0.550 | Acc: 70.649% \n",
      "[Epoch:13, Iter:1238] Loss: 0.550 | Acc: 70.474% \n",
      "[Epoch:13, Iter:1239] Loss: 0.550 | Acc: 70.513% \n",
      "[Epoch:13, Iter:1240] Loss: 0.548 | Acc: 70.650% \n",
      "[Epoch:13, Iter:1241] Loss: 0.548 | Acc: 70.780% \n",
      "[Epoch:13, Iter:1242] Loss: 0.550 | Acc: 70.619% \n",
      "[Epoch:13, Iter:1243] Loss: 0.549 | Acc: 70.744% \n",
      "[Epoch:13, Iter:1244] Loss: 0.550 | Acc: 70.818% \n",
      "[Epoch:13, Iter:1245] Loss: 0.550 | Acc: 70.889% \n",
      "[Epoch:13, Iter:1246] Loss: 0.549 | Acc: 70.957% \n",
      "[Epoch:13, Iter:1247] Loss: 0.548 | Acc: 70.979% \n",
      "[Epoch:13, Iter:1248] Loss: 0.548 | Acc: 71.000% \n",
      "[Epoch:13, Iter:1249] Loss: 0.547 | Acc: 70.980% \n",
      "[Epoch:13, Iter:1250] Loss: 0.547 | Acc: 71.080% \n",
      "[Epoch:13, Iter:1251] Loss: 0.549 | Acc: 70.902% \n",
      "[Epoch:13, Iter:1252] Loss: 0.548 | Acc: 71.038% \n",
      "[Epoch:13, Iter:1253] Loss: 0.547 | Acc: 71.132% \n",
      "[Epoch:13, Iter:1254] Loss: 0.547 | Acc: 71.222% \n",
      "[Epoch:13, Iter:1255] Loss: 0.547 | Acc: 71.236% \n",
      "[Epoch:13, Iter:1256] Loss: 0.546 | Acc: 71.464% \n",
      "[Epoch:13, Iter:1257] Loss: 0.546 | Acc: 71.439% \n",
      "[Epoch:13, Iter:1258] Loss: 0.544 | Acc: 71.517% \n",
      "[Epoch:13, Iter:1259] Loss: 0.545 | Acc: 71.492% \n",
      "[Epoch:13, Iter:1260] Loss: 0.544 | Acc: 71.567% \n",
      "[Epoch:13, Iter:1261] Loss: 0.544 | Acc: 71.377% \n",
      "[Epoch:13, Iter:1262] Loss: 0.545 | Acc: 71.355% \n",
      "[Epoch:13, Iter:1263] Loss: 0.545 | Acc: 71.365% \n",
      "[Epoch:13, Iter:1264] Loss: 0.544 | Acc: 71.469% \n",
      "[Epoch:13, Iter:1265] Loss: 0.544 | Acc: 71.569% \n",
      "[Epoch:13, Iter:1266] Loss: 0.544 | Acc: 71.667% \n",
      "[Epoch:13, Iter:1267] Loss: 0.547 | Acc: 71.433% \n",
      "[Epoch:13, Iter:1268] Loss: 0.546 | Acc: 71.559% \n",
      "[Epoch:13, Iter:1269] Loss: 0.547 | Acc: 71.420% \n",
      "[Epoch:13, Iter:1270] Loss: 0.545 | Acc: 71.514% \n",
      "[Epoch:13, Iter:1271] Loss: 0.546 | Acc: 71.493% \n",
      "[Epoch:13, Iter:1272] Loss: 0.548 | Acc: 71.472% \n",
      "[Epoch:13, Iter:1273] Loss: 0.548 | Acc: 71.479% \n",
      "[Epoch:13, Iter:1274] Loss: 0.548 | Acc: 71.459% \n",
      "[Epoch:13, Iter:1275] Loss: 0.549 | Acc: 71.387% \n",
      "[Epoch:13, Iter:1276] Loss: 0.550 | Acc: 71.447% \n",
      "[Epoch:13, Iter:1277] Loss: 0.549 | Acc: 71.377% \n",
      "[Epoch:13, Iter:1278] Loss: 0.548 | Acc: 71.513% \n",
      "[Epoch:13, Iter:1279] Loss: 0.547 | Acc: 71.519% \n",
      "[Epoch:13, Iter:1280] Loss: 0.548 | Acc: 71.500% \n",
      "[Epoch:13, Iter:1281] Loss: 0.547 | Acc: 71.605% \n",
      "[Epoch:13, Iter:1282] Loss: 0.548 | Acc: 71.659% \n",
      "[Epoch:13, Iter:1283] Loss: 0.549 | Acc: 71.590% \n",
      "[Epoch:13, Iter:1284] Loss: 0.548 | Acc: 71.619% \n",
      "[Epoch:13, Iter:1285] Loss: 0.548 | Acc: 71.694% \n",
      "[Epoch:13, Iter:1286] Loss: 0.547 | Acc: 71.674% \n",
      "[Epoch:13, Iter:1287] Loss: 0.546 | Acc: 71.862% \n",
      "[Epoch:13, Iter:1288] Loss: 0.546 | Acc: 71.841% \n",
      "[Epoch:13, Iter:1289] Loss: 0.545 | Acc: 71.888% \n",
      "[Epoch:13, Iter:1290] Loss: 0.544 | Acc: 71.911% \n",
      "[Epoch:13, Iter:1291] Loss: 0.544 | Acc: 72.022% \n",
      "[Epoch:13, Iter:1292] Loss: 0.545 | Acc: 71.978% \n",
      "[Epoch:13, Iter:1293] Loss: 0.545 | Acc: 72.022% \n",
      "[Epoch:13, Iter:1294] Loss: 0.546 | Acc: 72.043% \n",
      "[Epoch:13, Iter:1295] Loss: 0.545 | Acc: 72.189% \n",
      "[Epoch:13, Iter:1296] Loss: 0.545 | Acc: 72.188% \n",
      "[Epoch:13, Iter:1297] Loss: 0.546 | Acc: 72.082% \n",
      "[Epoch:13, Iter:1298] Loss: 0.547 | Acc: 72.082% \n",
      "[Epoch:13, Iter:1299] Loss: 0.547 | Acc: 72.121% \n",
      "[Epoch:13, Iter:1300] Loss: 0.546 | Acc: 72.120% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.900%\n",
      "Training set's accuracy (after quantization) is: 71.840%\n",
      "Test set's accuracy (before quantization) is: 72.100%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.540 | Train Acc: 72.900% | Test Loss: 0.544 | Test Acc: 72.100% \n",
      "Quantized Train Loss: 0.551 | Quantized Train Acc: 71.840% | Quantized Test Loss: 0.554 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 14\n",
      "[Epoch:14, Iter:1301] Loss: 0.607 | Acc: 68.000% \n",
      "[Epoch:14, Iter:1302] Loss: 0.575 | Acc: 68.000% \n",
      "[Epoch:14, Iter:1303] Loss: 0.572 | Acc: 69.333% \n",
      "[Epoch:14, Iter:1304] Loss: 0.548 | Acc: 72.500% \n",
      "[Epoch:14, Iter:1305] Loss: 0.527 | Acc: 74.000% \n",
      "[Epoch:14, Iter:1306] Loss: 0.514 | Acc: 75.000% \n",
      "[Epoch:14, Iter:1307] Loss: 0.514 | Acc: 74.857% \n",
      "[Epoch:14, Iter:1308] Loss: 0.511 | Acc: 75.000% \n",
      "[Epoch:14, Iter:1309] Loss: 0.504 | Acc: 75.333% \n",
      "[Epoch:14, Iter:1310] Loss: 0.507 | Acc: 75.400% \n",
      "[Epoch:14, Iter:1311] Loss: 0.519 | Acc: 74.545% \n",
      "[Epoch:14, Iter:1312] Loss: 0.525 | Acc: 74.500% \n",
      "[Epoch:14, Iter:1313] Loss: 0.533 | Acc: 73.692% \n",
      "[Epoch:14, Iter:1314] Loss: 0.533 | Acc: 73.286% \n",
      "[Epoch:14, Iter:1315] Loss: 0.531 | Acc: 73.333% \n",
      "[Epoch:14, Iter:1316] Loss: 0.536 | Acc: 73.500% \n",
      "[Epoch:14, Iter:1317] Loss: 0.536 | Acc: 73.882% \n",
      "[Epoch:14, Iter:1318] Loss: 0.547 | Acc: 73.000% \n",
      "[Epoch:14, Iter:1319] Loss: 0.551 | Acc: 72.737% \n",
      "[Epoch:14, Iter:1320] Loss: 0.547 | Acc: 73.000% \n",
      "[Epoch:14, Iter:1321] Loss: 0.557 | Acc: 72.476% \n",
      "[Epoch:14, Iter:1322] Loss: 0.557 | Acc: 72.636% \n",
      "[Epoch:14, Iter:1323] Loss: 0.554 | Acc: 72.783% \n",
      "[Epoch:14, Iter:1324] Loss: 0.559 | Acc: 72.417% \n",
      "[Epoch:14, Iter:1325] Loss: 0.557 | Acc: 72.720% \n",
      "[Epoch:14, Iter:1326] Loss: 0.552 | Acc: 73.077% \n",
      "[Epoch:14, Iter:1327] Loss: 0.556 | Acc: 72.741% \n",
      "[Epoch:14, Iter:1328] Loss: 0.556 | Acc: 72.500% \n",
      "[Epoch:14, Iter:1329] Loss: 0.560 | Acc: 72.207% \n",
      "[Epoch:14, Iter:1330] Loss: 0.555 | Acc: 72.533% \n",
      "[Epoch:14, Iter:1331] Loss: 0.560 | Acc: 72.387% \n",
      "[Epoch:14, Iter:1332] Loss: 0.556 | Acc: 72.562% \n",
      "[Epoch:14, Iter:1333] Loss: 0.553 | Acc: 72.848% \n",
      "[Epoch:14, Iter:1334] Loss: 0.555 | Acc: 72.647% \n",
      "[Epoch:14, Iter:1335] Loss: 0.557 | Acc: 72.457% \n",
      "[Epoch:14, Iter:1336] Loss: 0.555 | Acc: 72.444% \n",
      "[Epoch:14, Iter:1337] Loss: 0.553 | Acc: 72.432% \n",
      "[Epoch:14, Iter:1338] Loss: 0.553 | Acc: 72.579% \n",
      "[Epoch:14, Iter:1339] Loss: 0.553 | Acc: 72.615% \n",
      "[Epoch:14, Iter:1340] Loss: 0.552 | Acc: 72.650% \n",
      "[Epoch:14, Iter:1341] Loss: 0.551 | Acc: 72.780% \n",
      "[Epoch:14, Iter:1342] Loss: 0.547 | Acc: 73.095% \n",
      "[Epoch:14, Iter:1343] Loss: 0.547 | Acc: 73.023% \n",
      "[Epoch:14, Iter:1344] Loss: 0.547 | Acc: 72.909% \n",
      "[Epoch:14, Iter:1345] Loss: 0.547 | Acc: 72.933% \n",
      "[Epoch:14, Iter:1346] Loss: 0.546 | Acc: 72.870% \n",
      "[Epoch:14, Iter:1347] Loss: 0.546 | Acc: 72.851% \n",
      "[Epoch:14, Iter:1348] Loss: 0.547 | Acc: 72.792% \n",
      "[Epoch:14, Iter:1349] Loss: 0.547 | Acc: 72.776% \n",
      "[Epoch:14, Iter:1350] Loss: 0.548 | Acc: 72.560% \n",
      "[Epoch:14, Iter:1351] Loss: 0.548 | Acc: 72.627% \n",
      "[Epoch:14, Iter:1352] Loss: 0.547 | Acc: 72.808% \n",
      "[Epoch:14, Iter:1353] Loss: 0.548 | Acc: 72.792% \n",
      "[Epoch:14, Iter:1354] Loss: 0.546 | Acc: 72.926% \n",
      "[Epoch:14, Iter:1355] Loss: 0.548 | Acc: 72.836% \n",
      "[Epoch:14, Iter:1356] Loss: 0.548 | Acc: 72.750% \n",
      "[Epoch:14, Iter:1357] Loss: 0.547 | Acc: 72.877% \n",
      "[Epoch:14, Iter:1358] Loss: 0.548 | Acc: 72.759% \n",
      "[Epoch:14, Iter:1359] Loss: 0.546 | Acc: 72.847% \n",
      "[Epoch:14, Iter:1360] Loss: 0.547 | Acc: 72.667% \n",
      "[Epoch:14, Iter:1361] Loss: 0.548 | Acc: 72.492% \n",
      "[Epoch:14, Iter:1362] Loss: 0.548 | Acc: 72.613% \n",
      "[Epoch:14, Iter:1363] Loss: 0.547 | Acc: 72.698% \n",
      "[Epoch:14, Iter:1364] Loss: 0.547 | Acc: 72.688% \n",
      "[Epoch:14, Iter:1365] Loss: 0.547 | Acc: 72.677% \n",
      "[Epoch:14, Iter:1366] Loss: 0.547 | Acc: 72.515% \n",
      "[Epoch:14, Iter:1367] Loss: 0.548 | Acc: 72.388% \n",
      "[Epoch:14, Iter:1368] Loss: 0.549 | Acc: 72.294% \n",
      "[Epoch:14, Iter:1369] Loss: 0.549 | Acc: 72.377% \n",
      "[Epoch:14, Iter:1370] Loss: 0.549 | Acc: 72.400% \n",
      "[Epoch:14, Iter:1371] Loss: 0.550 | Acc: 72.451% \n",
      "[Epoch:14, Iter:1372] Loss: 0.548 | Acc: 72.500% \n",
      "[Epoch:14, Iter:1373] Loss: 0.548 | Acc: 72.493% \n",
      "[Epoch:14, Iter:1374] Loss: 0.548 | Acc: 72.432% \n",
      "[Epoch:14, Iter:1375] Loss: 0.548 | Acc: 72.480% \n",
      "[Epoch:14, Iter:1376] Loss: 0.548 | Acc: 72.474% \n",
      "[Epoch:14, Iter:1377] Loss: 0.548 | Acc: 72.416% \n",
      "[Epoch:14, Iter:1378] Loss: 0.548 | Acc: 72.462% \n",
      "[Epoch:14, Iter:1379] Loss: 0.548 | Acc: 72.532% \n",
      "[Epoch:14, Iter:1380] Loss: 0.550 | Acc: 72.450% \n",
      "[Epoch:14, Iter:1381] Loss: 0.551 | Acc: 72.370% \n",
      "[Epoch:14, Iter:1382] Loss: 0.551 | Acc: 72.317% \n",
      "[Epoch:14, Iter:1383] Loss: 0.550 | Acc: 72.337% \n",
      "[Epoch:14, Iter:1384] Loss: 0.551 | Acc: 72.262% \n",
      "[Epoch:14, Iter:1385] Loss: 0.551 | Acc: 72.188% \n",
      "[Epoch:14, Iter:1386] Loss: 0.551 | Acc: 72.186% \n",
      "[Epoch:14, Iter:1387] Loss: 0.550 | Acc: 72.276% \n",
      "[Epoch:14, Iter:1388] Loss: 0.550 | Acc: 72.273% \n",
      "[Epoch:14, Iter:1389] Loss: 0.550 | Acc: 72.337% \n",
      "[Epoch:14, Iter:1390] Loss: 0.550 | Acc: 72.378% \n",
      "[Epoch:14, Iter:1391] Loss: 0.551 | Acc: 72.330% \n",
      "[Epoch:14, Iter:1392] Loss: 0.551 | Acc: 72.326% \n",
      "[Epoch:14, Iter:1393] Loss: 0.549 | Acc: 72.430% \n",
      "[Epoch:14, Iter:1394] Loss: 0.549 | Acc: 72.426% \n",
      "[Epoch:14, Iter:1395] Loss: 0.548 | Acc: 72.463% \n",
      "[Epoch:14, Iter:1396] Loss: 0.547 | Acc: 72.542% \n",
      "[Epoch:14, Iter:1397] Loss: 0.548 | Acc: 72.495% \n",
      "[Epoch:14, Iter:1398] Loss: 0.548 | Acc: 72.551% \n",
      "[Epoch:14, Iter:1399] Loss: 0.547 | Acc: 72.606% \n",
      "[Epoch:14, Iter:1400] Loss: 0.546 | Acc: 72.660% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.620%\n",
      "Training set's accuracy (after quantization) is: 72.540%\n",
      "Test set's accuracy (before quantization) is: 73.300%\n",
      "Test set's accuracy (after quantization) is: 71.800%\n",
      "Train Loss: 0.546 | Train Acc: 72.620% | Test Loss: 0.550 | Test Acc: 73.300% \n",
      "Quantized Train Loss: 0.547 | Quantized Train Acc: 72.540% | Quantized Test Loss: 0.549 | Quantized Test Acc: 71.800% \n",
      "\n",
      "Epoch: 15\n",
      "[Epoch:15, Iter:1401] Loss: 0.646 | Acc: 66.000% \n",
      "[Epoch:15, Iter:1402] Loss: 0.611 | Acc: 66.000% \n",
      "[Epoch:15, Iter:1403] Loss: 0.590 | Acc: 67.333% \n",
      "[Epoch:15, Iter:1404] Loss: 0.576 | Acc: 68.500% \n",
      "[Epoch:15, Iter:1405] Loss: 0.578 | Acc: 67.600% \n",
      "[Epoch:15, Iter:1406] Loss: 0.555 | Acc: 70.000% \n",
      "[Epoch:15, Iter:1407] Loss: 0.534 | Acc: 72.286% \n",
      "[Epoch:15, Iter:1408] Loss: 0.530 | Acc: 73.000% \n",
      "[Epoch:15, Iter:1409] Loss: 0.522 | Acc: 74.000% \n",
      "[Epoch:15, Iter:1410] Loss: 0.514 | Acc: 74.400% \n",
      "[Epoch:15, Iter:1411] Loss: 0.506 | Acc: 75.091% \n",
      "[Epoch:15, Iter:1412] Loss: 0.514 | Acc: 74.667% \n",
      "[Epoch:15, Iter:1413] Loss: 0.519 | Acc: 74.308% \n",
      "[Epoch:15, Iter:1414] Loss: 0.513 | Acc: 74.429% \n",
      "[Epoch:15, Iter:1415] Loss: 0.520 | Acc: 74.133% \n",
      "[Epoch:15, Iter:1416] Loss: 0.520 | Acc: 74.125% \n",
      "[Epoch:15, Iter:1417] Loss: 0.523 | Acc: 74.000% \n",
      "[Epoch:15, Iter:1418] Loss: 0.538 | Acc: 73.000% \n",
      "[Epoch:15, Iter:1419] Loss: 0.538 | Acc: 72.947% \n",
      "[Epoch:15, Iter:1420] Loss: 0.542 | Acc: 73.000% \n",
      "[Epoch:15, Iter:1421] Loss: 0.541 | Acc: 73.429% \n",
      "[Epoch:15, Iter:1422] Loss: 0.549 | Acc: 72.818% \n",
      "[Epoch:15, Iter:1423] Loss: 0.547 | Acc: 72.870% \n",
      "[Epoch:15, Iter:1424] Loss: 0.549 | Acc: 72.750% \n",
      "[Epoch:15, Iter:1425] Loss: 0.547 | Acc: 72.800% \n",
      "[Epoch:15, Iter:1426] Loss: 0.546 | Acc: 72.923% \n",
      "[Epoch:15, Iter:1427] Loss: 0.550 | Acc: 73.037% \n",
      "[Epoch:15, Iter:1428] Loss: 0.547 | Acc: 73.071% \n",
      "[Epoch:15, Iter:1429] Loss: 0.545 | Acc: 73.310% \n",
      "[Epoch:15, Iter:1430] Loss: 0.545 | Acc: 73.200% \n",
      "[Epoch:15, Iter:1431] Loss: 0.546 | Acc: 73.226% \n",
      "[Epoch:15, Iter:1432] Loss: 0.545 | Acc: 73.188% \n",
      "[Epoch:15, Iter:1433] Loss: 0.548 | Acc: 72.848% \n",
      "[Epoch:15, Iter:1434] Loss: 0.546 | Acc: 73.000% \n",
      "[Epoch:15, Iter:1435] Loss: 0.547 | Acc: 73.086% \n",
      "[Epoch:15, Iter:1436] Loss: 0.545 | Acc: 73.333% \n",
      "[Epoch:15, Iter:1437] Loss: 0.548 | Acc: 73.135% \n",
      "[Epoch:15, Iter:1438] Loss: 0.549 | Acc: 72.842% \n",
      "[Epoch:15, Iter:1439] Loss: 0.550 | Acc: 72.667% \n",
      "[Epoch:15, Iter:1440] Loss: 0.547 | Acc: 72.850% \n",
      "[Epoch:15, Iter:1441] Loss: 0.547 | Acc: 72.878% \n",
      "[Epoch:15, Iter:1442] Loss: 0.546 | Acc: 73.048% \n",
      "[Epoch:15, Iter:1443] Loss: 0.544 | Acc: 73.163% \n",
      "[Epoch:15, Iter:1444] Loss: 0.546 | Acc: 73.091% \n",
      "[Epoch:15, Iter:1445] Loss: 0.545 | Acc: 73.244% \n",
      "[Epoch:15, Iter:1446] Loss: 0.546 | Acc: 73.087% \n",
      "[Epoch:15, Iter:1447] Loss: 0.546 | Acc: 73.191% \n",
      "[Epoch:15, Iter:1448] Loss: 0.545 | Acc: 73.167% \n",
      "[Epoch:15, Iter:1449] Loss: 0.547 | Acc: 72.980% \n",
      "[Epoch:15, Iter:1450] Loss: 0.546 | Acc: 73.080% \n",
      "[Epoch:15, Iter:1451] Loss: 0.545 | Acc: 73.216% \n",
      "[Epoch:15, Iter:1452] Loss: 0.545 | Acc: 73.231% \n",
      "[Epoch:15, Iter:1453] Loss: 0.545 | Acc: 73.208% \n",
      "[Epoch:15, Iter:1454] Loss: 0.544 | Acc: 73.222% \n",
      "[Epoch:15, Iter:1455] Loss: 0.543 | Acc: 73.236% \n",
      "[Epoch:15, Iter:1456] Loss: 0.544 | Acc: 73.143% \n",
      "[Epoch:15, Iter:1457] Loss: 0.543 | Acc: 73.193% \n",
      "[Epoch:15, Iter:1458] Loss: 0.544 | Acc: 73.034% \n",
      "[Epoch:15, Iter:1459] Loss: 0.543 | Acc: 73.220% \n",
      "[Epoch:15, Iter:1460] Loss: 0.543 | Acc: 73.167% \n",
      "[Epoch:15, Iter:1461] Loss: 0.545 | Acc: 72.984% \n",
      "[Epoch:15, Iter:1462] Loss: 0.545 | Acc: 73.032% \n",
      "[Epoch:15, Iter:1463] Loss: 0.546 | Acc: 73.016% \n",
      "[Epoch:15, Iter:1464] Loss: 0.545 | Acc: 73.031% \n",
      "[Epoch:15, Iter:1465] Loss: 0.543 | Acc: 73.108% \n",
      "[Epoch:15, Iter:1466] Loss: 0.543 | Acc: 73.152% \n",
      "[Epoch:15, Iter:1467] Loss: 0.543 | Acc: 73.194% \n",
      "[Epoch:15, Iter:1468] Loss: 0.542 | Acc: 73.324% \n",
      "[Epoch:15, Iter:1469] Loss: 0.540 | Acc: 73.391% \n",
      "[Epoch:15, Iter:1470] Loss: 0.541 | Acc: 73.429% \n",
      "[Epoch:15, Iter:1471] Loss: 0.541 | Acc: 73.380% \n",
      "[Epoch:15, Iter:1472] Loss: 0.541 | Acc: 73.278% \n",
      "[Epoch:15, Iter:1473] Loss: 0.541 | Acc: 73.260% \n",
      "[Epoch:15, Iter:1474] Loss: 0.541 | Acc: 73.243% \n",
      "[Epoch:15, Iter:1475] Loss: 0.542 | Acc: 73.200% \n",
      "[Epoch:15, Iter:1476] Loss: 0.543 | Acc: 73.026% \n",
      "[Epoch:15, Iter:1477] Loss: 0.543 | Acc: 72.961% \n",
      "[Epoch:15, Iter:1478] Loss: 0.545 | Acc: 72.795% \n",
      "[Epoch:15, Iter:1479] Loss: 0.545 | Acc: 72.785% \n",
      "[Epoch:15, Iter:1480] Loss: 0.545 | Acc: 72.775% \n",
      "[Epoch:15, Iter:1481] Loss: 0.546 | Acc: 72.691% \n",
      "[Epoch:15, Iter:1482] Loss: 0.546 | Acc: 72.683% \n",
      "[Epoch:15, Iter:1483] Loss: 0.545 | Acc: 72.699% \n",
      "[Epoch:15, Iter:1484] Loss: 0.546 | Acc: 72.548% \n",
      "[Epoch:15, Iter:1485] Loss: 0.547 | Acc: 72.447% \n",
      "[Epoch:15, Iter:1486] Loss: 0.547 | Acc: 72.395% \n",
      "[Epoch:15, Iter:1487] Loss: 0.546 | Acc: 72.506% \n",
      "[Epoch:15, Iter:1488] Loss: 0.547 | Acc: 72.500% \n",
      "[Epoch:15, Iter:1489] Loss: 0.547 | Acc: 72.517% \n",
      "[Epoch:15, Iter:1490] Loss: 0.547 | Acc: 72.489% \n",
      "[Epoch:15, Iter:1491] Loss: 0.547 | Acc: 72.462% \n",
      "[Epoch:15, Iter:1492] Loss: 0.546 | Acc: 72.543% \n",
      "[Epoch:15, Iter:1493] Loss: 0.546 | Acc: 72.581% \n",
      "[Epoch:15, Iter:1494] Loss: 0.547 | Acc: 72.468% \n",
      "[Epoch:15, Iter:1495] Loss: 0.547 | Acc: 72.463% \n",
      "[Epoch:15, Iter:1496] Loss: 0.547 | Acc: 72.458% \n",
      "[Epoch:15, Iter:1497] Loss: 0.547 | Acc: 72.474% \n",
      "[Epoch:15, Iter:1498] Loss: 0.546 | Acc: 72.551% \n",
      "[Epoch:15, Iter:1499] Loss: 0.545 | Acc: 72.606% \n",
      "[Epoch:15, Iter:1500] Loss: 0.546 | Acc: 72.540% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.620%\n",
      "Training set's accuracy (after quantization) is: 71.920%\n",
      "Test set's accuracy (before quantization) is: 71.600%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.544 | Train Acc: 72.620% | Test Loss: 0.548 | Test Acc: 71.600% \n",
      "Quantized Train Loss: 0.556 | Quantized Train Acc: 71.920% | Quantized Test Loss: 0.559 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 16\n",
      "[Epoch:16, Iter:1501] Loss: 0.538 | Acc: 76.000% \n",
      "[Epoch:16, Iter:1502] Loss: 0.533 | Acc: 76.000% \n",
      "[Epoch:16, Iter:1503] Loss: 0.533 | Acc: 76.667% \n",
      "[Epoch:16, Iter:1504] Loss: 0.532 | Acc: 74.000% \n",
      "[Epoch:16, Iter:1505] Loss: 0.531 | Acc: 73.600% \n",
      "[Epoch:16, Iter:1506] Loss: 0.533 | Acc: 73.667% \n",
      "[Epoch:16, Iter:1507] Loss: 0.546 | Acc: 72.857% \n",
      "[Epoch:16, Iter:1508] Loss: 0.536 | Acc: 72.750% \n",
      "[Epoch:16, Iter:1509] Loss: 0.525 | Acc: 73.556% \n",
      "[Epoch:16, Iter:1510] Loss: 0.528 | Acc: 73.800% \n",
      "[Epoch:16, Iter:1511] Loss: 0.526 | Acc: 74.182% \n",
      "[Epoch:16, Iter:1512] Loss: 0.533 | Acc: 74.000% \n",
      "[Epoch:16, Iter:1513] Loss: 0.533 | Acc: 74.000% \n",
      "[Epoch:16, Iter:1514] Loss: 0.530 | Acc: 74.143% \n",
      "[Epoch:16, Iter:1515] Loss: 0.533 | Acc: 73.733% \n",
      "[Epoch:16, Iter:1516] Loss: 0.535 | Acc: 73.625% \n",
      "[Epoch:16, Iter:1517] Loss: 0.541 | Acc: 72.941% \n",
      "[Epoch:16, Iter:1518] Loss: 0.540 | Acc: 73.111% \n",
      "[Epoch:16, Iter:1519] Loss: 0.544 | Acc: 72.316% \n",
      "[Epoch:16, Iter:1520] Loss: 0.543 | Acc: 72.600% \n",
      "[Epoch:16, Iter:1521] Loss: 0.544 | Acc: 72.571% \n",
      "[Epoch:16, Iter:1522] Loss: 0.541 | Acc: 72.727% \n",
      "[Epoch:16, Iter:1523] Loss: 0.538 | Acc: 72.783% \n",
      "[Epoch:16, Iter:1524] Loss: 0.536 | Acc: 72.917% \n",
      "[Epoch:16, Iter:1525] Loss: 0.535 | Acc: 73.280% \n",
      "[Epoch:16, Iter:1526] Loss: 0.534 | Acc: 73.462% \n",
      "[Epoch:16, Iter:1527] Loss: 0.535 | Acc: 73.407% \n",
      "[Epoch:16, Iter:1528] Loss: 0.534 | Acc: 73.643% \n",
      "[Epoch:16, Iter:1529] Loss: 0.533 | Acc: 73.931% \n",
      "[Epoch:16, Iter:1530] Loss: 0.532 | Acc: 73.933% \n",
      "[Epoch:16, Iter:1531] Loss: 0.532 | Acc: 73.806% \n",
      "[Epoch:16, Iter:1532] Loss: 0.535 | Acc: 73.750% \n",
      "[Epoch:16, Iter:1533] Loss: 0.534 | Acc: 73.758% \n",
      "[Epoch:16, Iter:1534] Loss: 0.533 | Acc: 73.941% \n",
      "[Epoch:16, Iter:1535] Loss: 0.533 | Acc: 73.943% \n",
      "[Epoch:16, Iter:1536] Loss: 0.533 | Acc: 73.778% \n",
      "[Epoch:16, Iter:1537] Loss: 0.530 | Acc: 74.000% \n",
      "[Epoch:16, Iter:1538] Loss: 0.534 | Acc: 73.895% \n",
      "[Epoch:16, Iter:1539] Loss: 0.535 | Acc: 73.846% \n",
      "[Epoch:16, Iter:1540] Loss: 0.533 | Acc: 73.900% \n",
      "[Epoch:16, Iter:1541] Loss: 0.532 | Acc: 74.049% \n",
      "[Epoch:16, Iter:1542] Loss: 0.534 | Acc: 73.905% \n",
      "[Epoch:16, Iter:1543] Loss: 0.535 | Acc: 73.767% \n",
      "[Epoch:16, Iter:1544] Loss: 0.536 | Acc: 73.727% \n",
      "[Epoch:16, Iter:1545] Loss: 0.539 | Acc: 73.422% \n",
      "[Epoch:16, Iter:1546] Loss: 0.540 | Acc: 73.391% \n",
      "[Epoch:16, Iter:1547] Loss: 0.538 | Acc: 73.489% \n",
      "[Epoch:16, Iter:1548] Loss: 0.537 | Acc: 73.625% \n",
      "[Epoch:16, Iter:1549] Loss: 0.537 | Acc: 73.633% \n",
      "[Epoch:16, Iter:1550] Loss: 0.536 | Acc: 73.720% \n",
      "[Epoch:16, Iter:1551] Loss: 0.538 | Acc: 73.725% \n",
      "[Epoch:16, Iter:1552] Loss: 0.537 | Acc: 73.692% \n",
      "[Epoch:16, Iter:1553] Loss: 0.538 | Acc: 73.623% \n",
      "[Epoch:16, Iter:1554] Loss: 0.538 | Acc: 73.556% \n",
      "[Epoch:16, Iter:1555] Loss: 0.537 | Acc: 73.564% \n",
      "[Epoch:16, Iter:1556] Loss: 0.538 | Acc: 73.429% \n",
      "[Epoch:16, Iter:1557] Loss: 0.539 | Acc: 73.404% \n",
      "[Epoch:16, Iter:1558] Loss: 0.539 | Acc: 73.345% \n",
      "[Epoch:16, Iter:1559] Loss: 0.540 | Acc: 73.356% \n",
      "[Epoch:16, Iter:1560] Loss: 0.541 | Acc: 73.267% \n",
      "[Epoch:16, Iter:1561] Loss: 0.542 | Acc: 73.115% \n",
      "[Epoch:16, Iter:1562] Loss: 0.544 | Acc: 72.968% \n",
      "[Epoch:16, Iter:1563] Loss: 0.543 | Acc: 72.984% \n",
      "[Epoch:16, Iter:1564] Loss: 0.543 | Acc: 72.938% \n",
      "[Epoch:16, Iter:1565] Loss: 0.544 | Acc: 72.892% \n",
      "[Epoch:16, Iter:1566] Loss: 0.545 | Acc: 72.848% \n",
      "[Epoch:16, Iter:1567] Loss: 0.545 | Acc: 72.866% \n",
      "[Epoch:16, Iter:1568] Loss: 0.545 | Acc: 72.765% \n",
      "[Epoch:16, Iter:1569] Loss: 0.546 | Acc: 72.667% \n",
      "[Epoch:16, Iter:1570] Loss: 0.546 | Acc: 72.686% \n",
      "[Epoch:16, Iter:1571] Loss: 0.548 | Acc: 72.648% \n",
      "[Epoch:16, Iter:1572] Loss: 0.547 | Acc: 72.639% \n",
      "[Epoch:16, Iter:1573] Loss: 0.547 | Acc: 72.685% \n",
      "[Epoch:16, Iter:1574] Loss: 0.548 | Acc: 72.649% \n",
      "[Epoch:16, Iter:1575] Loss: 0.549 | Acc: 72.507% \n",
      "[Epoch:16, Iter:1576] Loss: 0.548 | Acc: 72.605% \n",
      "[Epoch:16, Iter:1577] Loss: 0.548 | Acc: 72.675% \n",
      "[Epoch:16, Iter:1578] Loss: 0.548 | Acc: 72.667% \n",
      "[Epoch:16, Iter:1579] Loss: 0.549 | Acc: 72.532% \n",
      "[Epoch:16, Iter:1580] Loss: 0.548 | Acc: 72.550% \n",
      "[Epoch:16, Iter:1581] Loss: 0.548 | Acc: 72.420% \n",
      "[Epoch:16, Iter:1582] Loss: 0.547 | Acc: 72.537% \n",
      "[Epoch:16, Iter:1583] Loss: 0.546 | Acc: 72.651% \n",
      "[Epoch:16, Iter:1584] Loss: 0.547 | Acc: 72.619% \n",
      "[Epoch:16, Iter:1585] Loss: 0.548 | Acc: 72.494% \n",
      "[Epoch:16, Iter:1586] Loss: 0.548 | Acc: 72.605% \n",
      "[Epoch:16, Iter:1587] Loss: 0.548 | Acc: 72.667% \n",
      "[Epoch:16, Iter:1588] Loss: 0.548 | Acc: 72.614% \n",
      "[Epoch:16, Iter:1589] Loss: 0.548 | Acc: 72.674% \n",
      "[Epoch:16, Iter:1590] Loss: 0.547 | Acc: 72.711% \n",
      "[Epoch:16, Iter:1591] Loss: 0.546 | Acc: 72.747% \n",
      "[Epoch:16, Iter:1592] Loss: 0.545 | Acc: 72.783% \n",
      "[Epoch:16, Iter:1593] Loss: 0.544 | Acc: 72.839% \n",
      "[Epoch:16, Iter:1594] Loss: 0.543 | Acc: 72.872% \n",
      "[Epoch:16, Iter:1595] Loss: 0.544 | Acc: 72.842% \n",
      "[Epoch:16, Iter:1596] Loss: 0.543 | Acc: 72.833% \n",
      "[Epoch:16, Iter:1597] Loss: 0.543 | Acc: 72.784% \n",
      "[Epoch:16, Iter:1598] Loss: 0.543 | Acc: 72.755% \n",
      "[Epoch:16, Iter:1599] Loss: 0.544 | Acc: 72.747% \n",
      "[Epoch:16, Iter:1600] Loss: 0.545 | Acc: 72.660% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.620%\n",
      "Training set's accuracy (after quantization) is: 71.860%\n",
      "Test set's accuracy (before quantization) is: 72.000%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.546 | Train Acc: 72.620% | Test Loss: 0.550 | Test Acc: 72.000% \n",
      "Quantized Train Loss: 0.559 | Quantized Train Acc: 71.860% | Quantized Test Loss: 0.562 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 17\n",
      "[Epoch:17, Iter:1601] Loss: 0.544 | Acc: 76.000% \n",
      "[Epoch:17, Iter:1602] Loss: 0.510 | Acc: 76.000% \n",
      "[Epoch:17, Iter:1603] Loss: 0.501 | Acc: 77.333% \n",
      "[Epoch:17, Iter:1604] Loss: 0.516 | Acc: 75.500% \n",
      "[Epoch:17, Iter:1605] Loss: 0.513 | Acc: 76.000% \n",
      "[Epoch:17, Iter:1606] Loss: 0.523 | Acc: 74.333% \n",
      "[Epoch:17, Iter:1607] Loss: 0.529 | Acc: 74.571% \n",
      "[Epoch:17, Iter:1608] Loss: 0.529 | Acc: 74.750% \n",
      "[Epoch:17, Iter:1609] Loss: 0.530 | Acc: 74.222% \n",
      "[Epoch:17, Iter:1610] Loss: 0.535 | Acc: 75.000% \n",
      "[Epoch:17, Iter:1611] Loss: 0.538 | Acc: 74.909% \n",
      "[Epoch:17, Iter:1612] Loss: 0.535 | Acc: 75.500% \n",
      "[Epoch:17, Iter:1613] Loss: 0.534 | Acc: 75.385% \n",
      "[Epoch:17, Iter:1614] Loss: 0.538 | Acc: 74.571% \n",
      "[Epoch:17, Iter:1615] Loss: 0.542 | Acc: 74.267% \n",
      "[Epoch:17, Iter:1616] Loss: 0.548 | Acc: 73.500% \n",
      "[Epoch:17, Iter:1617] Loss: 0.548 | Acc: 72.941% \n",
      "[Epoch:17, Iter:1618] Loss: 0.548 | Acc: 72.778% \n",
      "[Epoch:17, Iter:1619] Loss: 0.553 | Acc: 72.316% \n",
      "[Epoch:17, Iter:1620] Loss: 0.551 | Acc: 72.300% \n",
      "[Epoch:17, Iter:1621] Loss: 0.545 | Acc: 72.667% \n",
      "[Epoch:17, Iter:1622] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:17, Iter:1623] Loss: 0.551 | Acc: 71.826% \n",
      "[Epoch:17, Iter:1624] Loss: 0.551 | Acc: 71.667% \n",
      "[Epoch:17, Iter:1625] Loss: 0.550 | Acc: 71.680% \n",
      "[Epoch:17, Iter:1626] Loss: 0.550 | Acc: 72.077% \n",
      "[Epoch:17, Iter:1627] Loss: 0.553 | Acc: 71.852% \n",
      "[Epoch:17, Iter:1628] Loss: 0.551 | Acc: 72.214% \n",
      "[Epoch:17, Iter:1629] Loss: 0.550 | Acc: 72.345% \n",
      "[Epoch:17, Iter:1630] Loss: 0.553 | Acc: 72.200% \n",
      "[Epoch:17, Iter:1631] Loss: 0.551 | Acc: 72.194% \n",
      "[Epoch:17, Iter:1632] Loss: 0.550 | Acc: 72.188% \n",
      "[Epoch:17, Iter:1633] Loss: 0.549 | Acc: 72.061% \n",
      "[Epoch:17, Iter:1634] Loss: 0.552 | Acc: 71.706% \n",
      "[Epoch:17, Iter:1635] Loss: 0.552 | Acc: 71.714% \n",
      "[Epoch:17, Iter:1636] Loss: 0.549 | Acc: 71.833% \n",
      "[Epoch:17, Iter:1637] Loss: 0.548 | Acc: 72.162% \n",
      "[Epoch:17, Iter:1638] Loss: 0.547 | Acc: 72.316% \n",
      "[Epoch:17, Iter:1639] Loss: 0.548 | Acc: 72.308% \n",
      "[Epoch:17, Iter:1640] Loss: 0.547 | Acc: 72.350% \n",
      "[Epoch:17, Iter:1641] Loss: 0.554 | Acc: 72.146% \n",
      "[Epoch:17, Iter:1642] Loss: 0.552 | Acc: 72.286% \n",
      "[Epoch:17, Iter:1643] Loss: 0.550 | Acc: 72.326% \n",
      "[Epoch:17, Iter:1644] Loss: 0.549 | Acc: 72.409% \n",
      "[Epoch:17, Iter:1645] Loss: 0.547 | Acc: 72.578% \n",
      "[Epoch:17, Iter:1646] Loss: 0.543 | Acc: 72.826% \n",
      "[Epoch:17, Iter:1647] Loss: 0.544 | Acc: 72.809% \n",
      "[Epoch:17, Iter:1648] Loss: 0.543 | Acc: 72.833% \n",
      "[Epoch:17, Iter:1649] Loss: 0.541 | Acc: 73.143% \n",
      "[Epoch:17, Iter:1650] Loss: 0.538 | Acc: 73.320% \n",
      "[Epoch:17, Iter:1651] Loss: 0.540 | Acc: 73.176% \n",
      "[Epoch:17, Iter:1652] Loss: 0.538 | Acc: 73.385% \n",
      "[Epoch:17, Iter:1653] Loss: 0.538 | Acc: 73.358% \n",
      "[Epoch:17, Iter:1654] Loss: 0.537 | Acc: 73.370% \n",
      "[Epoch:17, Iter:1655] Loss: 0.537 | Acc: 73.345% \n",
      "[Epoch:17, Iter:1656] Loss: 0.537 | Acc: 73.321% \n",
      "[Epoch:17, Iter:1657] Loss: 0.539 | Acc: 73.263% \n",
      "[Epoch:17, Iter:1658] Loss: 0.538 | Acc: 73.379% \n",
      "[Epoch:17, Iter:1659] Loss: 0.537 | Acc: 73.322% \n",
      "[Epoch:17, Iter:1660] Loss: 0.536 | Acc: 73.367% \n",
      "[Epoch:17, Iter:1661] Loss: 0.535 | Acc: 73.410% \n",
      "[Epoch:17, Iter:1662] Loss: 0.535 | Acc: 73.452% \n",
      "[Epoch:17, Iter:1663] Loss: 0.534 | Acc: 73.524% \n",
      "[Epoch:17, Iter:1664] Loss: 0.536 | Acc: 73.375% \n",
      "[Epoch:17, Iter:1665] Loss: 0.536 | Acc: 73.292% \n",
      "[Epoch:17, Iter:1666] Loss: 0.535 | Acc: 73.303% \n",
      "[Epoch:17, Iter:1667] Loss: 0.534 | Acc: 73.433% \n",
      "[Epoch:17, Iter:1668] Loss: 0.534 | Acc: 73.441% \n",
      "[Epoch:17, Iter:1669] Loss: 0.534 | Acc: 73.391% \n",
      "[Epoch:17, Iter:1670] Loss: 0.535 | Acc: 73.314% \n",
      "[Epoch:17, Iter:1671] Loss: 0.536 | Acc: 73.239% \n",
      "[Epoch:17, Iter:1672] Loss: 0.537 | Acc: 73.194% \n",
      "[Epoch:17, Iter:1673] Loss: 0.539 | Acc: 73.068% \n",
      "[Epoch:17, Iter:1674] Loss: 0.539 | Acc: 73.054% \n",
      "[Epoch:17, Iter:1675] Loss: 0.538 | Acc: 73.200% \n",
      "[Epoch:17, Iter:1676] Loss: 0.539 | Acc: 73.211% \n",
      "[Epoch:17, Iter:1677] Loss: 0.539 | Acc: 73.221% \n",
      "[Epoch:17, Iter:1678] Loss: 0.540 | Acc: 73.179% \n",
      "[Epoch:17, Iter:1679] Loss: 0.541 | Acc: 73.165% \n",
      "[Epoch:17, Iter:1680] Loss: 0.542 | Acc: 73.150% \n",
      "[Epoch:17, Iter:1681] Loss: 0.543 | Acc: 73.062% \n",
      "[Epoch:17, Iter:1682] Loss: 0.544 | Acc: 73.049% \n",
      "[Epoch:17, Iter:1683] Loss: 0.544 | Acc: 73.012% \n",
      "[Epoch:17, Iter:1684] Loss: 0.544 | Acc: 72.929% \n",
      "[Epoch:17, Iter:1685] Loss: 0.546 | Acc: 72.776% \n",
      "[Epoch:17, Iter:1686] Loss: 0.547 | Acc: 72.651% \n",
      "[Epoch:17, Iter:1687] Loss: 0.546 | Acc: 72.759% \n",
      "[Epoch:17, Iter:1688] Loss: 0.546 | Acc: 72.750% \n",
      "[Epoch:17, Iter:1689] Loss: 0.546 | Acc: 72.674% \n",
      "[Epoch:17, Iter:1690] Loss: 0.546 | Acc: 72.800% \n",
      "[Epoch:17, Iter:1691] Loss: 0.545 | Acc: 72.901% \n",
      "[Epoch:17, Iter:1692] Loss: 0.545 | Acc: 72.935% \n",
      "[Epoch:17, Iter:1693] Loss: 0.546 | Acc: 72.796% \n",
      "[Epoch:17, Iter:1694] Loss: 0.546 | Acc: 72.851% \n",
      "[Epoch:17, Iter:1695] Loss: 0.547 | Acc: 72.758% \n",
      "[Epoch:17, Iter:1696] Loss: 0.546 | Acc: 72.750% \n",
      "[Epoch:17, Iter:1697] Loss: 0.546 | Acc: 72.742% \n",
      "[Epoch:17, Iter:1698] Loss: 0.546 | Acc: 72.694% \n",
      "[Epoch:17, Iter:1699] Loss: 0.546 | Acc: 72.707% \n",
      "[Epoch:17, Iter:1700] Loss: 0.545 | Acc: 72.760% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.180%\n",
      "Training set's accuracy (after quantization) is: 72.300%\n",
      "Test set's accuracy (before quantization) is: 72.700%\n",
      "Test set's accuracy (after quantization) is: 71.600%\n",
      "Train Loss: 0.551 | Train Acc: 72.180% | Test Loss: 0.555 | Test Acc: 72.700% \n",
      "Quantized Train Loss: 0.548 | Quantized Train Acc: 72.300% | Quantized Test Loss: 0.550 | Quantized Test Acc: 71.600% \n",
      "\n",
      "Epoch: 18\n",
      "[Epoch:18, Iter:1701] Loss: 0.438 | Acc: 76.000% \n",
      "[Epoch:18, Iter:1702] Loss: 0.470 | Acc: 75.000% \n",
      "[Epoch:18, Iter:1703] Loss: 0.489 | Acc: 74.667% \n",
      "[Epoch:18, Iter:1704] Loss: 0.552 | Acc: 71.000% \n",
      "[Epoch:18, Iter:1705] Loss: 0.581 | Acc: 70.000% \n",
      "[Epoch:18, Iter:1706] Loss: 0.573 | Acc: 71.333% \n",
      "[Epoch:18, Iter:1707] Loss: 0.572 | Acc: 70.857% \n",
      "[Epoch:18, Iter:1708] Loss: 0.565 | Acc: 71.250% \n",
      "[Epoch:18, Iter:1709] Loss: 0.558 | Acc: 71.778% \n",
      "[Epoch:18, Iter:1710] Loss: 0.562 | Acc: 71.200% \n",
      "[Epoch:18, Iter:1711] Loss: 0.570 | Acc: 70.182% \n",
      "[Epoch:18, Iter:1712] Loss: 0.578 | Acc: 70.167% \n",
      "[Epoch:18, Iter:1713] Loss: 0.584 | Acc: 70.154% \n",
      "[Epoch:18, Iter:1714] Loss: 0.590 | Acc: 69.143% \n",
      "[Epoch:18, Iter:1715] Loss: 0.590 | Acc: 69.467% \n",
      "[Epoch:18, Iter:1716] Loss: 0.586 | Acc: 70.625% \n",
      "[Epoch:18, Iter:1717] Loss: 0.582 | Acc: 70.824% \n",
      "[Epoch:18, Iter:1718] Loss: 0.579 | Acc: 71.333% \n",
      "[Epoch:18, Iter:1719] Loss: 0.575 | Acc: 71.579% \n",
      "[Epoch:18, Iter:1720] Loss: 0.574 | Acc: 71.600% \n",
      "[Epoch:18, Iter:1721] Loss: 0.570 | Acc: 72.190% \n",
      "[Epoch:18, Iter:1722] Loss: 0.568 | Acc: 72.273% \n",
      "[Epoch:18, Iter:1723] Loss: 0.568 | Acc: 72.261% \n",
      "[Epoch:18, Iter:1724] Loss: 0.564 | Acc: 72.417% \n",
      "[Epoch:18, Iter:1725] Loss: 0.557 | Acc: 72.960% \n",
      "[Epoch:18, Iter:1726] Loss: 0.552 | Acc: 73.385% \n",
      "[Epoch:18, Iter:1727] Loss: 0.551 | Acc: 73.556% \n",
      "[Epoch:18, Iter:1728] Loss: 0.551 | Acc: 73.357% \n",
      "[Epoch:18, Iter:1729] Loss: 0.552 | Acc: 73.379% \n",
      "[Epoch:18, Iter:1730] Loss: 0.550 | Acc: 73.467% \n",
      "[Epoch:18, Iter:1731] Loss: 0.551 | Acc: 73.419% \n",
      "[Epoch:18, Iter:1732] Loss: 0.549 | Acc: 73.500% \n",
      "[Epoch:18, Iter:1733] Loss: 0.547 | Acc: 73.636% \n",
      "[Epoch:18, Iter:1734] Loss: 0.544 | Acc: 73.765% \n",
      "[Epoch:18, Iter:1735] Loss: 0.540 | Acc: 74.114% \n",
      "[Epoch:18, Iter:1736] Loss: 0.539 | Acc: 74.000% \n",
      "[Epoch:18, Iter:1737] Loss: 0.536 | Acc: 74.324% \n",
      "[Epoch:18, Iter:1738] Loss: 0.535 | Acc: 74.526% \n",
      "[Epoch:18, Iter:1739] Loss: 0.535 | Acc: 74.410% \n",
      "[Epoch:18, Iter:1740] Loss: 0.535 | Acc: 74.450% \n",
      "[Epoch:18, Iter:1741] Loss: 0.537 | Acc: 74.244% \n",
      "[Epoch:18, Iter:1742] Loss: 0.537 | Acc: 74.190% \n",
      "[Epoch:18, Iter:1743] Loss: 0.538 | Acc: 74.233% \n",
      "[Epoch:18, Iter:1744] Loss: 0.540 | Acc: 74.000% \n",
      "[Epoch:18, Iter:1745] Loss: 0.542 | Acc: 73.956% \n",
      "[Epoch:18, Iter:1746] Loss: 0.543 | Acc: 73.913% \n",
      "[Epoch:18, Iter:1747] Loss: 0.544 | Acc: 73.872% \n",
      "[Epoch:18, Iter:1748] Loss: 0.542 | Acc: 73.833% \n",
      "[Epoch:18, Iter:1749] Loss: 0.542 | Acc: 73.714% \n",
      "[Epoch:18, Iter:1750] Loss: 0.545 | Acc: 73.360% \n",
      "[Epoch:18, Iter:1751] Loss: 0.546 | Acc: 73.137% \n",
      "[Epoch:18, Iter:1752] Loss: 0.546 | Acc: 73.077% \n",
      "[Epoch:18, Iter:1753] Loss: 0.543 | Acc: 73.321% \n",
      "[Epoch:18, Iter:1754] Loss: 0.543 | Acc: 73.222% \n",
      "[Epoch:18, Iter:1755] Loss: 0.543 | Acc: 73.273% \n",
      "[Epoch:18, Iter:1756] Loss: 0.542 | Acc: 73.250% \n",
      "[Epoch:18, Iter:1757] Loss: 0.543 | Acc: 73.158% \n",
      "[Epoch:18, Iter:1758] Loss: 0.543 | Acc: 73.034% \n",
      "[Epoch:18, Iter:1759] Loss: 0.543 | Acc: 73.051% \n",
      "[Epoch:18, Iter:1760] Loss: 0.542 | Acc: 73.067% \n",
      "[Epoch:18, Iter:1761] Loss: 0.543 | Acc: 73.049% \n",
      "[Epoch:18, Iter:1762] Loss: 0.542 | Acc: 72.935% \n",
      "[Epoch:18, Iter:1763] Loss: 0.540 | Acc: 73.143% \n",
      "[Epoch:18, Iter:1764] Loss: 0.540 | Acc: 73.094% \n",
      "[Epoch:18, Iter:1765] Loss: 0.539 | Acc: 73.231% \n",
      "[Epoch:18, Iter:1766] Loss: 0.540 | Acc: 73.182% \n",
      "[Epoch:18, Iter:1767] Loss: 0.541 | Acc: 73.045% \n",
      "[Epoch:18, Iter:1768] Loss: 0.541 | Acc: 73.029% \n",
      "[Epoch:18, Iter:1769] Loss: 0.541 | Acc: 73.043% \n",
      "[Epoch:18, Iter:1770] Loss: 0.543 | Acc: 72.829% \n",
      "[Epoch:18, Iter:1771] Loss: 0.543 | Acc: 72.845% \n",
      "[Epoch:18, Iter:1772] Loss: 0.544 | Acc: 72.778% \n",
      "[Epoch:18, Iter:1773] Loss: 0.544 | Acc: 72.767% \n",
      "[Epoch:18, Iter:1774] Loss: 0.544 | Acc: 72.757% \n",
      "[Epoch:18, Iter:1775] Loss: 0.545 | Acc: 72.693% \n",
      "[Epoch:18, Iter:1776] Loss: 0.545 | Acc: 72.763% \n",
      "[Epoch:18, Iter:1777] Loss: 0.545 | Acc: 72.779% \n",
      "[Epoch:18, Iter:1778] Loss: 0.546 | Acc: 72.744% \n",
      "[Epoch:18, Iter:1779] Loss: 0.545 | Acc: 72.810% \n",
      "[Epoch:18, Iter:1780] Loss: 0.546 | Acc: 72.750% \n",
      "[Epoch:18, Iter:1781] Loss: 0.546 | Acc: 72.790% \n",
      "[Epoch:18, Iter:1782] Loss: 0.547 | Acc: 72.829% \n",
      "[Epoch:18, Iter:1783] Loss: 0.547 | Acc: 72.795% \n",
      "[Epoch:18, Iter:1784] Loss: 0.546 | Acc: 72.857% \n",
      "[Epoch:18, Iter:1785] Loss: 0.547 | Acc: 72.729% \n",
      "[Epoch:18, Iter:1786] Loss: 0.547 | Acc: 72.744% \n",
      "[Epoch:18, Iter:1787] Loss: 0.546 | Acc: 72.851% \n",
      "[Epoch:18, Iter:1788] Loss: 0.546 | Acc: 72.795% \n",
      "[Epoch:18, Iter:1789] Loss: 0.546 | Acc: 72.854% \n",
      "[Epoch:18, Iter:1790] Loss: 0.545 | Acc: 72.889% \n",
      "[Epoch:18, Iter:1791] Loss: 0.543 | Acc: 72.967% \n",
      "[Epoch:18, Iter:1792] Loss: 0.544 | Acc: 72.804% \n",
      "[Epoch:18, Iter:1793] Loss: 0.545 | Acc: 72.731% \n",
      "[Epoch:18, Iter:1794] Loss: 0.544 | Acc: 72.787% \n",
      "[Epoch:18, Iter:1795] Loss: 0.544 | Acc: 72.800% \n",
      "[Epoch:18, Iter:1796] Loss: 0.544 | Acc: 72.812% \n",
      "[Epoch:18, Iter:1797] Loss: 0.544 | Acc: 72.804% \n",
      "[Epoch:18, Iter:1798] Loss: 0.543 | Acc: 72.776% \n",
      "[Epoch:18, Iter:1799] Loss: 0.543 | Acc: 72.727% \n",
      "[Epoch:18, Iter:1800] Loss: 0.544 | Acc: 72.600% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.680%\n",
      "Training set's accuracy (after quantization) is: 71.980%\n",
      "Test set's accuracy (before quantization) is: 72.600%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.541 | Train Acc: 72.680% | Test Loss: 0.545 | Test Acc: 72.600% \n",
      "Quantized Train Loss: 0.555 | Quantized Train Acc: 71.980% | Quantized Test Loss: 0.558 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 19\n",
      "[Epoch:19, Iter:1801] Loss: 0.475 | Acc: 76.000% \n",
      "[Epoch:19, Iter:1802] Loss: 0.608 | Acc: 65.000% \n",
      "[Epoch:19, Iter:1803] Loss: 0.601 | Acc: 64.667% \n",
      "[Epoch:19, Iter:1804] Loss: 0.590 | Acc: 66.500% \n",
      "[Epoch:19, Iter:1805] Loss: 0.559 | Acc: 69.600% \n",
      "[Epoch:19, Iter:1806] Loss: 0.559 | Acc: 69.667% \n",
      "[Epoch:19, Iter:1807] Loss: 0.559 | Acc: 68.857% \n",
      "[Epoch:19, Iter:1808] Loss: 0.543 | Acc: 70.000% \n",
      "[Epoch:19, Iter:1809] Loss: 0.552 | Acc: 69.778% \n",
      "[Epoch:19, Iter:1810] Loss: 0.552 | Acc: 69.800% \n",
      "[Epoch:19, Iter:1811] Loss: 0.555 | Acc: 69.818% \n",
      "[Epoch:19, Iter:1812] Loss: 0.557 | Acc: 69.667% \n",
      "[Epoch:19, Iter:1813] Loss: 0.567 | Acc: 69.077% \n",
      "[Epoch:19, Iter:1814] Loss: 0.569 | Acc: 68.857% \n",
      "[Epoch:19, Iter:1815] Loss: 0.575 | Acc: 68.400% \n",
      "[Epoch:19, Iter:1816] Loss: 0.568 | Acc: 69.375% \n",
      "[Epoch:19, Iter:1817] Loss: 0.571 | Acc: 69.412% \n",
      "[Epoch:19, Iter:1818] Loss: 0.567 | Acc: 69.889% \n",
      "[Epoch:19, Iter:1819] Loss: 0.567 | Acc: 69.789% \n",
      "[Epoch:19, Iter:1820] Loss: 0.566 | Acc: 69.600% \n",
      "[Epoch:19, Iter:1821] Loss: 0.563 | Acc: 69.714% \n",
      "[Epoch:19, Iter:1822] Loss: 0.559 | Acc: 69.909% \n",
      "[Epoch:19, Iter:1823] Loss: 0.553 | Acc: 70.348% \n",
      "[Epoch:19, Iter:1824] Loss: 0.551 | Acc: 70.667% \n",
      "[Epoch:19, Iter:1825] Loss: 0.549 | Acc: 70.800% \n",
      "[Epoch:19, Iter:1826] Loss: 0.546 | Acc: 71.077% \n",
      "[Epoch:19, Iter:1827] Loss: 0.542 | Acc: 71.407% \n",
      "[Epoch:19, Iter:1828] Loss: 0.544 | Acc: 71.429% \n",
      "[Epoch:19, Iter:1829] Loss: 0.540 | Acc: 71.448% \n",
      "[Epoch:19, Iter:1830] Loss: 0.541 | Acc: 71.400% \n",
      "[Epoch:19, Iter:1831] Loss: 0.548 | Acc: 70.839% \n",
      "[Epoch:19, Iter:1832] Loss: 0.547 | Acc: 70.938% \n",
      "[Epoch:19, Iter:1833] Loss: 0.546 | Acc: 71.152% \n",
      "[Epoch:19, Iter:1834] Loss: 0.547 | Acc: 71.059% \n",
      "[Epoch:19, Iter:1835] Loss: 0.546 | Acc: 71.143% \n",
      "[Epoch:19, Iter:1836] Loss: 0.547 | Acc: 71.222% \n",
      "[Epoch:19, Iter:1837] Loss: 0.545 | Acc: 71.243% \n",
      "[Epoch:19, Iter:1838] Loss: 0.541 | Acc: 71.579% \n",
      "[Epoch:19, Iter:1839] Loss: 0.541 | Acc: 71.744% \n",
      "[Epoch:19, Iter:1840] Loss: 0.538 | Acc: 72.050% \n",
      "[Epoch:19, Iter:1841] Loss: 0.538 | Acc: 72.000% \n",
      "[Epoch:19, Iter:1842] Loss: 0.538 | Acc: 72.048% \n",
      "[Epoch:19, Iter:1843] Loss: 0.539 | Acc: 72.047% \n",
      "[Epoch:19, Iter:1844] Loss: 0.540 | Acc: 71.909% \n",
      "[Epoch:19, Iter:1845] Loss: 0.541 | Acc: 71.956% \n",
      "[Epoch:19, Iter:1846] Loss: 0.539 | Acc: 71.957% \n",
      "[Epoch:19, Iter:1847] Loss: 0.540 | Acc: 71.830% \n",
      "[Epoch:19, Iter:1848] Loss: 0.538 | Acc: 72.167% \n",
      "[Epoch:19, Iter:1849] Loss: 0.538 | Acc: 72.122% \n",
      "[Epoch:19, Iter:1850] Loss: 0.537 | Acc: 72.240% \n",
      "[Epoch:19, Iter:1851] Loss: 0.537 | Acc: 72.275% \n",
      "[Epoch:19, Iter:1852] Loss: 0.536 | Acc: 72.269% \n",
      "[Epoch:19, Iter:1853] Loss: 0.536 | Acc: 72.377% \n",
      "[Epoch:19, Iter:1854] Loss: 0.534 | Acc: 72.407% \n",
      "[Epoch:19, Iter:1855] Loss: 0.536 | Acc: 72.327% \n",
      "[Epoch:19, Iter:1856] Loss: 0.536 | Acc: 72.250% \n",
      "[Epoch:19, Iter:1857] Loss: 0.536 | Acc: 72.211% \n",
      "[Epoch:19, Iter:1858] Loss: 0.537 | Acc: 72.069% \n",
      "[Epoch:19, Iter:1859] Loss: 0.536 | Acc: 72.237% \n",
      "[Epoch:19, Iter:1860] Loss: 0.538 | Acc: 72.300% \n",
      "[Epoch:19, Iter:1861] Loss: 0.537 | Acc: 72.361% \n",
      "[Epoch:19, Iter:1862] Loss: 0.541 | Acc: 72.161% \n",
      "[Epoch:19, Iter:1863] Loss: 0.541 | Acc: 72.159% \n",
      "[Epoch:19, Iter:1864] Loss: 0.541 | Acc: 72.188% \n",
      "[Epoch:19, Iter:1865] Loss: 0.541 | Acc: 72.215% \n",
      "[Epoch:19, Iter:1866] Loss: 0.542 | Acc: 72.152% \n",
      "[Epoch:19, Iter:1867] Loss: 0.543 | Acc: 72.179% \n",
      "[Epoch:19, Iter:1868] Loss: 0.542 | Acc: 72.265% \n",
      "[Epoch:19, Iter:1869] Loss: 0.542 | Acc: 72.290% \n",
      "[Epoch:19, Iter:1870] Loss: 0.543 | Acc: 72.229% \n",
      "[Epoch:19, Iter:1871] Loss: 0.544 | Acc: 72.169% \n",
      "[Epoch:19, Iter:1872] Loss: 0.543 | Acc: 72.250% \n",
      "[Epoch:19, Iter:1873] Loss: 0.542 | Acc: 72.356% \n",
      "[Epoch:19, Iter:1874] Loss: 0.542 | Acc: 72.432% \n",
      "[Epoch:19, Iter:1875] Loss: 0.542 | Acc: 72.507% \n",
      "[Epoch:19, Iter:1876] Loss: 0.542 | Acc: 72.447% \n",
      "[Epoch:19, Iter:1877] Loss: 0.545 | Acc: 72.338% \n",
      "[Epoch:19, Iter:1878] Loss: 0.544 | Acc: 72.385% \n",
      "[Epoch:19, Iter:1879] Loss: 0.545 | Acc: 72.278% \n",
      "[Epoch:19, Iter:1880] Loss: 0.545 | Acc: 72.250% \n",
      "[Epoch:19, Iter:1881] Loss: 0.545 | Acc: 72.321% \n",
      "[Epoch:19, Iter:1882] Loss: 0.544 | Acc: 72.341% \n",
      "[Epoch:19, Iter:1883] Loss: 0.545 | Acc: 72.386% \n",
      "[Epoch:19, Iter:1884] Loss: 0.546 | Acc: 72.310% \n",
      "[Epoch:19, Iter:1885] Loss: 0.545 | Acc: 72.400% \n",
      "[Epoch:19, Iter:1886] Loss: 0.546 | Acc: 72.419% \n",
      "[Epoch:19, Iter:1887] Loss: 0.546 | Acc: 72.391% \n",
      "[Epoch:19, Iter:1888] Loss: 0.546 | Acc: 72.364% \n",
      "[Epoch:19, Iter:1889] Loss: 0.546 | Acc: 72.360% \n",
      "[Epoch:19, Iter:1890] Loss: 0.546 | Acc: 72.400% \n",
      "[Epoch:19, Iter:1891] Loss: 0.545 | Acc: 72.440% \n",
      "[Epoch:19, Iter:1892] Loss: 0.545 | Acc: 72.391% \n",
      "[Epoch:19, Iter:1893] Loss: 0.546 | Acc: 72.387% \n",
      "[Epoch:19, Iter:1894] Loss: 0.545 | Acc: 72.426% \n",
      "[Epoch:19, Iter:1895] Loss: 0.548 | Acc: 72.358% \n",
      "[Epoch:19, Iter:1896] Loss: 0.548 | Acc: 72.417% \n",
      "[Epoch:19, Iter:1897] Loss: 0.548 | Acc: 72.392% \n",
      "[Epoch:19, Iter:1898] Loss: 0.548 | Acc: 72.367% \n",
      "[Epoch:19, Iter:1899] Loss: 0.547 | Acc: 72.444% \n",
      "[Epoch:19, Iter:1900] Loss: 0.547 | Acc: 72.460% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.080%\n",
      "Training set's accuracy (after quantization) is: 72.020%\n",
      "Test set's accuracy (before quantization) is: 72.400%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.541 | Train Acc: 73.080% | Test Loss: 0.546 | Test Acc: 72.400% \n",
      "Quantized Train Loss: 0.550 | Quantized Train Acc: 72.020% | Quantized Test Loss: 0.552 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 20\n",
      "[Epoch:20, Iter:1901] Loss: 0.619 | Acc: 68.000% \n",
      "[Epoch:20, Iter:1902] Loss: 0.650 | Acc: 66.000% \n",
      "[Epoch:20, Iter:1903] Loss: 0.608 | Acc: 66.000% \n",
      "[Epoch:20, Iter:1904] Loss: 0.576 | Acc: 69.000% \n",
      "[Epoch:20, Iter:1905] Loss: 0.587 | Acc: 68.000% \n",
      "[Epoch:20, Iter:1906] Loss: 0.590 | Acc: 67.000% \n",
      "[Epoch:20, Iter:1907] Loss: 0.582 | Acc: 68.286% \n",
      "[Epoch:20, Iter:1908] Loss: 0.572 | Acc: 69.500% \n",
      "[Epoch:20, Iter:1909] Loss: 0.573 | Acc: 69.778% \n",
      "[Epoch:20, Iter:1910] Loss: 0.568 | Acc: 69.600% \n",
      "[Epoch:20, Iter:1911] Loss: 0.582 | Acc: 69.818% \n",
      "[Epoch:20, Iter:1912] Loss: 0.585 | Acc: 69.333% \n",
      "[Epoch:20, Iter:1913] Loss: 0.591 | Acc: 68.923% \n",
      "[Epoch:20, Iter:1914] Loss: 0.588 | Acc: 69.429% \n",
      "[Epoch:20, Iter:1915] Loss: 0.573 | Acc: 70.133% \n",
      "[Epoch:20, Iter:1916] Loss: 0.571 | Acc: 70.375% \n",
      "[Epoch:20, Iter:1917] Loss: 0.561 | Acc: 71.529% \n",
      "[Epoch:20, Iter:1918] Loss: 0.557 | Acc: 71.667% \n",
      "[Epoch:20, Iter:1919] Loss: 0.562 | Acc: 71.474% \n",
      "[Epoch:20, Iter:1920] Loss: 0.556 | Acc: 72.100% \n",
      "[Epoch:20, Iter:1921] Loss: 0.550 | Acc: 72.286% \n",
      "[Epoch:20, Iter:1922] Loss: 0.548 | Acc: 72.364% \n",
      "[Epoch:20, Iter:1923] Loss: 0.548 | Acc: 72.435% \n",
      "[Epoch:20, Iter:1924] Loss: 0.548 | Acc: 72.667% \n",
      "[Epoch:20, Iter:1925] Loss: 0.547 | Acc: 72.960% \n",
      "[Epoch:20, Iter:1926] Loss: 0.546 | Acc: 73.000% \n",
      "[Epoch:20, Iter:1927] Loss: 0.542 | Acc: 73.333% \n",
      "[Epoch:20, Iter:1928] Loss: 0.546 | Acc: 73.000% \n",
      "[Epoch:20, Iter:1929] Loss: 0.543 | Acc: 73.103% \n",
      "[Epoch:20, Iter:1930] Loss: 0.545 | Acc: 72.933% \n",
      "[Epoch:20, Iter:1931] Loss: 0.546 | Acc: 72.645% \n",
      "[Epoch:20, Iter:1932] Loss: 0.545 | Acc: 72.750% \n",
      "[Epoch:20, Iter:1933] Loss: 0.544 | Acc: 72.909% \n",
      "[Epoch:20, Iter:1934] Loss: 0.544 | Acc: 72.824% \n",
      "[Epoch:20, Iter:1935] Loss: 0.544 | Acc: 72.800% \n",
      "[Epoch:20, Iter:1936] Loss: 0.543 | Acc: 72.722% \n",
      "[Epoch:20, Iter:1937] Loss: 0.543 | Acc: 72.595% \n",
      "[Epoch:20, Iter:1938] Loss: 0.544 | Acc: 72.421% \n",
      "[Epoch:20, Iter:1939] Loss: 0.545 | Acc: 72.359% \n",
      "[Epoch:20, Iter:1940] Loss: 0.544 | Acc: 72.350% \n",
      "[Epoch:20, Iter:1941] Loss: 0.543 | Acc: 72.488% \n",
      "[Epoch:20, Iter:1942] Loss: 0.541 | Acc: 72.476% \n",
      "[Epoch:20, Iter:1943] Loss: 0.542 | Acc: 72.465% \n",
      "[Epoch:20, Iter:1944] Loss: 0.541 | Acc: 72.591% \n",
      "[Epoch:20, Iter:1945] Loss: 0.538 | Acc: 72.844% \n",
      "[Epoch:20, Iter:1946] Loss: 0.535 | Acc: 73.087% \n",
      "[Epoch:20, Iter:1947] Loss: 0.535 | Acc: 73.021% \n",
      "[Epoch:20, Iter:1948] Loss: 0.535 | Acc: 73.083% \n",
      "[Epoch:20, Iter:1949] Loss: 0.535 | Acc: 73.102% \n",
      "[Epoch:20, Iter:1950] Loss: 0.533 | Acc: 73.240% \n",
      "[Epoch:20, Iter:1951] Loss: 0.535 | Acc: 73.098% \n",
      "[Epoch:20, Iter:1952] Loss: 0.538 | Acc: 72.885% \n",
      "[Epoch:20, Iter:1953] Loss: 0.537 | Acc: 72.981% \n",
      "[Epoch:20, Iter:1954] Loss: 0.536 | Acc: 73.000% \n",
      "[Epoch:20, Iter:1955] Loss: 0.537 | Acc: 73.018% \n",
      "[Epoch:20, Iter:1956] Loss: 0.536 | Acc: 73.036% \n",
      "[Epoch:20, Iter:1957] Loss: 0.534 | Acc: 73.158% \n",
      "[Epoch:20, Iter:1958] Loss: 0.533 | Acc: 73.207% \n",
      "[Epoch:20, Iter:1959] Loss: 0.535 | Acc: 73.119% \n",
      "[Epoch:20, Iter:1960] Loss: 0.535 | Acc: 73.100% \n",
      "[Epoch:20, Iter:1961] Loss: 0.536 | Acc: 73.016% \n",
      "[Epoch:20, Iter:1962] Loss: 0.535 | Acc: 73.065% \n",
      "[Epoch:20, Iter:1963] Loss: 0.539 | Acc: 72.825% \n",
      "[Epoch:20, Iter:1964] Loss: 0.540 | Acc: 72.781% \n",
      "[Epoch:20, Iter:1965] Loss: 0.540 | Acc: 72.800% \n",
      "[Epoch:20, Iter:1966] Loss: 0.540 | Acc: 72.788% \n",
      "[Epoch:20, Iter:1967] Loss: 0.539 | Acc: 72.836% \n",
      "[Epoch:20, Iter:1968] Loss: 0.540 | Acc: 72.676% \n",
      "[Epoch:20, Iter:1969] Loss: 0.541 | Acc: 72.580% \n",
      "[Epoch:20, Iter:1970] Loss: 0.541 | Acc: 72.657% \n",
      "[Epoch:20, Iter:1971] Loss: 0.539 | Acc: 72.761% \n",
      "[Epoch:20, Iter:1972] Loss: 0.539 | Acc: 72.778% \n",
      "[Epoch:20, Iter:1973] Loss: 0.540 | Acc: 72.685% \n",
      "[Epoch:20, Iter:1974] Loss: 0.540 | Acc: 72.757% \n",
      "[Epoch:20, Iter:1975] Loss: 0.540 | Acc: 72.800% \n",
      "[Epoch:20, Iter:1976] Loss: 0.541 | Acc: 72.842% \n",
      "[Epoch:20, Iter:1977] Loss: 0.542 | Acc: 72.779% \n",
      "[Epoch:20, Iter:1978] Loss: 0.541 | Acc: 72.846% \n",
      "[Epoch:20, Iter:1979] Loss: 0.541 | Acc: 72.911% \n",
      "[Epoch:20, Iter:1980] Loss: 0.540 | Acc: 72.925% \n",
      "[Epoch:20, Iter:1981] Loss: 0.540 | Acc: 72.963% \n",
      "[Epoch:20, Iter:1982] Loss: 0.540 | Acc: 72.902% \n",
      "[Epoch:20, Iter:1983] Loss: 0.542 | Acc: 72.843% \n",
      "[Epoch:20, Iter:1984] Loss: 0.542 | Acc: 72.786% \n",
      "[Epoch:20, Iter:1985] Loss: 0.542 | Acc: 72.682% \n",
      "[Epoch:20, Iter:1986] Loss: 0.543 | Acc: 72.628% \n",
      "[Epoch:20, Iter:1987] Loss: 0.543 | Acc: 72.575% \n",
      "[Epoch:20, Iter:1988] Loss: 0.544 | Acc: 72.455% \n",
      "[Epoch:20, Iter:1989] Loss: 0.545 | Acc: 72.382% \n",
      "[Epoch:20, Iter:1990] Loss: 0.544 | Acc: 72.467% \n",
      "[Epoch:20, Iter:1991] Loss: 0.544 | Acc: 72.484% \n",
      "[Epoch:20, Iter:1992] Loss: 0.543 | Acc: 72.522% \n",
      "[Epoch:20, Iter:1993] Loss: 0.543 | Acc: 72.624% \n",
      "[Epoch:20, Iter:1994] Loss: 0.544 | Acc: 72.553% \n",
      "[Epoch:20, Iter:1995] Loss: 0.544 | Acc: 72.547% \n",
      "[Epoch:20, Iter:1996] Loss: 0.544 | Acc: 72.562% \n",
      "[Epoch:20, Iter:1997] Loss: 0.544 | Acc: 72.598% \n",
      "[Epoch:20, Iter:1998] Loss: 0.543 | Acc: 72.551% \n",
      "[Epoch:20, Iter:1999] Loss: 0.543 | Acc: 72.525% \n",
      "[Epoch:20, Iter:2000] Loss: 0.543 | Acc: 72.500% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.580%\n",
      "Training set's accuracy (after quantization) is: 71.720%\n",
      "Test set's accuracy (before quantization) is: 71.700%\n",
      "Test set's accuracy (after quantization) is: 71.100%\n",
      "Train Loss: 0.549 | Train Acc: 72.580% | Test Loss: 0.552 | Test Acc: 71.700% \n",
      "Quantized Train Loss: 0.558 | Quantized Train Acc: 71.720% | Quantized Test Loss: 0.561 | Quantized Test Acc: 71.100% \n",
      "\n",
      "Epoch: 21\n",
      "[Epoch:21, Iter:2001] Loss: 0.551 | Acc: 74.000% \n",
      "[Epoch:21, Iter:2002] Loss: 0.508 | Acc: 75.000% \n",
      "[Epoch:21, Iter:2003] Loss: 0.552 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2004] Loss: 0.554 | Acc: 73.500% \n",
      "[Epoch:21, Iter:2005] Loss: 0.552 | Acc: 72.800% \n",
      "[Epoch:21, Iter:2006] Loss: 0.546 | Acc: 73.333% \n",
      "[Epoch:21, Iter:2007] Loss: 0.536 | Acc: 73.429% \n",
      "[Epoch:21, Iter:2008] Loss: 0.534 | Acc: 73.250% \n",
      "[Epoch:21, Iter:2009] Loss: 0.526 | Acc: 74.000% \n",
      "[Epoch:21, Iter:2010] Loss: 0.534 | Acc: 73.600% \n",
      "[Epoch:21, Iter:2011] Loss: 0.544 | Acc: 73.091% \n",
      "[Epoch:21, Iter:2012] Loss: 0.546 | Acc: 73.333% \n",
      "[Epoch:21, Iter:2013] Loss: 0.553 | Acc: 72.615% \n",
      "[Epoch:21, Iter:2014] Loss: 0.551 | Acc: 73.429% \n",
      "[Epoch:21, Iter:2015] Loss: 0.555 | Acc: 73.067% \n",
      "[Epoch:21, Iter:2016] Loss: 0.561 | Acc: 72.625% \n",
      "[Epoch:21, Iter:2017] Loss: 0.568 | Acc: 72.353% \n",
      "[Epoch:21, Iter:2018] Loss: 0.566 | Acc: 72.333% \n",
      "[Epoch:21, Iter:2019] Loss: 0.563 | Acc: 72.526% \n",
      "[Epoch:21, Iter:2020] Loss: 0.566 | Acc: 71.800% \n",
      "[Epoch:21, Iter:2021] Loss: 0.568 | Acc: 71.333% \n",
      "[Epoch:21, Iter:2022] Loss: 0.572 | Acc: 71.182% \n",
      "[Epoch:21, Iter:2023] Loss: 0.571 | Acc: 71.217% \n",
      "[Epoch:21, Iter:2024] Loss: 0.573 | Acc: 71.250% \n",
      "[Epoch:21, Iter:2025] Loss: 0.574 | Acc: 71.200% \n",
      "[Epoch:21, Iter:2026] Loss: 0.573 | Acc: 71.154% \n",
      "[Epoch:21, Iter:2027] Loss: 0.573 | Acc: 71.111% \n",
      "[Epoch:21, Iter:2028] Loss: 0.576 | Acc: 70.857% \n",
      "[Epoch:21, Iter:2029] Loss: 0.575 | Acc: 70.897% \n",
      "[Epoch:21, Iter:2030] Loss: 0.574 | Acc: 71.067% \n",
      "[Epoch:21, Iter:2031] Loss: 0.572 | Acc: 71.355% \n",
      "[Epoch:21, Iter:2032] Loss: 0.572 | Acc: 71.375% \n",
      "[Epoch:21, Iter:2033] Loss: 0.569 | Acc: 71.576% \n",
      "[Epoch:21, Iter:2034] Loss: 0.570 | Acc: 71.588% \n",
      "[Epoch:21, Iter:2035] Loss: 0.569 | Acc: 71.657% \n",
      "[Epoch:21, Iter:2036] Loss: 0.569 | Acc: 71.556% \n",
      "[Epoch:21, Iter:2037] Loss: 0.569 | Acc: 71.676% \n",
      "[Epoch:21, Iter:2038] Loss: 0.568 | Acc: 71.789% \n",
      "[Epoch:21, Iter:2039] Loss: 0.567 | Acc: 71.846% \n",
      "[Epoch:21, Iter:2040] Loss: 0.569 | Acc: 71.450% \n",
      "[Epoch:21, Iter:2041] Loss: 0.569 | Acc: 71.512% \n",
      "[Epoch:21, Iter:2042] Loss: 0.568 | Acc: 71.619% \n",
      "[Epoch:21, Iter:2043] Loss: 0.565 | Acc: 71.814% \n",
      "[Epoch:21, Iter:2044] Loss: 0.565 | Acc: 71.727% \n",
      "[Epoch:21, Iter:2045] Loss: 0.563 | Acc: 71.867% \n",
      "[Epoch:21, Iter:2046] Loss: 0.561 | Acc: 71.913% \n",
      "[Epoch:21, Iter:2047] Loss: 0.563 | Acc: 71.830% \n",
      "[Epoch:21, Iter:2048] Loss: 0.564 | Acc: 71.667% \n",
      "[Epoch:21, Iter:2049] Loss: 0.564 | Acc: 71.592% \n",
      "[Epoch:21, Iter:2050] Loss: 0.564 | Acc: 71.720% \n",
      "[Epoch:21, Iter:2051] Loss: 0.563 | Acc: 71.882% \n",
      "[Epoch:21, Iter:2052] Loss: 0.560 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2053] Loss: 0.561 | Acc: 71.925% \n",
      "[Epoch:21, Iter:2054] Loss: 0.560 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2055] Loss: 0.563 | Acc: 71.709% \n",
      "[Epoch:21, Iter:2056] Loss: 0.562 | Acc: 71.857% \n",
      "[Epoch:21, Iter:2057] Loss: 0.560 | Acc: 71.965% \n",
      "[Epoch:21, Iter:2058] Loss: 0.560 | Acc: 71.966% \n",
      "[Epoch:21, Iter:2059] Loss: 0.559 | Acc: 72.068% \n",
      "[Epoch:21, Iter:2060] Loss: 0.559 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2061] Loss: 0.558 | Acc: 71.967% \n",
      "[Epoch:21, Iter:2062] Loss: 0.558 | Acc: 72.065% \n",
      "[Epoch:21, Iter:2063] Loss: 0.557 | Acc: 71.968% \n",
      "[Epoch:21, Iter:2064] Loss: 0.556 | Acc: 72.031% \n",
      "[Epoch:21, Iter:2065] Loss: 0.556 | Acc: 71.969% \n",
      "[Epoch:21, Iter:2066] Loss: 0.557 | Acc: 71.818% \n",
      "[Epoch:21, Iter:2067] Loss: 0.556 | Acc: 71.821% \n",
      "[Epoch:21, Iter:2068] Loss: 0.557 | Acc: 71.794% \n",
      "[Epoch:21, Iter:2069] Loss: 0.555 | Acc: 71.913% \n",
      "[Epoch:21, Iter:2070] Loss: 0.554 | Acc: 71.914% \n",
      "[Epoch:21, Iter:2071] Loss: 0.554 | Acc: 71.944% \n",
      "[Epoch:21, Iter:2072] Loss: 0.553 | Acc: 72.028% \n",
      "[Epoch:21, Iter:2073] Loss: 0.553 | Acc: 72.082% \n",
      "[Epoch:21, Iter:2074] Loss: 0.552 | Acc: 72.108% \n",
      "[Epoch:21, Iter:2075] Loss: 0.550 | Acc: 72.240% \n",
      "[Epoch:21, Iter:2076] Loss: 0.549 | Acc: 72.289% \n",
      "[Epoch:21, Iter:2077] Loss: 0.551 | Acc: 72.234% \n",
      "[Epoch:21, Iter:2078] Loss: 0.549 | Acc: 72.385% \n",
      "[Epoch:21, Iter:2079] Loss: 0.549 | Acc: 72.456% \n",
      "[Epoch:21, Iter:2080] Loss: 0.548 | Acc: 72.475% \n",
      "[Epoch:21, Iter:2081] Loss: 0.548 | Acc: 72.568% \n",
      "[Epoch:21, Iter:2082] Loss: 0.548 | Acc: 72.537% \n",
      "[Epoch:21, Iter:2083] Loss: 0.547 | Acc: 72.602% \n",
      "[Epoch:21, Iter:2084] Loss: 0.548 | Acc: 72.524% \n",
      "[Epoch:21, Iter:2085] Loss: 0.548 | Acc: 72.447% \n",
      "[Epoch:21, Iter:2086] Loss: 0.548 | Acc: 72.512% \n",
      "[Epoch:21, Iter:2087] Loss: 0.549 | Acc: 72.414% \n",
      "[Epoch:21, Iter:2088] Loss: 0.549 | Acc: 72.386% \n",
      "[Epoch:21, Iter:2089] Loss: 0.549 | Acc: 72.404% \n",
      "[Epoch:21, Iter:2090] Loss: 0.549 | Acc: 72.378% \n",
      "[Epoch:21, Iter:2091] Loss: 0.548 | Acc: 72.418% \n",
      "[Epoch:21, Iter:2092] Loss: 0.549 | Acc: 72.413% \n",
      "[Epoch:21, Iter:2093] Loss: 0.549 | Acc: 72.409% \n",
      "[Epoch:21, Iter:2094] Loss: 0.549 | Acc: 72.404% \n",
      "[Epoch:21, Iter:2095] Loss: 0.548 | Acc: 72.442% \n",
      "[Epoch:21, Iter:2096] Loss: 0.548 | Acc: 72.417% \n",
      "[Epoch:21, Iter:2097] Loss: 0.547 | Acc: 72.474% \n",
      "[Epoch:21, Iter:2098] Loss: 0.546 | Acc: 72.469% \n",
      "[Epoch:21, Iter:2099] Loss: 0.545 | Acc: 72.566% \n",
      "[Epoch:21, Iter:2100] Loss: 0.545 | Acc: 72.520% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.000%\n",
      "Training set's accuracy (after quantization) is: 71.820%\n",
      "Test set's accuracy (before quantization) is: 71.300%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.550 | Train Acc: 72.000% | Test Loss: 0.554 | Test Acc: 71.300% \n",
      "Quantized Train Loss: 0.559 | Quantized Train Acc: 71.820% | Quantized Test Loss: 0.562 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 22\n",
      "[Epoch:22, Iter:2101] Loss: 0.622 | Acc: 66.000% \n",
      "[Epoch:22, Iter:2102] Loss: 0.586 | Acc: 68.000% \n",
      "[Epoch:22, Iter:2103] Loss: 0.558 | Acc: 71.333% \n",
      "[Epoch:22, Iter:2104] Loss: 0.544 | Acc: 71.000% \n",
      "[Epoch:22, Iter:2105] Loss: 0.541 | Acc: 70.800% \n",
      "[Epoch:22, Iter:2106] Loss: 0.530 | Acc: 71.667% \n",
      "[Epoch:22, Iter:2107] Loss: 0.528 | Acc: 71.429% \n",
      "[Epoch:22, Iter:2108] Loss: 0.528 | Acc: 71.250% \n",
      "[Epoch:22, Iter:2109] Loss: 0.529 | Acc: 72.000% \n",
      "[Epoch:22, Iter:2110] Loss: 0.526 | Acc: 72.600% \n",
      "[Epoch:22, Iter:2111] Loss: 0.514 | Acc: 73.455% \n",
      "[Epoch:22, Iter:2112] Loss: 0.509 | Acc: 73.500% \n",
      "[Epoch:22, Iter:2113] Loss: 0.503 | Acc: 73.846% \n",
      "[Epoch:22, Iter:2114] Loss: 0.509 | Acc: 73.571% \n",
      "[Epoch:22, Iter:2115] Loss: 0.506 | Acc: 74.000% \n",
      "[Epoch:22, Iter:2116] Loss: 0.514 | Acc: 73.500% \n",
      "[Epoch:22, Iter:2117] Loss: 0.514 | Acc: 73.412% \n",
      "[Epoch:22, Iter:2118] Loss: 0.517 | Acc: 73.444% \n",
      "[Epoch:22, Iter:2119] Loss: 0.516 | Acc: 73.263% \n",
      "[Epoch:22, Iter:2120] Loss: 0.524 | Acc: 72.500% \n",
      "[Epoch:22, Iter:2121] Loss: 0.531 | Acc: 71.810% \n",
      "[Epoch:22, Iter:2122] Loss: 0.530 | Acc: 71.909% \n",
      "[Epoch:22, Iter:2123] Loss: 0.531 | Acc: 71.739% \n",
      "[Epoch:22, Iter:2124] Loss: 0.538 | Acc: 71.333% \n",
      "[Epoch:22, Iter:2125] Loss: 0.537 | Acc: 71.280% \n",
      "[Epoch:22, Iter:2126] Loss: 0.538 | Acc: 71.231% \n",
      "[Epoch:22, Iter:2127] Loss: 0.542 | Acc: 70.963% \n",
      "[Epoch:22, Iter:2128] Loss: 0.545 | Acc: 71.071% \n",
      "[Epoch:22, Iter:2129] Loss: 0.543 | Acc: 71.310% \n",
      "[Epoch:22, Iter:2130] Loss: 0.547 | Acc: 70.933% \n",
      "[Epoch:22, Iter:2131] Loss: 0.545 | Acc: 71.161% \n",
      "[Epoch:22, Iter:2132] Loss: 0.543 | Acc: 71.188% \n",
      "[Epoch:22, Iter:2133] Loss: 0.548 | Acc: 71.030% \n",
      "[Epoch:22, Iter:2134] Loss: 0.548 | Acc: 71.059% \n",
      "[Epoch:22, Iter:2135] Loss: 0.547 | Acc: 71.143% \n",
      "[Epoch:22, Iter:2136] Loss: 0.545 | Acc: 71.222% \n",
      "[Epoch:22, Iter:2137] Loss: 0.544 | Acc: 71.405% \n",
      "[Epoch:22, Iter:2138] Loss: 0.545 | Acc: 71.263% \n",
      "[Epoch:22, Iter:2139] Loss: 0.545 | Acc: 71.436% \n",
      "[Epoch:22, Iter:2140] Loss: 0.544 | Acc: 71.650% \n",
      "[Epoch:22, Iter:2141] Loss: 0.544 | Acc: 71.610% \n",
      "[Epoch:22, Iter:2142] Loss: 0.544 | Acc: 71.714% \n",
      "[Epoch:22, Iter:2143] Loss: 0.542 | Acc: 71.907% \n",
      "[Epoch:22, Iter:2144] Loss: 0.541 | Acc: 71.955% \n",
      "[Epoch:22, Iter:2145] Loss: 0.544 | Acc: 71.733% \n",
      "[Epoch:22, Iter:2146] Loss: 0.546 | Acc: 71.478% \n",
      "[Epoch:22, Iter:2147] Loss: 0.546 | Acc: 71.489% \n",
      "[Epoch:22, Iter:2148] Loss: 0.546 | Acc: 71.417% \n",
      "[Epoch:22, Iter:2149] Loss: 0.543 | Acc: 71.592% \n",
      "[Epoch:22, Iter:2150] Loss: 0.547 | Acc: 71.280% \n",
      "[Epoch:22, Iter:2151] Loss: 0.549 | Acc: 71.294% \n",
      "[Epoch:22, Iter:2152] Loss: 0.550 | Acc: 71.269% \n",
      "[Epoch:22, Iter:2153] Loss: 0.549 | Acc: 71.321% \n",
      "[Epoch:22, Iter:2154] Loss: 0.549 | Acc: 71.296% \n",
      "[Epoch:22, Iter:2155] Loss: 0.550 | Acc: 71.309% \n",
      "[Epoch:22, Iter:2156] Loss: 0.551 | Acc: 71.250% \n",
      "[Epoch:22, Iter:2157] Loss: 0.551 | Acc: 71.263% \n",
      "[Epoch:22, Iter:2158] Loss: 0.551 | Acc: 71.172% \n",
      "[Epoch:22, Iter:2159] Loss: 0.552 | Acc: 71.119% \n",
      "[Epoch:22, Iter:2160] Loss: 0.551 | Acc: 71.233% \n",
      "[Epoch:22, Iter:2161] Loss: 0.549 | Acc: 71.377% \n",
      "[Epoch:22, Iter:2162] Loss: 0.550 | Acc: 71.355% \n",
      "[Epoch:22, Iter:2163] Loss: 0.550 | Acc: 71.460% \n",
      "[Epoch:22, Iter:2164] Loss: 0.549 | Acc: 71.438% \n",
      "[Epoch:22, Iter:2165] Loss: 0.548 | Acc: 71.538% \n",
      "[Epoch:22, Iter:2166] Loss: 0.546 | Acc: 71.727% \n",
      "[Epoch:22, Iter:2167] Loss: 0.545 | Acc: 71.731% \n",
      "[Epoch:22, Iter:2168] Loss: 0.545 | Acc: 71.706% \n",
      "[Epoch:22, Iter:2169] Loss: 0.547 | Acc: 71.681% \n",
      "[Epoch:22, Iter:2170] Loss: 0.546 | Acc: 71.771% \n",
      "[Epoch:22, Iter:2171] Loss: 0.547 | Acc: 71.746% \n",
      "[Epoch:22, Iter:2172] Loss: 0.546 | Acc: 71.778% \n",
      "[Epoch:22, Iter:2173] Loss: 0.547 | Acc: 71.726% \n",
      "[Epoch:22, Iter:2174] Loss: 0.550 | Acc: 71.459% \n",
      "[Epoch:22, Iter:2175] Loss: 0.550 | Acc: 71.360% \n",
      "[Epoch:22, Iter:2176] Loss: 0.549 | Acc: 71.474% \n",
      "[Epoch:22, Iter:2177] Loss: 0.548 | Acc: 71.558% \n",
      "[Epoch:22, Iter:2178] Loss: 0.547 | Acc: 71.615% \n",
      "[Epoch:22, Iter:2179] Loss: 0.547 | Acc: 71.646% \n",
      "[Epoch:22, Iter:2180] Loss: 0.547 | Acc: 71.625% \n",
      "[Epoch:22, Iter:2181] Loss: 0.547 | Acc: 71.630% \n",
      "[Epoch:22, Iter:2182] Loss: 0.546 | Acc: 71.707% \n",
      "[Epoch:22, Iter:2183] Loss: 0.546 | Acc: 71.735% \n",
      "[Epoch:22, Iter:2184] Loss: 0.545 | Acc: 71.786% \n",
      "[Epoch:22, Iter:2185] Loss: 0.546 | Acc: 71.694% \n",
      "[Epoch:22, Iter:2186] Loss: 0.545 | Acc: 71.744% \n",
      "[Epoch:22, Iter:2187] Loss: 0.544 | Acc: 71.816% \n",
      "[Epoch:22, Iter:2188] Loss: 0.544 | Acc: 71.864% \n",
      "[Epoch:22, Iter:2189] Loss: 0.545 | Acc: 71.820% \n",
      "[Epoch:22, Iter:2190] Loss: 0.544 | Acc: 71.867% \n",
      "[Epoch:22, Iter:2191] Loss: 0.544 | Acc: 71.890% \n",
      "[Epoch:22, Iter:2192] Loss: 0.544 | Acc: 71.848% \n",
      "[Epoch:22, Iter:2193] Loss: 0.545 | Acc: 71.763% \n",
      "[Epoch:22, Iter:2194] Loss: 0.545 | Acc: 71.787% \n",
      "[Epoch:22, Iter:2195] Loss: 0.545 | Acc: 71.811% \n",
      "[Epoch:22, Iter:2196] Loss: 0.546 | Acc: 71.750% \n",
      "[Epoch:22, Iter:2197] Loss: 0.546 | Acc: 71.753% \n",
      "[Epoch:22, Iter:2198] Loss: 0.545 | Acc: 71.816% \n",
      "[Epoch:22, Iter:2199] Loss: 0.545 | Acc: 71.859% \n",
      "[Epoch:22, Iter:2200] Loss: 0.544 | Acc: 71.940% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 71.800%\n",
      "Training set's accuracy (after quantization) is: 72.720%\n",
      "Test set's accuracy (before quantization) is: 71.800%\n",
      "Test set's accuracy (after quantization) is: 72.400%\n",
      "Train Loss: 0.568 | Train Acc: 71.800% | Test Loss: 0.573 | Test Acc: 71.800% \n",
      "Quantized Train Loss: 0.544 | Quantized Train Acc: 72.720% | Quantized Test Loss: 0.546 | Quantized Test Acc: 72.400% \n",
      "\n",
      "Epoch: 23\n",
      "[Epoch:23, Iter:2201] Loss: 0.499 | Acc: 70.000% \n",
      "[Epoch:23, Iter:2202] Loss: 0.488 | Acc: 74.000% \n",
      "[Epoch:23, Iter:2203] Loss: 0.485 | Acc: 75.333% \n",
      "[Epoch:23, Iter:2204] Loss: 0.488 | Acc: 76.000% \n",
      "[Epoch:23, Iter:2205] Loss: 0.487 | Acc: 75.200% \n",
      "[Epoch:23, Iter:2206] Loss: 0.496 | Acc: 75.000% \n",
      "[Epoch:23, Iter:2207] Loss: 0.498 | Acc: 74.571% \n",
      "[Epoch:23, Iter:2208] Loss: 0.496 | Acc: 75.500% \n",
      "[Epoch:23, Iter:2209] Loss: 0.502 | Acc: 74.889% \n",
      "[Epoch:23, Iter:2210] Loss: 0.504 | Acc: 74.800% \n",
      "[Epoch:23, Iter:2211] Loss: 0.512 | Acc: 74.182% \n",
      "[Epoch:23, Iter:2212] Loss: 0.517 | Acc: 74.167% \n",
      "[Epoch:23, Iter:2213] Loss: 0.518 | Acc: 74.000% \n",
      "[Epoch:23, Iter:2214] Loss: 0.515 | Acc: 74.286% \n",
      "[Epoch:23, Iter:2215] Loss: 0.511 | Acc: 74.533% \n",
      "[Epoch:23, Iter:2216] Loss: 0.510 | Acc: 74.375% \n",
      "[Epoch:23, Iter:2217] Loss: 0.509 | Acc: 74.706% \n",
      "[Epoch:23, Iter:2218] Loss: 0.514 | Acc: 74.556% \n",
      "[Epoch:23, Iter:2219] Loss: 0.521 | Acc: 74.211% \n",
      "[Epoch:23, Iter:2220] Loss: 0.527 | Acc: 73.700% \n",
      "[Epoch:23, Iter:2221] Loss: 0.527 | Acc: 73.714% \n",
      "[Epoch:23, Iter:2222] Loss: 0.524 | Acc: 73.818% \n",
      "[Epoch:23, Iter:2223] Loss: 0.531 | Acc: 73.304% \n",
      "[Epoch:23, Iter:2224] Loss: 0.533 | Acc: 73.333% \n",
      "[Epoch:23, Iter:2225] Loss: 0.532 | Acc: 73.440% \n",
      "[Epoch:23, Iter:2226] Loss: 0.531 | Acc: 73.538% \n",
      "[Epoch:23, Iter:2227] Loss: 0.529 | Acc: 73.926% \n",
      "[Epoch:23, Iter:2228] Loss: 0.533 | Acc: 73.714% \n",
      "[Epoch:23, Iter:2229] Loss: 0.533 | Acc: 73.655% \n",
      "[Epoch:23, Iter:2230] Loss: 0.529 | Acc: 73.933% \n",
      "[Epoch:23, Iter:2231] Loss: 0.527 | Acc: 74.129% \n",
      "[Epoch:23, Iter:2232] Loss: 0.527 | Acc: 74.062% \n",
      "[Epoch:23, Iter:2233] Loss: 0.529 | Acc: 73.879% \n",
      "[Epoch:23, Iter:2234] Loss: 0.530 | Acc: 73.706% \n",
      "[Epoch:23, Iter:2235] Loss: 0.532 | Acc: 73.543% \n",
      "[Epoch:23, Iter:2236] Loss: 0.531 | Acc: 73.667% \n",
      "[Epoch:23, Iter:2237] Loss: 0.529 | Acc: 73.892% \n",
      "[Epoch:23, Iter:2238] Loss: 0.529 | Acc: 73.789% \n",
      "[Epoch:23, Iter:2239] Loss: 0.530 | Acc: 73.795% \n",
      "[Epoch:23, Iter:2240] Loss: 0.533 | Acc: 73.650% \n",
      "[Epoch:23, Iter:2241] Loss: 0.533 | Acc: 73.659% \n",
      "[Epoch:23, Iter:2242] Loss: 0.532 | Acc: 73.571% \n",
      "[Epoch:23, Iter:2243] Loss: 0.532 | Acc: 73.721% \n",
      "[Epoch:23, Iter:2244] Loss: 0.533 | Acc: 73.727% \n",
      "[Epoch:23, Iter:2245] Loss: 0.533 | Acc: 73.600% \n",
      "[Epoch:23, Iter:2246] Loss: 0.533 | Acc: 73.565% \n",
      "[Epoch:23, Iter:2247] Loss: 0.534 | Acc: 73.489% \n",
      "[Epoch:23, Iter:2248] Loss: 0.533 | Acc: 73.542% \n",
      "[Epoch:23, Iter:2249] Loss: 0.536 | Acc: 73.347% \n",
      "[Epoch:23, Iter:2250] Loss: 0.538 | Acc: 73.120% \n",
      "[Epoch:23, Iter:2251] Loss: 0.537 | Acc: 73.137% \n",
      "[Epoch:23, Iter:2252] Loss: 0.537 | Acc: 73.269% \n",
      "[Epoch:23, Iter:2253] Loss: 0.536 | Acc: 73.283% \n",
      "[Epoch:23, Iter:2254] Loss: 0.537 | Acc: 73.148% \n",
      "[Epoch:23, Iter:2255] Loss: 0.538 | Acc: 72.945% \n",
      "[Epoch:23, Iter:2256] Loss: 0.536 | Acc: 73.143% \n",
      "[Epoch:23, Iter:2257] Loss: 0.538 | Acc: 73.193% \n",
      "[Epoch:23, Iter:2258] Loss: 0.538 | Acc: 73.172% \n",
      "[Epoch:23, Iter:2259] Loss: 0.538 | Acc: 73.153% \n",
      "[Epoch:23, Iter:2260] Loss: 0.536 | Acc: 73.267% \n",
      "[Epoch:23, Iter:2261] Loss: 0.535 | Acc: 73.377% \n",
      "[Epoch:23, Iter:2262] Loss: 0.535 | Acc: 73.290% \n",
      "[Epoch:23, Iter:2263] Loss: 0.534 | Acc: 73.270% \n",
      "[Epoch:23, Iter:2264] Loss: 0.535 | Acc: 73.188% \n",
      "[Epoch:23, Iter:2265] Loss: 0.535 | Acc: 73.108% \n",
      "[Epoch:23, Iter:2266] Loss: 0.537 | Acc: 73.121% \n",
      "[Epoch:23, Iter:2267] Loss: 0.537 | Acc: 73.075% \n",
      "[Epoch:23, Iter:2268] Loss: 0.539 | Acc: 73.059% \n",
      "[Epoch:23, Iter:2269] Loss: 0.541 | Acc: 72.957% \n",
      "[Epoch:23, Iter:2270] Loss: 0.542 | Acc: 72.943% \n",
      "[Epoch:23, Iter:2271] Loss: 0.541 | Acc: 72.958% \n",
      "[Epoch:23, Iter:2272] Loss: 0.544 | Acc: 72.750% \n",
      "[Epoch:23, Iter:2273] Loss: 0.544 | Acc: 72.658% \n",
      "[Epoch:23, Iter:2274] Loss: 0.544 | Acc: 72.676% \n",
      "[Epoch:23, Iter:2275] Loss: 0.545 | Acc: 72.640% \n",
      "[Epoch:23, Iter:2276] Loss: 0.545 | Acc: 72.658% \n",
      "[Epoch:23, Iter:2277] Loss: 0.545 | Acc: 72.701% \n",
      "[Epoch:23, Iter:2278] Loss: 0.546 | Acc: 72.641% \n",
      "[Epoch:23, Iter:2279] Loss: 0.545 | Acc: 72.734% \n",
      "[Epoch:23, Iter:2280] Loss: 0.545 | Acc: 72.775% \n",
      "[Epoch:23, Iter:2281] Loss: 0.547 | Acc: 72.691% \n",
      "[Epoch:23, Iter:2282] Loss: 0.547 | Acc: 72.683% \n",
      "[Epoch:23, Iter:2283] Loss: 0.547 | Acc: 72.747% \n",
      "[Epoch:23, Iter:2284] Loss: 0.546 | Acc: 72.738% \n",
      "[Epoch:23, Iter:2285] Loss: 0.545 | Acc: 72.776% \n",
      "[Epoch:23, Iter:2286] Loss: 0.545 | Acc: 72.791% \n",
      "[Epoch:23, Iter:2287] Loss: 0.546 | Acc: 72.713% \n",
      "[Epoch:23, Iter:2288] Loss: 0.547 | Acc: 72.614% \n",
      "[Epoch:23, Iter:2289] Loss: 0.547 | Acc: 72.629% \n",
      "[Epoch:23, Iter:2290] Loss: 0.547 | Acc: 72.689% \n",
      "[Epoch:23, Iter:2291] Loss: 0.548 | Acc: 72.637% \n",
      "[Epoch:23, Iter:2292] Loss: 0.548 | Acc: 72.522% \n",
      "[Epoch:23, Iter:2293] Loss: 0.548 | Acc: 72.495% \n",
      "[Epoch:23, Iter:2294] Loss: 0.549 | Acc: 72.468% \n",
      "[Epoch:23, Iter:2295] Loss: 0.548 | Acc: 72.568% \n",
      "[Epoch:23, Iter:2296] Loss: 0.548 | Acc: 72.521% \n",
      "[Epoch:23, Iter:2297] Loss: 0.548 | Acc: 72.454% \n",
      "[Epoch:23, Iter:2298] Loss: 0.548 | Acc: 72.510% \n",
      "[Epoch:23, Iter:2299] Loss: 0.548 | Acc: 72.424% \n",
      "[Epoch:23, Iter:2300] Loss: 0.548 | Acc: 72.400% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.720%\n",
      "Training set's accuracy (after quantization) is: 71.820%\n",
      "Test set's accuracy (before quantization) is: 71.700%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.545 | Train Acc: 72.720% | Test Loss: 0.549 | Test Acc: 71.700% \n",
      "Quantized Train Loss: 0.557 | Quantized Train Acc: 71.820% | Quantized Test Loss: 0.559 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 24\n",
      "[Epoch:24, Iter:2301] Loss: 0.546 | Acc: 64.000% \n",
      "[Epoch:24, Iter:2302] Loss: 0.576 | Acc: 65.000% \n",
      "[Epoch:24, Iter:2303] Loss: 0.567 | Acc: 68.667% \n",
      "[Epoch:24, Iter:2304] Loss: 0.540 | Acc: 72.000% \n",
      "[Epoch:24, Iter:2305] Loss: 0.533 | Acc: 72.000% \n",
      "[Epoch:24, Iter:2306] Loss: 0.529 | Acc: 73.000% \n",
      "[Epoch:24, Iter:2307] Loss: 0.528 | Acc: 73.429% \n",
      "[Epoch:24, Iter:2308] Loss: 0.517 | Acc: 74.250% \n",
      "[Epoch:24, Iter:2309] Loss: 0.512 | Acc: 74.222% \n",
      "[Epoch:24, Iter:2310] Loss: 0.510 | Acc: 74.200% \n",
      "[Epoch:24, Iter:2311] Loss: 0.505 | Acc: 74.909% \n",
      "[Epoch:24, Iter:2312] Loss: 0.503 | Acc: 75.167% \n",
      "[Epoch:24, Iter:2313] Loss: 0.498 | Acc: 75.846% \n",
      "[Epoch:24, Iter:2314] Loss: 0.491 | Acc: 76.286% \n",
      "[Epoch:24, Iter:2315] Loss: 0.501 | Acc: 75.733% \n",
      "[Epoch:24, Iter:2316] Loss: 0.501 | Acc: 75.750% \n",
      "[Epoch:24, Iter:2317] Loss: 0.506 | Acc: 75.294% \n",
      "[Epoch:24, Iter:2318] Loss: 0.508 | Acc: 75.222% \n",
      "[Epoch:24, Iter:2319] Loss: 0.514 | Acc: 74.737% \n",
      "[Epoch:24, Iter:2320] Loss: 0.513 | Acc: 74.900% \n",
      "[Epoch:24, Iter:2321] Loss: 0.514 | Acc: 75.048% \n",
      "[Epoch:24, Iter:2322] Loss: 0.519 | Acc: 74.727% \n",
      "[Epoch:24, Iter:2323] Loss: 0.517 | Acc: 75.217% \n",
      "[Epoch:24, Iter:2324] Loss: 0.520 | Acc: 74.833% \n",
      "[Epoch:24, Iter:2325] Loss: 0.522 | Acc: 74.640% \n",
      "[Epoch:24, Iter:2326] Loss: 0.524 | Acc: 74.385% \n",
      "[Epoch:24, Iter:2327] Loss: 0.528 | Acc: 74.370% \n",
      "[Epoch:24, Iter:2328] Loss: 0.531 | Acc: 74.000% \n",
      "[Epoch:24, Iter:2329] Loss: 0.532 | Acc: 74.138% \n",
      "[Epoch:24, Iter:2330] Loss: 0.534 | Acc: 73.933% \n",
      "[Epoch:24, Iter:2331] Loss: 0.532 | Acc: 74.065% \n",
      "[Epoch:24, Iter:2332] Loss: 0.534 | Acc: 73.875% \n",
      "[Epoch:24, Iter:2333] Loss: 0.535 | Acc: 73.697% \n",
      "[Epoch:24, Iter:2334] Loss: 0.533 | Acc: 73.765% \n",
      "[Epoch:24, Iter:2335] Loss: 0.536 | Acc: 73.543% \n",
      "[Epoch:24, Iter:2336] Loss: 0.538 | Acc: 73.556% \n",
      "[Epoch:24, Iter:2337] Loss: 0.540 | Acc: 73.351% \n",
      "[Epoch:24, Iter:2338] Loss: 0.539 | Acc: 73.474% \n",
      "[Epoch:24, Iter:2339] Loss: 0.542 | Acc: 73.333% \n",
      "[Epoch:24, Iter:2340] Loss: 0.544 | Acc: 73.150% \n",
      "[Epoch:24, Iter:2341] Loss: 0.545 | Acc: 73.024% \n",
      "[Epoch:24, Iter:2342] Loss: 0.545 | Acc: 73.048% \n",
      "[Epoch:24, Iter:2343] Loss: 0.545 | Acc: 72.884% \n",
      "[Epoch:24, Iter:2344] Loss: 0.547 | Acc: 72.909% \n",
      "[Epoch:24, Iter:2345] Loss: 0.547 | Acc: 72.889% \n",
      "[Epoch:24, Iter:2346] Loss: 0.544 | Acc: 73.000% \n",
      "[Epoch:24, Iter:2347] Loss: 0.544 | Acc: 72.979% \n",
      "[Epoch:24, Iter:2348] Loss: 0.544 | Acc: 73.042% \n",
      "[Epoch:24, Iter:2349] Loss: 0.545 | Acc: 72.816% \n",
      "[Epoch:24, Iter:2350] Loss: 0.548 | Acc: 72.560% \n",
      "[Epoch:24, Iter:2351] Loss: 0.548 | Acc: 72.627% \n",
      "[Epoch:24, Iter:2352] Loss: 0.550 | Acc: 72.577% \n",
      "[Epoch:24, Iter:2353] Loss: 0.551 | Acc: 72.491% \n",
      "[Epoch:24, Iter:2354] Loss: 0.549 | Acc: 72.704% \n",
      "[Epoch:24, Iter:2355] Loss: 0.549 | Acc: 72.691% \n",
      "[Epoch:24, Iter:2356] Loss: 0.549 | Acc: 72.643% \n",
      "[Epoch:24, Iter:2357] Loss: 0.548 | Acc: 72.807% \n",
      "[Epoch:24, Iter:2358] Loss: 0.550 | Acc: 72.690% \n",
      "[Epoch:24, Iter:2359] Loss: 0.551 | Acc: 72.576% \n",
      "[Epoch:24, Iter:2360] Loss: 0.552 | Acc: 72.433% \n",
      "[Epoch:24, Iter:2361] Loss: 0.551 | Acc: 72.393% \n",
      "[Epoch:24, Iter:2362] Loss: 0.551 | Acc: 72.323% \n",
      "[Epoch:24, Iter:2363] Loss: 0.551 | Acc: 72.317% \n",
      "[Epoch:24, Iter:2364] Loss: 0.551 | Acc: 72.281% \n",
      "[Epoch:24, Iter:2365] Loss: 0.552 | Acc: 72.246% \n",
      "[Epoch:24, Iter:2366] Loss: 0.552 | Acc: 72.121% \n",
      "[Epoch:24, Iter:2367] Loss: 0.552 | Acc: 72.239% \n",
      "[Epoch:24, Iter:2368] Loss: 0.550 | Acc: 72.441% \n",
      "[Epoch:24, Iter:2369] Loss: 0.551 | Acc: 72.290% \n",
      "[Epoch:24, Iter:2370] Loss: 0.551 | Acc: 72.229% \n",
      "[Epoch:24, Iter:2371] Loss: 0.553 | Acc: 72.225% \n",
      "[Epoch:24, Iter:2372] Loss: 0.552 | Acc: 72.333% \n",
      "[Epoch:24, Iter:2373] Loss: 0.551 | Acc: 72.301% \n",
      "[Epoch:24, Iter:2374] Loss: 0.552 | Acc: 72.162% \n",
      "[Epoch:24, Iter:2375] Loss: 0.554 | Acc: 71.973% \n",
      "[Epoch:24, Iter:2376] Loss: 0.554 | Acc: 71.947% \n",
      "[Epoch:24, Iter:2377] Loss: 0.553 | Acc: 72.000% \n",
      "[Epoch:24, Iter:2378] Loss: 0.553 | Acc: 71.974% \n",
      "[Epoch:24, Iter:2379] Loss: 0.553 | Acc: 71.924% \n",
      "[Epoch:24, Iter:2380] Loss: 0.553 | Acc: 71.975% \n",
      "[Epoch:24, Iter:2381] Loss: 0.552 | Acc: 72.000% \n",
      "[Epoch:24, Iter:2382] Loss: 0.552 | Acc: 71.927% \n",
      "[Epoch:24, Iter:2383] Loss: 0.551 | Acc: 72.120% \n",
      "[Epoch:24, Iter:2384] Loss: 0.550 | Acc: 72.167% \n",
      "[Epoch:24, Iter:2385] Loss: 0.554 | Acc: 71.953% \n",
      "[Epoch:24, Iter:2386] Loss: 0.554 | Acc: 71.930% \n",
      "[Epoch:24, Iter:2387] Loss: 0.553 | Acc: 72.115% \n",
      "[Epoch:24, Iter:2388] Loss: 0.552 | Acc: 72.068% \n",
      "[Epoch:24, Iter:2389] Loss: 0.553 | Acc: 72.000% \n",
      "[Epoch:24, Iter:2390] Loss: 0.551 | Acc: 72.089% \n",
      "[Epoch:24, Iter:2391] Loss: 0.552 | Acc: 72.044% \n",
      "[Epoch:24, Iter:2392] Loss: 0.552 | Acc: 72.065% \n",
      "[Epoch:24, Iter:2393] Loss: 0.551 | Acc: 72.151% \n",
      "[Epoch:24, Iter:2394] Loss: 0.550 | Acc: 72.128% \n",
      "[Epoch:24, Iter:2395] Loss: 0.549 | Acc: 72.126% \n",
      "[Epoch:24, Iter:2396] Loss: 0.549 | Acc: 72.167% \n",
      "[Epoch:24, Iter:2397] Loss: 0.548 | Acc: 72.206% \n",
      "[Epoch:24, Iter:2398] Loss: 0.549 | Acc: 72.143% \n",
      "[Epoch:24, Iter:2399] Loss: 0.548 | Acc: 72.202% \n",
      "[Epoch:24, Iter:2400] Loss: 0.548 | Acc: 72.160% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.960%\n",
      "Training set's accuracy (after quantization) is: 71.800%\n",
      "Test set's accuracy (before quantization) is: 72.300%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.540 | Train Acc: 72.960% | Test Loss: 0.544 | Test Acc: 72.300% \n",
      "Quantized Train Loss: 0.551 | Quantized Train Acc: 71.800% | Quantized Test Loss: 0.553 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 25\n",
      "[Epoch:25, Iter:2401] Loss: 0.490 | Acc: 78.000% \n",
      "[Epoch:25, Iter:2402] Loss: 0.493 | Acc: 76.000% \n",
      "[Epoch:25, Iter:2403] Loss: 0.493 | Acc: 74.667% \n",
      "[Epoch:25, Iter:2404] Loss: 0.532 | Acc: 71.500% \n",
      "[Epoch:25, Iter:2405] Loss: 0.525 | Acc: 74.000% \n",
      "[Epoch:25, Iter:2406] Loss: 0.525 | Acc: 73.667% \n",
      "[Epoch:25, Iter:2407] Loss: 0.523 | Acc: 74.000% \n",
      "[Epoch:25, Iter:2408] Loss: 0.534 | Acc: 74.250% \n",
      "[Epoch:25, Iter:2409] Loss: 0.526 | Acc: 75.111% \n",
      "[Epoch:25, Iter:2410] Loss: 0.545 | Acc: 73.800% \n",
      "[Epoch:25, Iter:2411] Loss: 0.538 | Acc: 74.545% \n",
      "[Epoch:25, Iter:2412] Loss: 0.536 | Acc: 74.833% \n",
      "[Epoch:25, Iter:2413] Loss: 0.534 | Acc: 74.923% \n",
      "[Epoch:25, Iter:2414] Loss: 0.536 | Acc: 75.000% \n",
      "[Epoch:25, Iter:2415] Loss: 0.533 | Acc: 74.800% \n",
      "[Epoch:25, Iter:2416] Loss: 0.530 | Acc: 74.875% \n",
      "[Epoch:25, Iter:2417] Loss: 0.538 | Acc: 74.353% \n",
      "[Epoch:25, Iter:2418] Loss: 0.539 | Acc: 74.333% \n",
      "[Epoch:25, Iter:2419] Loss: 0.536 | Acc: 74.421% \n",
      "[Epoch:25, Iter:2420] Loss: 0.533 | Acc: 74.400% \n",
      "[Epoch:25, Iter:2421] Loss: 0.537 | Acc: 74.000% \n",
      "[Epoch:25, Iter:2422] Loss: 0.535 | Acc: 74.273% \n",
      "[Epoch:25, Iter:2423] Loss: 0.536 | Acc: 74.348% \n",
      "[Epoch:25, Iter:2424] Loss: 0.538 | Acc: 73.917% \n",
      "[Epoch:25, Iter:2425] Loss: 0.536 | Acc: 74.240% \n",
      "[Epoch:25, Iter:2426] Loss: 0.531 | Acc: 74.692% \n",
      "[Epoch:25, Iter:2427] Loss: 0.537 | Acc: 74.370% \n",
      "[Epoch:25, Iter:2428] Loss: 0.535 | Acc: 74.286% \n",
      "[Epoch:25, Iter:2429] Loss: 0.536 | Acc: 74.207% \n",
      "[Epoch:25, Iter:2430] Loss: 0.534 | Acc: 74.200% \n",
      "[Epoch:25, Iter:2431] Loss: 0.538 | Acc: 73.742% \n",
      "[Epoch:25, Iter:2432] Loss: 0.537 | Acc: 73.812% \n",
      "[Epoch:25, Iter:2433] Loss: 0.535 | Acc: 74.000% \n",
      "[Epoch:25, Iter:2434] Loss: 0.533 | Acc: 74.000% \n",
      "[Epoch:25, Iter:2435] Loss: 0.534 | Acc: 73.829% \n",
      "[Epoch:25, Iter:2436] Loss: 0.533 | Acc: 73.667% \n",
      "[Epoch:25, Iter:2437] Loss: 0.533 | Acc: 73.676% \n",
      "[Epoch:25, Iter:2438] Loss: 0.533 | Acc: 73.737% \n",
      "[Epoch:25, Iter:2439] Loss: 0.534 | Acc: 73.692% \n",
      "[Epoch:25, Iter:2440] Loss: 0.535 | Acc: 73.700% \n",
      "[Epoch:25, Iter:2441] Loss: 0.536 | Acc: 73.366% \n",
      "[Epoch:25, Iter:2442] Loss: 0.535 | Acc: 73.571% \n",
      "[Epoch:25, Iter:2443] Loss: 0.536 | Acc: 73.488% \n",
      "[Epoch:25, Iter:2444] Loss: 0.536 | Acc: 73.364% \n",
      "[Epoch:25, Iter:2445] Loss: 0.537 | Acc: 73.333% \n",
      "[Epoch:25, Iter:2446] Loss: 0.539 | Acc: 73.174% \n",
      "[Epoch:25, Iter:2447] Loss: 0.538 | Acc: 73.064% \n",
      "[Epoch:25, Iter:2448] Loss: 0.538 | Acc: 73.042% \n",
      "[Epoch:25, Iter:2449] Loss: 0.539 | Acc: 72.939% \n",
      "[Epoch:25, Iter:2450] Loss: 0.540 | Acc: 72.880% \n",
      "[Epoch:25, Iter:2451] Loss: 0.540 | Acc: 72.863% \n",
      "[Epoch:25, Iter:2452] Loss: 0.539 | Acc: 72.923% \n",
      "[Epoch:25, Iter:2453] Loss: 0.536 | Acc: 73.094% \n",
      "[Epoch:25, Iter:2454] Loss: 0.536 | Acc: 73.185% \n",
      "[Epoch:25, Iter:2455] Loss: 0.537 | Acc: 73.091% \n",
      "[Epoch:25, Iter:2456] Loss: 0.535 | Acc: 73.179% \n",
      "[Epoch:25, Iter:2457] Loss: 0.537 | Acc: 73.053% \n",
      "[Epoch:25, Iter:2458] Loss: 0.536 | Acc: 73.138% \n",
      "[Epoch:25, Iter:2459] Loss: 0.536 | Acc: 73.186% \n",
      "[Epoch:25, Iter:2460] Loss: 0.537 | Acc: 73.133% \n",
      "[Epoch:25, Iter:2461] Loss: 0.540 | Acc: 72.918% \n",
      "[Epoch:25, Iter:2462] Loss: 0.541 | Acc: 72.871% \n",
      "[Epoch:25, Iter:2463] Loss: 0.540 | Acc: 73.016% \n",
      "[Epoch:25, Iter:2464] Loss: 0.540 | Acc: 73.000% \n",
      "[Epoch:25, Iter:2465] Loss: 0.540 | Acc: 72.985% \n",
      "[Epoch:25, Iter:2466] Loss: 0.540 | Acc: 72.879% \n",
      "[Epoch:25, Iter:2467] Loss: 0.540 | Acc: 72.866% \n",
      "[Epoch:25, Iter:2468] Loss: 0.541 | Acc: 72.882% \n",
      "[Epoch:25, Iter:2469] Loss: 0.542 | Acc: 72.928% \n",
      "[Epoch:25, Iter:2470] Loss: 0.540 | Acc: 72.943% \n",
      "[Epoch:25, Iter:2471] Loss: 0.540 | Acc: 73.042% \n",
      "[Epoch:25, Iter:2472] Loss: 0.540 | Acc: 73.056% \n",
      "[Epoch:25, Iter:2473] Loss: 0.539 | Acc: 73.151% \n",
      "[Epoch:25, Iter:2474] Loss: 0.539 | Acc: 73.135% \n",
      "[Epoch:25, Iter:2475] Loss: 0.538 | Acc: 73.333% \n",
      "[Epoch:25, Iter:2476] Loss: 0.540 | Acc: 73.237% \n",
      "[Epoch:25, Iter:2477] Loss: 0.541 | Acc: 73.169% \n",
      "[Epoch:25, Iter:2478] Loss: 0.542 | Acc: 73.128% \n",
      "[Epoch:25, Iter:2479] Loss: 0.542 | Acc: 73.089% \n",
      "[Epoch:25, Iter:2480] Loss: 0.542 | Acc: 73.000% \n",
      "[Epoch:25, Iter:2481] Loss: 0.543 | Acc: 72.889% \n",
      "[Epoch:25, Iter:2482] Loss: 0.543 | Acc: 72.976% \n",
      "[Epoch:25, Iter:2483] Loss: 0.543 | Acc: 73.060% \n",
      "[Epoch:25, Iter:2484] Loss: 0.542 | Acc: 73.071% \n",
      "[Epoch:25, Iter:2485] Loss: 0.546 | Acc: 72.941% \n",
      "[Epoch:25, Iter:2486] Loss: 0.545 | Acc: 72.930% \n",
      "[Epoch:25, Iter:2487] Loss: 0.543 | Acc: 73.011% \n",
      "[Epoch:25, Iter:2488] Loss: 0.545 | Acc: 72.886% \n",
      "[Epoch:25, Iter:2489] Loss: 0.545 | Acc: 72.921% \n",
      "[Epoch:25, Iter:2490] Loss: 0.545 | Acc: 72.933% \n",
      "[Epoch:25, Iter:2491] Loss: 0.545 | Acc: 72.879% \n",
      "[Epoch:25, Iter:2492] Loss: 0.545 | Acc: 72.870% \n",
      "[Epoch:25, Iter:2493] Loss: 0.546 | Acc: 72.839% \n",
      "[Epoch:25, Iter:2494] Loss: 0.545 | Acc: 72.851% \n",
      "[Epoch:25, Iter:2495] Loss: 0.545 | Acc: 72.842% \n",
      "[Epoch:25, Iter:2496] Loss: 0.546 | Acc: 72.750% \n",
      "[Epoch:25, Iter:2497] Loss: 0.547 | Acc: 72.742% \n",
      "[Epoch:25, Iter:2498] Loss: 0.547 | Acc: 72.735% \n",
      "[Epoch:25, Iter:2499] Loss: 0.547 | Acc: 72.687% \n",
      "[Epoch:25, Iter:2500] Loss: 0.547 | Acc: 72.680% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.940%\n",
      "Training set's accuracy (after quantization) is: 71.860%\n",
      "Test set's accuracy (before quantization) is: 72.100%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.540 | Train Acc: 72.940% | Test Loss: 0.544 | Test Acc: 72.100% \n",
      "Quantized Train Loss: 0.551 | Quantized Train Acc: 71.860% | Quantized Test Loss: 0.554 | Quantized Test Acc: 70.900% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394292613a8849e190ddd85b793391e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.135 MB uploaded\\r'), FloatProgress(value=0.009875410298944693, max=1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td></td></tr><tr><td>batch_gradient</td><td></td></tr><tr><td>quantized_accuracy</td><td></td></tr><tr><td>quantized_test_accuracy</td><td></td></tr><tr><td>quantized_test_loss</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>weight_distance</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>72.93999</td></tr><tr><td>batch_gradient</td><td>0.01134</td></tr><tr><td>quantized_accuracy</td><td>71.86</td></tr><tr><td>quantized_test_accuracy</td><td>70.9</td></tr><tr><td>quantized_test_loss</td><td>0.55399</td></tr><tr><td>test_accuracy</td><td>72.10001</td></tr><tr><td>test_loss</td><td>0.54401</td></tr><tr><td>weight_distance</td><td>0.40814</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SGD</strong> at: <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/86ignwg9' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/86ignwg9</a><br/> View project at: <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250711_035309-86ignwg9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SGD\n",
    "net, optimizer = init(\n",
    "    project_name=\"LogisticRegression_binary\",\n",
    "    opt_name=\"SGD\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    architecture=\"MLP\",\n",
    "    dataset_name=\"LogisticRegression\",\n",
    "    lr=LR,\n",
    ")\n",
    "\n",
    "lr_decay_epochs = [30]\n",
    "for decay_epoch in lr_decay_epochs:\n",
    "    if pre_epoch > decay_epoch:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] *= 0.5\n",
    "\n",
    "# Train\n",
    "for epoch in range(pre_epoch, EPOCH):\n",
    "    print(\"\\nEpoch: %d\" % (epoch + 1))\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    if epoch in lr_decay_epochs:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] *= 0.5\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # prepare dataset\n",
    "        length = len(trainloader)\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # forward & backward\n",
    "        outputs = net(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        sum_loss += loss.item()\n",
    "        predicted = torch.where(\n",
    "            outputs > 0.5,\n",
    "            torch.tensor(1.0, device=device),\n",
    "            torch.tensor(0.0, device=device),\n",
    "        )\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        print(\n",
    "            \"[Epoch:%d, Iter:%d] Loss: %.03f | Acc: %.3f%% \"\n",
    "            % (\n",
    "                epoch + 1,\n",
    "                (i + 1 + (epoch) * length),\n",
    "                sum_loss / (i + 1),\n",
    "                100.0 * correct / total,\n",
    "            )\n",
    "        )\n",
    "    print(\"Waiting Test...\")\n",
    "    # calculate weight_dist and batch_gradient\n",
    "    model_copy = copy.deepcopy(net)\n",
    "    weight_dist = torch.norm(w_star - model_copy.fc1.weight.cpu()).item()\n",
    "    model_copy.to(device)\n",
    "    model_copy.train()\n",
    "    weights = [p for name, p in net.named_parameters() if \"bias\" not in name]\n",
    "    bias = [p for name, p in net.named_parameters() if \"bias\" in name]\n",
    "    parameters = [\n",
    "        {\"params\": weights, \"tag\": \"weights\"},\n",
    "        {\"params\": bias, \"tag\": \"bias\"},\n",
    "    ]\n",
    "    optimizerx = optim.SGD(parameters, lr=LR)\n",
    "    grad = torch.zeros(model_copy.fc1.weight.shape, device=device)\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_copy(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        grad += model_copy.fc1.weight.grad\n",
    "        optimizerx.step()\n",
    "        model_copy.zero_grad()\n",
    "    grad = grad / len(trainloader)\n",
    "    batch_gradient = torch.norm(grad).item()\n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_acc = net.evaluate(\n",
    "            trainloader, criterion, device, eval=False, qt=False\n",
    "        )\n",
    "        qtrain_loss, qtrain_acc = net.evaluate(\n",
    "            trainloader, criterion, device, eval=False, qt=True\n",
    "        )\n",
    "        test_loss, test_acc = net.evaluate(\n",
    "            testloader, criterion, device, eval=True, qt=False\n",
    "        )\n",
    "        qtest_loss, qtest_acc = net.evaluate(\n",
    "            testloader, criterion, device, eval=True, qt=True\n",
    "        )\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"test_loss\": test_loss,\n",
    "                \"quantized_test_loss\": qtest_loss,\n",
    "                \"accuracy\": train_acc,\n",
    "                \"quantized_accuracy\": qtrain_acc,\n",
    "                \"test_accuracy\": test_acc,\n",
    "                \"quantized_test_accuracy\": qtest_acc,\n",
    "                \"weight_distance\": weight_dist,\n",
    "                \"batch_gradient\": batch_gradient,\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            \"Train Loss: %.03f | Train Acc: %.3f%% | Test Loss: %.03f | Test Acc: %.3f%% \"\n",
    "            % (\n",
    "                train_loss,\n",
    "                train_acc,\n",
    "                test_loss,\n",
    "                test_acc,\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Quantized Train Loss: %.03f | Quantized Train Acc: %.3f%% | Quantized Test Loss: %.03f | Quantized Test Acc: %.3f%% \"\n",
    "            % (\n",
    "                qtrain_loss,\n",
    "                qtrain_acc,\n",
    "                qtest_loss,\n",
    "                qtest_acc,\n",
    "            )\n",
    "        )\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69b9d46375a40e097c3b48ff9211a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888884685, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lenovo\\Desktop\\CUHK\\Research-2025\\exp\\wandb\\run-20250711_035346-sa3gxbth</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/sa3gxbth' target=\"_blank\">ASkewSGD</a></strong> to <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/sa3gxbth' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/sa3gxbth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "[Epoch:1, Iter:1] Loss: 0.760 | Acc: 28.000% \n",
      "[Epoch:1, Iter:2] Loss: 0.665 | Acc: 51.000% \n",
      "[Epoch:1, Iter:3] Loss: 0.632 | Acc: 59.333% \n",
      "[Epoch:1, Iter:4] Loss: 0.603 | Acc: 64.000% \n",
      "[Epoch:1, Iter:5] Loss: 0.593 | Acc: 66.000% \n",
      "[Epoch:1, Iter:6] Loss: 0.597 | Acc: 66.667% \n",
      "[Epoch:1, Iter:7] Loss: 0.590 | Acc: 68.000% \n",
      "[Epoch:1, Iter:8] Loss: 0.588 | Acc: 68.500% \n",
      "[Epoch:1, Iter:9] Loss: 0.595 | Acc: 68.000% \n",
      "[Epoch:1, Iter:10] Loss: 0.600 | Acc: 67.200% \n",
      "[Epoch:1, Iter:11] Loss: 0.602 | Acc: 66.727% \n",
      "[Epoch:1, Iter:12] Loss: 0.606 | Acc: 66.000% \n",
      "[Epoch:1, Iter:13] Loss: 0.604 | Acc: 66.769% \n",
      "[Epoch:1, Iter:14] Loss: 0.600 | Acc: 67.143% \n",
      "[Epoch:1, Iter:15] Loss: 0.599 | Acc: 67.467% \n",
      "[Epoch:1, Iter:16] Loss: 0.591 | Acc: 68.375% \n",
      "[Epoch:1, Iter:17] Loss: 0.586 | Acc: 68.941% \n",
      "[Epoch:1, Iter:18] Loss: 0.584 | Acc: 69.222% \n",
      "[Epoch:1, Iter:19] Loss: 0.576 | Acc: 69.895% \n",
      "[Epoch:1, Iter:20] Loss: 0.580 | Acc: 69.700% \n",
      "[Epoch:1, Iter:21] Loss: 0.580 | Acc: 69.810% \n",
      "[Epoch:1, Iter:22] Loss: 0.585 | Acc: 69.182% \n",
      "[Epoch:1, Iter:23] Loss: 0.585 | Acc: 69.304% \n",
      "[Epoch:1, Iter:24] Loss: 0.582 | Acc: 69.500% \n",
      "[Epoch:1, Iter:25] Loss: 0.584 | Acc: 69.360% \n",
      "[Epoch:1, Iter:26] Loss: 0.583 | Acc: 69.462% \n",
      "[Epoch:1, Iter:27] Loss: 0.581 | Acc: 69.704% \n",
      "[Epoch:1, Iter:28] Loss: 0.581 | Acc: 69.643% \n",
      "[Epoch:1, Iter:29] Loss: 0.576 | Acc: 70.000% \n",
      "[Epoch:1, Iter:30] Loss: 0.577 | Acc: 69.933% \n",
      "[Epoch:1, Iter:31] Loss: 0.579 | Acc: 69.677% \n",
      "[Epoch:1, Iter:32] Loss: 0.579 | Acc: 69.688% \n",
      "[Epoch:1, Iter:33] Loss: 0.578 | Acc: 69.758% \n",
      "[Epoch:1, Iter:34] Loss: 0.576 | Acc: 69.882% \n",
      "[Epoch:1, Iter:35] Loss: 0.579 | Acc: 69.657% \n",
      "[Epoch:1, Iter:36] Loss: 0.580 | Acc: 69.389% \n",
      "[Epoch:1, Iter:37] Loss: 0.583 | Acc: 68.919% \n",
      "[Epoch:1, Iter:38] Loss: 0.585 | Acc: 68.632% \n",
      "[Epoch:1, Iter:39] Loss: 0.586 | Acc: 68.667% \n",
      "[Epoch:1, Iter:40] Loss: 0.587 | Acc: 68.600% \n",
      "[Epoch:1, Iter:41] Loss: 0.586 | Acc: 68.537% \n",
      "[Epoch:1, Iter:42] Loss: 0.586 | Acc: 68.524% \n",
      "[Epoch:1, Iter:43] Loss: 0.586 | Acc: 68.465% \n",
      "[Epoch:1, Iter:44] Loss: 0.585 | Acc: 68.636% \n",
      "[Epoch:1, Iter:45] Loss: 0.583 | Acc: 68.800% \n",
      "[Epoch:1, Iter:46] Loss: 0.583 | Acc: 68.696% \n",
      "[Epoch:1, Iter:47] Loss: 0.583 | Acc: 68.638% \n",
      "[Epoch:1, Iter:48] Loss: 0.584 | Acc: 68.500% \n",
      "[Epoch:1, Iter:49] Loss: 0.583 | Acc: 68.612% \n",
      "[Epoch:1, Iter:50] Loss: 0.583 | Acc: 68.640% \n",
      "[Epoch:1, Iter:51] Loss: 0.582 | Acc: 68.784% \n",
      "[Epoch:1, Iter:52] Loss: 0.582 | Acc: 68.846% \n",
      "[Epoch:1, Iter:53] Loss: 0.583 | Acc: 68.717% \n",
      "[Epoch:1, Iter:54] Loss: 0.583 | Acc: 68.778% \n",
      "[Epoch:1, Iter:55] Loss: 0.582 | Acc: 68.873% \n",
      "[Epoch:1, Iter:56] Loss: 0.582 | Acc: 68.821% \n",
      "[Epoch:1, Iter:57] Loss: 0.582 | Acc: 68.912% \n",
      "[Epoch:1, Iter:58] Loss: 0.581 | Acc: 69.069% \n",
      "[Epoch:1, Iter:59] Loss: 0.580 | Acc: 69.153% \n",
      "[Epoch:1, Iter:60] Loss: 0.580 | Acc: 69.167% \n",
      "[Epoch:1, Iter:61] Loss: 0.581 | Acc: 69.049% \n",
      "[Epoch:1, Iter:62] Loss: 0.581 | Acc: 69.065% \n",
      "[Epoch:1, Iter:63] Loss: 0.581 | Acc: 69.048% \n",
      "[Epoch:1, Iter:64] Loss: 0.581 | Acc: 69.031% \n",
      "[Epoch:1, Iter:65] Loss: 0.580 | Acc: 69.138% \n",
      "[Epoch:1, Iter:66] Loss: 0.579 | Acc: 69.212% \n",
      "[Epoch:1, Iter:67] Loss: 0.578 | Acc: 69.284% \n",
      "[Epoch:1, Iter:68] Loss: 0.578 | Acc: 69.294% \n",
      "[Epoch:1, Iter:69] Loss: 0.577 | Acc: 69.275% \n",
      "[Epoch:1, Iter:70] Loss: 0.577 | Acc: 69.314% \n",
      "[Epoch:1, Iter:71] Loss: 0.576 | Acc: 69.408% \n",
      "[Epoch:1, Iter:72] Loss: 0.575 | Acc: 69.528% \n",
      "[Epoch:1, Iter:73] Loss: 0.574 | Acc: 69.616% \n",
      "[Epoch:1, Iter:74] Loss: 0.572 | Acc: 69.811% \n",
      "[Epoch:1, Iter:75] Loss: 0.574 | Acc: 69.653% \n",
      "[Epoch:1, Iter:76] Loss: 0.575 | Acc: 69.605% \n",
      "[Epoch:1, Iter:77] Loss: 0.574 | Acc: 69.714% \n",
      "[Epoch:1, Iter:78] Loss: 0.573 | Acc: 69.872% \n",
      "[Epoch:1, Iter:79] Loss: 0.572 | Acc: 69.899% \n",
      "[Epoch:1, Iter:80] Loss: 0.571 | Acc: 70.000% \n",
      "[Epoch:1, Iter:81] Loss: 0.572 | Acc: 70.025% \n",
      "[Epoch:1, Iter:82] Loss: 0.571 | Acc: 70.098% \n",
      "[Epoch:1, Iter:83] Loss: 0.571 | Acc: 70.241% \n",
      "[Epoch:1, Iter:84] Loss: 0.569 | Acc: 70.357% \n",
      "[Epoch:1, Iter:85] Loss: 0.571 | Acc: 70.212% \n",
      "[Epoch:1, Iter:86] Loss: 0.571 | Acc: 70.279% \n",
      "[Epoch:1, Iter:87] Loss: 0.570 | Acc: 70.414% \n",
      "[Epoch:1, Iter:88] Loss: 0.569 | Acc: 70.455% \n",
      "[Epoch:1, Iter:89] Loss: 0.570 | Acc: 70.404% \n",
      "[Epoch:1, Iter:90] Loss: 0.569 | Acc: 70.400% \n",
      "[Epoch:1, Iter:91] Loss: 0.569 | Acc: 70.440% \n",
      "[Epoch:1, Iter:92] Loss: 0.568 | Acc: 70.478% \n",
      "[Epoch:1, Iter:93] Loss: 0.568 | Acc: 70.516% \n",
      "[Epoch:1, Iter:94] Loss: 0.568 | Acc: 70.532% \n",
      "[Epoch:1, Iter:95] Loss: 0.567 | Acc: 70.674% \n",
      "[Epoch:1, Iter:96] Loss: 0.568 | Acc: 70.479% \n",
      "[Epoch:1, Iter:97] Loss: 0.568 | Acc: 70.515% \n",
      "[Epoch:1, Iter:98] Loss: 0.568 | Acc: 70.510% \n",
      "[Epoch:1, Iter:99] Loss: 0.568 | Acc: 70.545% \n",
      "[Epoch:1, Iter:100] Loss: 0.568 | Acc: 70.560% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.020%\n",
      "Training set's accuracy (after quantization) is: 72.920%\n",
      "Test set's accuracy (before quantization) is: 72.800%\n",
      "Test set's accuracy (after quantization) is: 73.000%\n",
      "Train Loss: 0.544 | Train Acc: 73.020% | Test Loss: 0.546 | Test Acc: 72.800% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.920% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.000% \n",
      "\n",
      "Epoch: 2\n",
      "[Epoch:2, Iter:101] Loss: 0.653 | Acc: 68.000% \n",
      "[Epoch:2, Iter:102] Loss: 0.602 | Acc: 69.000% \n",
      "[Epoch:2, Iter:103] Loss: 0.608 | Acc: 66.000% \n",
      "[Epoch:2, Iter:104] Loss: 0.619 | Acc: 64.500% \n",
      "[Epoch:2, Iter:105] Loss: 0.604 | Acc: 65.200% \n",
      "[Epoch:2, Iter:106] Loss: 0.594 | Acc: 66.000% \n",
      "[Epoch:2, Iter:107] Loss: 0.591 | Acc: 65.429% \n",
      "[Epoch:2, Iter:108] Loss: 0.587 | Acc: 67.000% \n",
      "[Epoch:2, Iter:109] Loss: 0.581 | Acc: 67.778% \n",
      "[Epoch:2, Iter:110] Loss: 0.567 | Acc: 69.600% \n",
      "[Epoch:2, Iter:111] Loss: 0.575 | Acc: 69.091% \n",
      "[Epoch:2, Iter:112] Loss: 0.568 | Acc: 70.333% \n",
      "[Epoch:2, Iter:113] Loss: 0.554 | Acc: 71.538% \n",
      "[Epoch:2, Iter:114] Loss: 0.559 | Acc: 71.429% \n",
      "[Epoch:2, Iter:115] Loss: 0.556 | Acc: 71.600% \n",
      "[Epoch:2, Iter:116] Loss: 0.554 | Acc: 71.500% \n",
      "[Epoch:2, Iter:117] Loss: 0.552 | Acc: 71.529% \n",
      "[Epoch:2, Iter:118] Loss: 0.560 | Acc: 71.333% \n",
      "[Epoch:2, Iter:119] Loss: 0.560 | Acc: 71.263% \n",
      "[Epoch:2, Iter:120] Loss: 0.558 | Acc: 71.400% \n",
      "[Epoch:2, Iter:121] Loss: 0.564 | Acc: 71.048% \n",
      "[Epoch:2, Iter:122] Loss: 0.567 | Acc: 70.273% \n",
      "[Epoch:2, Iter:123] Loss: 0.566 | Acc: 70.261% \n",
      "[Epoch:2, Iter:124] Loss: 0.564 | Acc: 70.500% \n",
      "[Epoch:2, Iter:125] Loss: 0.562 | Acc: 70.720% \n",
      "[Epoch:2, Iter:126] Loss: 0.563 | Acc: 70.615% \n",
      "[Epoch:2, Iter:127] Loss: 0.561 | Acc: 70.889% \n",
      "[Epoch:2, Iter:128] Loss: 0.560 | Acc: 70.929% \n",
      "[Epoch:2, Iter:129] Loss: 0.557 | Acc: 71.103% \n",
      "[Epoch:2, Iter:130] Loss: 0.559 | Acc: 70.800% \n",
      "[Epoch:2, Iter:131] Loss: 0.557 | Acc: 70.903% \n",
      "[Epoch:2, Iter:132] Loss: 0.557 | Acc: 70.812% \n",
      "[Epoch:2, Iter:133] Loss: 0.556 | Acc: 70.848% \n",
      "[Epoch:2, Iter:134] Loss: 0.556 | Acc: 70.824% \n",
      "[Epoch:2, Iter:135] Loss: 0.556 | Acc: 70.743% \n",
      "[Epoch:2, Iter:136] Loss: 0.558 | Acc: 70.556% \n",
      "[Epoch:2, Iter:137] Loss: 0.559 | Acc: 70.378% \n",
      "[Epoch:2, Iter:138] Loss: 0.558 | Acc: 70.526% \n",
      "[Epoch:2, Iter:139] Loss: 0.555 | Acc: 70.821% \n",
      "[Epoch:2, Iter:140] Loss: 0.556 | Acc: 70.800% \n",
      "[Epoch:2, Iter:141] Loss: 0.554 | Acc: 71.122% \n",
      "[Epoch:2, Iter:142] Loss: 0.559 | Acc: 70.810% \n",
      "[Epoch:2, Iter:143] Loss: 0.558 | Acc: 70.837% \n",
      "[Epoch:2, Iter:144] Loss: 0.558 | Acc: 70.909% \n",
      "[Epoch:2, Iter:145] Loss: 0.557 | Acc: 70.978% \n",
      "[Epoch:2, Iter:146] Loss: 0.555 | Acc: 71.174% \n",
      "[Epoch:2, Iter:147] Loss: 0.554 | Acc: 71.362% \n",
      "[Epoch:2, Iter:148] Loss: 0.553 | Acc: 71.375% \n",
      "[Epoch:2, Iter:149] Loss: 0.551 | Acc: 71.469% \n",
      "[Epoch:2, Iter:150] Loss: 0.551 | Acc: 71.520% \n",
      "[Epoch:2, Iter:151] Loss: 0.550 | Acc: 71.608% \n",
      "[Epoch:2, Iter:152] Loss: 0.547 | Acc: 71.885% \n",
      "[Epoch:2, Iter:153] Loss: 0.545 | Acc: 72.075% \n",
      "[Epoch:2, Iter:154] Loss: 0.545 | Acc: 72.111% \n",
      "[Epoch:2, Iter:155] Loss: 0.545 | Acc: 72.109% \n",
      "[Epoch:2, Iter:156] Loss: 0.546 | Acc: 72.071% \n",
      "[Epoch:2, Iter:157] Loss: 0.544 | Acc: 72.246% \n",
      "[Epoch:2, Iter:158] Loss: 0.544 | Acc: 72.276% \n",
      "[Epoch:2, Iter:159] Loss: 0.544 | Acc: 72.169% \n",
      "[Epoch:2, Iter:160] Loss: 0.542 | Acc: 72.367% \n",
      "[Epoch:2, Iter:161] Loss: 0.542 | Acc: 72.328% \n",
      "[Epoch:2, Iter:162] Loss: 0.542 | Acc: 72.387% \n",
      "[Epoch:2, Iter:163] Loss: 0.543 | Acc: 72.349% \n",
      "[Epoch:2, Iter:164] Loss: 0.543 | Acc: 72.312% \n",
      "[Epoch:2, Iter:165] Loss: 0.544 | Acc: 72.246% \n",
      "[Epoch:2, Iter:166] Loss: 0.546 | Acc: 72.121% \n",
      "[Epoch:2, Iter:167] Loss: 0.545 | Acc: 72.090% \n",
      "[Epoch:2, Iter:168] Loss: 0.545 | Acc: 72.059% \n",
      "[Epoch:2, Iter:169] Loss: 0.547 | Acc: 71.971% \n",
      "[Epoch:2, Iter:170] Loss: 0.547 | Acc: 71.914% \n",
      "[Epoch:2, Iter:171] Loss: 0.547 | Acc: 71.915% \n",
      "[Epoch:2, Iter:172] Loss: 0.545 | Acc: 72.056% \n",
      "[Epoch:2, Iter:173] Loss: 0.544 | Acc: 72.027% \n",
      "[Epoch:2, Iter:174] Loss: 0.545 | Acc: 71.919% \n",
      "[Epoch:2, Iter:175] Loss: 0.546 | Acc: 71.973% \n",
      "[Epoch:2, Iter:176] Loss: 0.546 | Acc: 71.974% \n",
      "[Epoch:2, Iter:177] Loss: 0.547 | Acc: 71.922% \n",
      "[Epoch:2, Iter:178] Loss: 0.548 | Acc: 71.897% \n",
      "[Epoch:2, Iter:179] Loss: 0.547 | Acc: 71.924% \n",
      "[Epoch:2, Iter:180] Loss: 0.547 | Acc: 71.925% \n",
      "[Epoch:2, Iter:181] Loss: 0.548 | Acc: 71.827% \n",
      "[Epoch:2, Iter:182] Loss: 0.548 | Acc: 71.829% \n",
      "[Epoch:2, Iter:183] Loss: 0.547 | Acc: 71.904% \n",
      "[Epoch:2, Iter:184] Loss: 0.547 | Acc: 71.881% \n",
      "[Epoch:2, Iter:185] Loss: 0.546 | Acc: 71.976% \n",
      "[Epoch:2, Iter:186] Loss: 0.546 | Acc: 72.023% \n",
      "[Epoch:2, Iter:187] Loss: 0.546 | Acc: 72.000% \n",
      "[Epoch:2, Iter:188] Loss: 0.546 | Acc: 71.977% \n",
      "[Epoch:2, Iter:189] Loss: 0.547 | Acc: 71.888% \n",
      "[Epoch:2, Iter:190] Loss: 0.546 | Acc: 72.000% \n",
      "[Epoch:2, Iter:191] Loss: 0.546 | Acc: 72.044% \n",
      "[Epoch:2, Iter:192] Loss: 0.546 | Acc: 72.022% \n",
      "[Epoch:2, Iter:193] Loss: 0.546 | Acc: 72.043% \n",
      "[Epoch:2, Iter:194] Loss: 0.547 | Acc: 72.064% \n",
      "[Epoch:2, Iter:195] Loss: 0.546 | Acc: 72.168% \n",
      "[Epoch:2, Iter:196] Loss: 0.546 | Acc: 72.208% \n",
      "[Epoch:2, Iter:197] Loss: 0.546 | Acc: 72.268% \n",
      "[Epoch:2, Iter:198] Loss: 0.546 | Acc: 72.245% \n",
      "[Epoch:2, Iter:199] Loss: 0.547 | Acc: 72.263% \n",
      "[Epoch:2, Iter:200] Loss: 0.547 | Acc: 72.220% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.980%\n",
      "Training set's accuracy (after quantization) is: 72.780%\n",
      "Test set's accuracy (before quantization) is: 72.900%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.540 | Train Acc: 72.980% | Test Loss: 0.544 | Test Acc: 72.900% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.780% | Quantized Test Loss: 0.544 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 3\n",
      "[Epoch:3, Iter:201] Loss: 0.591 | Acc: 66.000% \n",
      "[Epoch:3, Iter:202] Loss: 0.579 | Acc: 68.000% \n",
      "[Epoch:3, Iter:203] Loss: 0.551 | Acc: 70.000% \n",
      "[Epoch:3, Iter:204] Loss: 0.538 | Acc: 71.500% \n",
      "[Epoch:3, Iter:205] Loss: 0.524 | Acc: 72.400% \n",
      "[Epoch:3, Iter:206] Loss: 0.523 | Acc: 72.667% \n",
      "[Epoch:3, Iter:207] Loss: 0.540 | Acc: 71.429% \n",
      "[Epoch:3, Iter:208] Loss: 0.546 | Acc: 71.000% \n",
      "[Epoch:3, Iter:209] Loss: 0.540 | Acc: 71.556% \n",
      "[Epoch:3, Iter:210] Loss: 0.534 | Acc: 72.000% \n",
      "[Epoch:3, Iter:211] Loss: 0.526 | Acc: 73.091% \n",
      "[Epoch:3, Iter:212] Loss: 0.525 | Acc: 73.500% \n",
      "[Epoch:3, Iter:213] Loss: 0.527 | Acc: 73.846% \n",
      "[Epoch:3, Iter:214] Loss: 0.535 | Acc: 73.714% \n",
      "[Epoch:3, Iter:215] Loss: 0.539 | Acc: 73.067% \n",
      "[Epoch:3, Iter:216] Loss: 0.540 | Acc: 73.125% \n",
      "[Epoch:3, Iter:217] Loss: 0.536 | Acc: 73.412% \n",
      "[Epoch:3, Iter:218] Loss: 0.539 | Acc: 73.222% \n",
      "[Epoch:3, Iter:219] Loss: 0.541 | Acc: 72.842% \n",
      "[Epoch:3, Iter:220] Loss: 0.543 | Acc: 72.500% \n",
      "[Epoch:3, Iter:221] Loss: 0.539 | Acc: 72.762% \n",
      "[Epoch:3, Iter:222] Loss: 0.536 | Acc: 73.091% \n",
      "[Epoch:3, Iter:223] Loss: 0.540 | Acc: 72.696% \n",
      "[Epoch:3, Iter:224] Loss: 0.545 | Acc: 72.333% \n",
      "[Epoch:3, Iter:225] Loss: 0.546 | Acc: 72.240% \n",
      "[Epoch:3, Iter:226] Loss: 0.549 | Acc: 71.538% \n",
      "[Epoch:3, Iter:227] Loss: 0.548 | Acc: 71.778% \n",
      "[Epoch:3, Iter:228] Loss: 0.546 | Acc: 72.286% \n",
      "[Epoch:3, Iter:229] Loss: 0.541 | Acc: 72.690% \n",
      "[Epoch:3, Iter:230] Loss: 0.538 | Acc: 72.933% \n",
      "[Epoch:3, Iter:231] Loss: 0.535 | Acc: 73.032% \n",
      "[Epoch:3, Iter:232] Loss: 0.533 | Acc: 73.250% \n",
      "[Epoch:3, Iter:233] Loss: 0.537 | Acc: 73.091% \n",
      "[Epoch:3, Iter:234] Loss: 0.532 | Acc: 73.235% \n",
      "[Epoch:3, Iter:235] Loss: 0.533 | Acc: 73.314% \n",
      "[Epoch:3, Iter:236] Loss: 0.534 | Acc: 73.111% \n",
      "[Epoch:3, Iter:237] Loss: 0.535 | Acc: 73.189% \n",
      "[Epoch:3, Iter:238] Loss: 0.535 | Acc: 73.158% \n",
      "[Epoch:3, Iter:239] Loss: 0.540 | Acc: 72.821% \n",
      "[Epoch:3, Iter:240] Loss: 0.541 | Acc: 72.750% \n",
      "[Epoch:3, Iter:241] Loss: 0.542 | Acc: 72.683% \n",
      "[Epoch:3, Iter:242] Loss: 0.542 | Acc: 72.714% \n",
      "[Epoch:3, Iter:243] Loss: 0.544 | Acc: 72.605% \n",
      "[Epoch:3, Iter:244] Loss: 0.544 | Acc: 72.591% \n",
      "[Epoch:3, Iter:245] Loss: 0.545 | Acc: 72.533% \n",
      "[Epoch:3, Iter:246] Loss: 0.543 | Acc: 72.652% \n",
      "[Epoch:3, Iter:247] Loss: 0.542 | Acc: 72.681% \n",
      "[Epoch:3, Iter:248] Loss: 0.542 | Acc: 72.750% \n",
      "[Epoch:3, Iter:249] Loss: 0.542 | Acc: 72.694% \n",
      "[Epoch:3, Iter:250] Loss: 0.541 | Acc: 72.840% \n",
      "[Epoch:3, Iter:251] Loss: 0.540 | Acc: 72.941% \n",
      "[Epoch:3, Iter:252] Loss: 0.541 | Acc: 72.962% \n",
      "[Epoch:3, Iter:253] Loss: 0.540 | Acc: 73.019% \n",
      "[Epoch:3, Iter:254] Loss: 0.541 | Acc: 72.926% \n",
      "[Epoch:3, Iter:255] Loss: 0.542 | Acc: 72.727% \n",
      "[Epoch:3, Iter:256] Loss: 0.541 | Acc: 72.821% \n",
      "[Epoch:3, Iter:257] Loss: 0.542 | Acc: 72.702% \n",
      "[Epoch:3, Iter:258] Loss: 0.543 | Acc: 72.552% \n",
      "[Epoch:3, Iter:259] Loss: 0.544 | Acc: 72.407% \n",
      "[Epoch:3, Iter:260] Loss: 0.545 | Acc: 72.433% \n",
      "[Epoch:3, Iter:261] Loss: 0.545 | Acc: 72.426% \n",
      "[Epoch:3, Iter:262] Loss: 0.544 | Acc: 72.516% \n",
      "[Epoch:3, Iter:263] Loss: 0.544 | Acc: 72.444% \n",
      "[Epoch:3, Iter:264] Loss: 0.545 | Acc: 72.312% \n",
      "[Epoch:3, Iter:265] Loss: 0.545 | Acc: 72.277% \n",
      "[Epoch:3, Iter:266] Loss: 0.545 | Acc: 72.242% \n",
      "[Epoch:3, Iter:267] Loss: 0.545 | Acc: 72.239% \n",
      "[Epoch:3, Iter:268] Loss: 0.544 | Acc: 72.324% \n",
      "[Epoch:3, Iter:269] Loss: 0.544 | Acc: 72.319% \n",
      "[Epoch:3, Iter:270] Loss: 0.544 | Acc: 72.400% \n",
      "[Epoch:3, Iter:271] Loss: 0.543 | Acc: 72.423% \n",
      "[Epoch:3, Iter:272] Loss: 0.543 | Acc: 72.472% \n",
      "[Epoch:3, Iter:273] Loss: 0.544 | Acc: 72.466% \n",
      "[Epoch:3, Iter:274] Loss: 0.543 | Acc: 72.541% \n",
      "[Epoch:3, Iter:275] Loss: 0.543 | Acc: 72.453% \n",
      "[Epoch:3, Iter:276] Loss: 0.544 | Acc: 72.342% \n",
      "[Epoch:3, Iter:277] Loss: 0.544 | Acc: 72.364% \n",
      "[Epoch:3, Iter:278] Loss: 0.544 | Acc: 72.282% \n",
      "[Epoch:3, Iter:279] Loss: 0.545 | Acc: 72.278% \n",
      "[Epoch:3, Iter:280] Loss: 0.544 | Acc: 72.325% \n",
      "[Epoch:3, Iter:281] Loss: 0.544 | Acc: 72.469% \n",
      "[Epoch:3, Iter:282] Loss: 0.544 | Acc: 72.512% \n",
      "[Epoch:3, Iter:283] Loss: 0.544 | Acc: 72.578% \n",
      "[Epoch:3, Iter:284] Loss: 0.543 | Acc: 72.619% \n",
      "[Epoch:3, Iter:285] Loss: 0.543 | Acc: 72.659% \n",
      "[Epoch:3, Iter:286] Loss: 0.543 | Acc: 72.651% \n",
      "[Epoch:3, Iter:287] Loss: 0.544 | Acc: 72.598% \n",
      "[Epoch:3, Iter:288] Loss: 0.544 | Acc: 72.545% \n",
      "[Epoch:3, Iter:289] Loss: 0.543 | Acc: 72.629% \n",
      "[Epoch:3, Iter:290] Loss: 0.543 | Acc: 72.600% \n",
      "[Epoch:3, Iter:291] Loss: 0.542 | Acc: 72.637% \n",
      "[Epoch:3, Iter:292] Loss: 0.543 | Acc: 72.565% \n",
      "[Epoch:3, Iter:293] Loss: 0.544 | Acc: 72.538% \n",
      "[Epoch:3, Iter:294] Loss: 0.544 | Acc: 72.468% \n",
      "[Epoch:3, Iter:295] Loss: 0.544 | Acc: 72.505% \n",
      "[Epoch:3, Iter:296] Loss: 0.544 | Acc: 72.521% \n",
      "[Epoch:3, Iter:297] Loss: 0.543 | Acc: 72.660% \n",
      "[Epoch:3, Iter:298] Loss: 0.543 | Acc: 72.735% \n",
      "[Epoch:3, Iter:299] Loss: 0.543 | Acc: 72.788% \n",
      "[Epoch:3, Iter:300] Loss: 0.542 | Acc: 72.840% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.700%\n",
      "Training set's accuracy (after quantization) is: 72.760%\n",
      "Test set's accuracy (before quantization) is: 73.200%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.541 | Train Acc: 72.700% | Test Loss: 0.545 | Test Acc: 73.200% \n",
      "Quantized Train Loss: 0.542 | Quantized Train Acc: 72.760% | Quantized Test Loss: 0.544 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 4\n",
      "[Epoch:4, Iter:301] Loss: 0.586 | Acc: 70.000% \n",
      "[Epoch:4, Iter:302] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:4, Iter:303] Loss: 0.535 | Acc: 75.333% \n",
      "[Epoch:4, Iter:304] Loss: 0.521 | Acc: 74.500% \n",
      "[Epoch:4, Iter:305] Loss: 0.515 | Acc: 74.800% \n",
      "[Epoch:4, Iter:306] Loss: 0.510 | Acc: 74.667% \n",
      "[Epoch:4, Iter:307] Loss: 0.517 | Acc: 73.429% \n",
      "[Epoch:4, Iter:308] Loss: 0.526 | Acc: 73.000% \n",
      "[Epoch:4, Iter:309] Loss: 0.526 | Acc: 73.333% \n",
      "[Epoch:4, Iter:310] Loss: 0.525 | Acc: 73.400% \n",
      "[Epoch:4, Iter:311] Loss: 0.527 | Acc: 73.091% \n",
      "[Epoch:4, Iter:312] Loss: 0.539 | Acc: 72.167% \n",
      "[Epoch:4, Iter:313] Loss: 0.543 | Acc: 71.538% \n",
      "[Epoch:4, Iter:314] Loss: 0.538 | Acc: 71.857% \n",
      "[Epoch:4, Iter:315] Loss: 0.543 | Acc: 72.000% \n",
      "[Epoch:4, Iter:316] Loss: 0.534 | Acc: 72.875% \n",
      "[Epoch:4, Iter:317] Loss: 0.535 | Acc: 72.588% \n",
      "[Epoch:4, Iter:318] Loss: 0.536 | Acc: 72.556% \n",
      "[Epoch:4, Iter:319] Loss: 0.534 | Acc: 73.053% \n",
      "[Epoch:4, Iter:320] Loss: 0.532 | Acc: 73.200% \n",
      "[Epoch:4, Iter:321] Loss: 0.534 | Acc: 73.048% \n",
      "[Epoch:4, Iter:322] Loss: 0.534 | Acc: 72.909% \n",
      "[Epoch:4, Iter:323] Loss: 0.540 | Acc: 72.435% \n",
      "[Epoch:4, Iter:324] Loss: 0.540 | Acc: 72.333% \n",
      "[Epoch:4, Iter:325] Loss: 0.546 | Acc: 71.920% \n",
      "[Epoch:4, Iter:326] Loss: 0.547 | Acc: 72.000% \n",
      "[Epoch:4, Iter:327] Loss: 0.549 | Acc: 71.630% \n",
      "[Epoch:4, Iter:328] Loss: 0.551 | Acc: 71.500% \n",
      "[Epoch:4, Iter:329] Loss: 0.551 | Acc: 71.448% \n",
      "[Epoch:4, Iter:330] Loss: 0.550 | Acc: 71.467% \n",
      "[Epoch:4, Iter:331] Loss: 0.545 | Acc: 71.935% \n",
      "[Epoch:4, Iter:332] Loss: 0.547 | Acc: 71.875% \n",
      "[Epoch:4, Iter:333] Loss: 0.547 | Acc: 72.061% \n",
      "[Epoch:4, Iter:334] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:4, Iter:335] Loss: 0.551 | Acc: 71.829% \n",
      "[Epoch:4, Iter:336] Loss: 0.553 | Acc: 71.500% \n",
      "[Epoch:4, Iter:337] Loss: 0.550 | Acc: 71.676% \n",
      "[Epoch:4, Iter:338] Loss: 0.551 | Acc: 71.526% \n",
      "[Epoch:4, Iter:339] Loss: 0.551 | Acc: 71.436% \n",
      "[Epoch:4, Iter:340] Loss: 0.551 | Acc: 71.500% \n",
      "[Epoch:4, Iter:341] Loss: 0.549 | Acc: 71.707% \n",
      "[Epoch:4, Iter:342] Loss: 0.549 | Acc: 71.667% \n",
      "[Epoch:4, Iter:343] Loss: 0.550 | Acc: 71.674% \n",
      "[Epoch:4, Iter:344] Loss: 0.549 | Acc: 71.818% \n",
      "[Epoch:4, Iter:345] Loss: 0.549 | Acc: 71.956% \n",
      "[Epoch:4, Iter:346] Loss: 0.549 | Acc: 71.957% \n",
      "[Epoch:4, Iter:347] Loss: 0.550 | Acc: 71.915% \n",
      "[Epoch:4, Iter:348] Loss: 0.552 | Acc: 71.708% \n",
      "[Epoch:4, Iter:349] Loss: 0.553 | Acc: 71.510% \n",
      "[Epoch:4, Iter:350] Loss: 0.551 | Acc: 71.680% \n",
      "[Epoch:4, Iter:351] Loss: 0.551 | Acc: 71.725% \n",
      "[Epoch:4, Iter:352] Loss: 0.550 | Acc: 71.808% \n",
      "[Epoch:4, Iter:353] Loss: 0.552 | Acc: 71.623% \n",
      "[Epoch:4, Iter:354] Loss: 0.551 | Acc: 71.778% \n",
      "[Epoch:4, Iter:355] Loss: 0.550 | Acc: 71.745% \n",
      "[Epoch:4, Iter:356] Loss: 0.549 | Acc: 71.786% \n",
      "[Epoch:4, Iter:357] Loss: 0.549 | Acc: 71.754% \n",
      "[Epoch:4, Iter:358] Loss: 0.549 | Acc: 71.828% \n",
      "[Epoch:4, Iter:359] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:4, Iter:360] Loss: 0.548 | Acc: 71.900% \n",
      "[Epoch:4, Iter:361] Loss: 0.546 | Acc: 72.033% \n",
      "[Epoch:4, Iter:362] Loss: 0.545 | Acc: 72.097% \n",
      "[Epoch:4, Iter:363] Loss: 0.544 | Acc: 72.190% \n",
      "[Epoch:4, Iter:364] Loss: 0.543 | Acc: 72.344% \n",
      "[Epoch:4, Iter:365] Loss: 0.541 | Acc: 72.492% \n",
      "[Epoch:4, Iter:366] Loss: 0.539 | Acc: 72.636% \n",
      "[Epoch:4, Iter:367] Loss: 0.540 | Acc: 72.567% \n",
      "[Epoch:4, Iter:368] Loss: 0.540 | Acc: 72.559% \n",
      "[Epoch:4, Iter:369] Loss: 0.540 | Acc: 72.609% \n",
      "[Epoch:4, Iter:370] Loss: 0.538 | Acc: 72.829% \n",
      "[Epoch:4, Iter:371] Loss: 0.538 | Acc: 72.845% \n",
      "[Epoch:4, Iter:372] Loss: 0.541 | Acc: 72.611% \n",
      "[Epoch:4, Iter:373] Loss: 0.542 | Acc: 72.603% \n",
      "[Epoch:4, Iter:374] Loss: 0.543 | Acc: 72.568% \n",
      "[Epoch:4, Iter:375] Loss: 0.543 | Acc: 72.507% \n",
      "[Epoch:4, Iter:376] Loss: 0.544 | Acc: 72.447% \n",
      "[Epoch:4, Iter:377] Loss: 0.544 | Acc: 72.468% \n",
      "[Epoch:4, Iter:378] Loss: 0.544 | Acc: 72.487% \n",
      "[Epoch:4, Iter:379] Loss: 0.544 | Acc: 72.557% \n",
      "[Epoch:4, Iter:380] Loss: 0.543 | Acc: 72.725% \n",
      "[Epoch:4, Iter:381] Loss: 0.541 | Acc: 72.889% \n",
      "[Epoch:4, Iter:382] Loss: 0.543 | Acc: 72.780% \n",
      "[Epoch:4, Iter:383] Loss: 0.542 | Acc: 72.819% \n",
      "[Epoch:4, Iter:384] Loss: 0.542 | Acc: 72.738% \n",
      "[Epoch:4, Iter:385] Loss: 0.543 | Acc: 72.776% \n",
      "[Epoch:4, Iter:386] Loss: 0.545 | Acc: 72.651% \n",
      "[Epoch:4, Iter:387] Loss: 0.544 | Acc: 72.667% \n",
      "[Epoch:4, Iter:388] Loss: 0.544 | Acc: 72.682% \n",
      "[Epoch:4, Iter:389] Loss: 0.545 | Acc: 72.562% \n",
      "[Epoch:4, Iter:390] Loss: 0.544 | Acc: 72.644% \n",
      "[Epoch:4, Iter:391] Loss: 0.545 | Acc: 72.593% \n",
      "[Epoch:4, Iter:392] Loss: 0.545 | Acc: 72.522% \n",
      "[Epoch:4, Iter:393] Loss: 0.544 | Acc: 72.645% \n",
      "[Epoch:4, Iter:394] Loss: 0.545 | Acc: 72.553% \n",
      "[Epoch:4, Iter:395] Loss: 0.546 | Acc: 72.442% \n",
      "[Epoch:4, Iter:396] Loss: 0.545 | Acc: 72.500% \n",
      "[Epoch:4, Iter:397] Loss: 0.545 | Acc: 72.392% \n",
      "[Epoch:4, Iter:398] Loss: 0.545 | Acc: 72.388% \n",
      "[Epoch:4, Iter:399] Loss: 0.545 | Acc: 72.384% \n",
      "[Epoch:4, Iter:400] Loss: 0.545 | Acc: 72.340% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.680%\n",
      "Training set's accuracy (after quantization) is: 72.580%\n",
      "Test set's accuracy (before quantization) is: 72.900%\n",
      "Test set's accuracy (after quantization) is: 72.000%\n",
      "Train Loss: 0.540 | Train Acc: 72.680% | Test Loss: 0.544 | Test Acc: 72.900% \n",
      "Quantized Train Loss: 0.546 | Quantized Train Acc: 72.580% | Quantized Test Loss: 0.549 | Quantized Test Acc: 72.000% \n",
      "\n",
      "Epoch: 5\n",
      "[Epoch:5, Iter:401] Loss: 0.514 | Acc: 74.000% \n",
      "[Epoch:5, Iter:402] Loss: 0.543 | Acc: 69.000% \n",
      "[Epoch:5, Iter:403] Loss: 0.554 | Acc: 72.667% \n",
      "[Epoch:5, Iter:404] Loss: 0.579 | Acc: 70.500% \n",
      "[Epoch:5, Iter:405] Loss: 0.560 | Acc: 72.400% \n",
      "[Epoch:5, Iter:406] Loss: 0.539 | Acc: 73.667% \n",
      "[Epoch:5, Iter:407] Loss: 0.535 | Acc: 74.000% \n",
      "[Epoch:5, Iter:408] Loss: 0.536 | Acc: 73.250% \n",
      "[Epoch:5, Iter:409] Loss: 0.527 | Acc: 73.778% \n",
      "[Epoch:5, Iter:410] Loss: 0.531 | Acc: 73.200% \n",
      "[Epoch:5, Iter:411] Loss: 0.529 | Acc: 72.909% \n",
      "[Epoch:5, Iter:412] Loss: 0.521 | Acc: 73.667% \n",
      "[Epoch:5, Iter:413] Loss: 0.515 | Acc: 74.154% \n",
      "[Epoch:5, Iter:414] Loss: 0.518 | Acc: 74.143% \n",
      "[Epoch:5, Iter:415] Loss: 0.521 | Acc: 74.133% \n",
      "[Epoch:5, Iter:416] Loss: 0.522 | Acc: 74.125% \n",
      "[Epoch:5, Iter:417] Loss: 0.528 | Acc: 73.882% \n",
      "[Epoch:5, Iter:418] Loss: 0.534 | Acc: 73.333% \n",
      "[Epoch:5, Iter:419] Loss: 0.543 | Acc: 72.737% \n",
      "[Epoch:5, Iter:420] Loss: 0.548 | Acc: 72.300% \n",
      "[Epoch:5, Iter:421] Loss: 0.549 | Acc: 72.095% \n",
      "[Epoch:5, Iter:422] Loss: 0.549 | Acc: 72.182% \n",
      "[Epoch:5, Iter:423] Loss: 0.542 | Acc: 72.609% \n",
      "[Epoch:5, Iter:424] Loss: 0.540 | Acc: 72.750% \n",
      "[Epoch:5, Iter:425] Loss: 0.541 | Acc: 72.960% \n",
      "[Epoch:5, Iter:426] Loss: 0.545 | Acc: 72.769% \n",
      "[Epoch:5, Iter:427] Loss: 0.547 | Acc: 72.444% \n",
      "[Epoch:5, Iter:428] Loss: 0.547 | Acc: 72.714% \n",
      "[Epoch:5, Iter:429] Loss: 0.552 | Acc: 72.552% \n",
      "[Epoch:5, Iter:430] Loss: 0.551 | Acc: 72.600% \n",
      "[Epoch:5, Iter:431] Loss: 0.553 | Acc: 72.581% \n",
      "[Epoch:5, Iter:432] Loss: 0.550 | Acc: 72.812% \n",
      "[Epoch:5, Iter:433] Loss: 0.548 | Acc: 72.909% \n",
      "[Epoch:5, Iter:434] Loss: 0.549 | Acc: 72.941% \n",
      "[Epoch:5, Iter:435] Loss: 0.547 | Acc: 73.143% \n",
      "[Epoch:5, Iter:436] Loss: 0.546 | Acc: 73.222% \n",
      "[Epoch:5, Iter:437] Loss: 0.543 | Acc: 73.405% \n",
      "[Epoch:5, Iter:438] Loss: 0.545 | Acc: 73.158% \n",
      "[Epoch:5, Iter:439] Loss: 0.547 | Acc: 73.077% \n",
      "[Epoch:5, Iter:440] Loss: 0.549 | Acc: 72.900% \n",
      "[Epoch:5, Iter:441] Loss: 0.548 | Acc: 72.878% \n",
      "[Epoch:5, Iter:442] Loss: 0.547 | Acc: 72.905% \n",
      "[Epoch:5, Iter:443] Loss: 0.545 | Acc: 73.023% \n",
      "[Epoch:5, Iter:444] Loss: 0.541 | Acc: 73.455% \n",
      "[Epoch:5, Iter:445] Loss: 0.539 | Acc: 73.467% \n",
      "[Epoch:5, Iter:446] Loss: 0.540 | Acc: 73.522% \n",
      "[Epoch:5, Iter:447] Loss: 0.537 | Acc: 73.702% \n",
      "[Epoch:5, Iter:448] Loss: 0.537 | Acc: 73.500% \n",
      "[Epoch:5, Iter:449] Loss: 0.538 | Acc: 73.224% \n",
      "[Epoch:5, Iter:450] Loss: 0.540 | Acc: 73.120% \n",
      "[Epoch:5, Iter:451] Loss: 0.540 | Acc: 73.059% \n",
      "[Epoch:5, Iter:452] Loss: 0.543 | Acc: 72.885% \n",
      "[Epoch:5, Iter:453] Loss: 0.545 | Acc: 72.679% \n",
      "[Epoch:5, Iter:454] Loss: 0.546 | Acc: 72.667% \n",
      "[Epoch:5, Iter:455] Loss: 0.545 | Acc: 72.618% \n",
      "[Epoch:5, Iter:456] Loss: 0.547 | Acc: 72.429% \n",
      "[Epoch:5, Iter:457] Loss: 0.547 | Acc: 72.386% \n",
      "[Epoch:5, Iter:458] Loss: 0.549 | Acc: 72.345% \n",
      "[Epoch:5, Iter:459] Loss: 0.548 | Acc: 72.441% \n",
      "[Epoch:5, Iter:460] Loss: 0.545 | Acc: 72.667% \n",
      "[Epoch:5, Iter:461] Loss: 0.546 | Acc: 72.590% \n",
      "[Epoch:5, Iter:462] Loss: 0.547 | Acc: 72.613% \n",
      "[Epoch:5, Iter:463] Loss: 0.547 | Acc: 72.540% \n",
      "[Epoch:5, Iter:464] Loss: 0.547 | Acc: 72.656% \n",
      "[Epoch:5, Iter:465] Loss: 0.546 | Acc: 72.738% \n",
      "[Epoch:5, Iter:466] Loss: 0.544 | Acc: 72.879% \n",
      "[Epoch:5, Iter:467] Loss: 0.545 | Acc: 72.806% \n",
      "[Epoch:5, Iter:468] Loss: 0.545 | Acc: 72.735% \n",
      "[Epoch:5, Iter:469] Loss: 0.544 | Acc: 72.783% \n",
      "[Epoch:5, Iter:470] Loss: 0.543 | Acc: 72.886% \n",
      "[Epoch:5, Iter:471] Loss: 0.544 | Acc: 72.845% \n",
      "[Epoch:5, Iter:472] Loss: 0.544 | Acc: 72.833% \n",
      "[Epoch:5, Iter:473] Loss: 0.544 | Acc: 72.877% \n",
      "[Epoch:5, Iter:474] Loss: 0.544 | Acc: 72.919% \n",
      "[Epoch:5, Iter:475] Loss: 0.543 | Acc: 72.933% \n",
      "[Epoch:5, Iter:476] Loss: 0.544 | Acc: 72.921% \n",
      "[Epoch:5, Iter:477] Loss: 0.543 | Acc: 72.857% \n",
      "[Epoch:5, Iter:478] Loss: 0.543 | Acc: 72.795% \n",
      "[Epoch:5, Iter:479] Loss: 0.544 | Acc: 72.785% \n",
      "[Epoch:5, Iter:480] Loss: 0.543 | Acc: 72.825% \n",
      "[Epoch:5, Iter:481] Loss: 0.544 | Acc: 72.741% \n",
      "[Epoch:5, Iter:482] Loss: 0.545 | Acc: 72.732% \n",
      "[Epoch:5, Iter:483] Loss: 0.546 | Acc: 72.699% \n",
      "[Epoch:5, Iter:484] Loss: 0.547 | Acc: 72.548% \n",
      "[Epoch:5, Iter:485] Loss: 0.547 | Acc: 72.494% \n",
      "[Epoch:5, Iter:486] Loss: 0.547 | Acc: 72.442% \n",
      "[Epoch:5, Iter:487] Loss: 0.547 | Acc: 72.368% \n",
      "[Epoch:5, Iter:488] Loss: 0.547 | Acc: 72.409% \n",
      "[Epoch:5, Iter:489] Loss: 0.546 | Acc: 72.427% \n",
      "[Epoch:5, Iter:490] Loss: 0.545 | Acc: 72.533% \n",
      "[Epoch:5, Iter:491] Loss: 0.544 | Acc: 72.527% \n",
      "[Epoch:5, Iter:492] Loss: 0.543 | Acc: 72.543% \n",
      "[Epoch:5, Iter:493] Loss: 0.543 | Acc: 72.581% \n",
      "[Epoch:5, Iter:494] Loss: 0.544 | Acc: 72.489% \n",
      "[Epoch:5, Iter:495] Loss: 0.544 | Acc: 72.463% \n",
      "[Epoch:5, Iter:496] Loss: 0.545 | Acc: 72.417% \n",
      "[Epoch:5, Iter:497] Loss: 0.545 | Acc: 72.474% \n",
      "[Epoch:5, Iter:498] Loss: 0.545 | Acc: 72.429% \n",
      "[Epoch:5, Iter:499] Loss: 0.544 | Acc: 72.525% \n",
      "[Epoch:5, Iter:500] Loss: 0.544 | Acc: 72.520% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.500%\n",
      "Training set's accuracy (after quantization) is: 72.780%\n",
      "Test set's accuracy (before quantization) is: 72.900%\n",
      "Test set's accuracy (after quantization) is: 72.400%\n",
      "Train Loss: 0.547 | Train Acc: 72.500% | Test Loss: 0.551 | Test Acc: 72.900% \n",
      "Quantized Train Loss: 0.543 | Quantized Train Acc: 72.780% | Quantized Test Loss: 0.546 | Quantized Test Acc: 72.400% \n",
      "\n",
      "Epoch: 6\n",
      "[Epoch:6, Iter:501] Loss: 0.607 | Acc: 70.000% \n",
      "[Epoch:6, Iter:502] Loss: 0.521 | Acc: 76.000% \n",
      "[Epoch:6, Iter:503] Loss: 0.494 | Acc: 76.667% \n",
      "[Epoch:6, Iter:504] Loss: 0.507 | Acc: 76.500% \n",
      "[Epoch:6, Iter:505] Loss: 0.499 | Acc: 77.200% \n",
      "[Epoch:6, Iter:506] Loss: 0.486 | Acc: 77.333% \n",
      "[Epoch:6, Iter:507] Loss: 0.514 | Acc: 75.143% \n",
      "[Epoch:6, Iter:508] Loss: 0.512 | Acc: 75.000% \n",
      "[Epoch:6, Iter:509] Loss: 0.513 | Acc: 75.111% \n",
      "[Epoch:6, Iter:510] Loss: 0.509 | Acc: 75.000% \n",
      "[Epoch:6, Iter:511] Loss: 0.503 | Acc: 75.091% \n",
      "[Epoch:6, Iter:512] Loss: 0.508 | Acc: 74.500% \n",
      "[Epoch:6, Iter:513] Loss: 0.510 | Acc: 74.308% \n",
      "[Epoch:6, Iter:514] Loss: 0.509 | Acc: 74.857% \n",
      "[Epoch:6, Iter:515] Loss: 0.514 | Acc: 74.800% \n",
      "[Epoch:6, Iter:516] Loss: 0.521 | Acc: 74.625% \n",
      "[Epoch:6, Iter:517] Loss: 0.526 | Acc: 74.235% \n",
      "[Epoch:6, Iter:518] Loss: 0.525 | Acc: 74.000% \n",
      "[Epoch:6, Iter:519] Loss: 0.533 | Acc: 73.158% \n",
      "[Epoch:6, Iter:520] Loss: 0.532 | Acc: 73.200% \n",
      "[Epoch:6, Iter:521] Loss: 0.531 | Acc: 73.238% \n",
      "[Epoch:6, Iter:522] Loss: 0.532 | Acc: 73.091% \n",
      "[Epoch:6, Iter:523] Loss: 0.530 | Acc: 72.957% \n",
      "[Epoch:6, Iter:524] Loss: 0.530 | Acc: 72.917% \n",
      "[Epoch:6, Iter:525] Loss: 0.533 | Acc: 72.800% \n",
      "[Epoch:6, Iter:526] Loss: 0.532 | Acc: 73.077% \n",
      "[Epoch:6, Iter:527] Loss: 0.533 | Acc: 72.741% \n",
      "[Epoch:6, Iter:528] Loss: 0.531 | Acc: 73.000% \n",
      "[Epoch:6, Iter:529] Loss: 0.535 | Acc: 72.690% \n",
      "[Epoch:6, Iter:530] Loss: 0.536 | Acc: 72.600% \n",
      "[Epoch:6, Iter:531] Loss: 0.537 | Acc: 72.581% \n",
      "[Epoch:6, Iter:532] Loss: 0.538 | Acc: 72.625% \n",
      "[Epoch:6, Iter:533] Loss: 0.541 | Acc: 72.242% \n",
      "[Epoch:6, Iter:534] Loss: 0.540 | Acc: 72.412% \n",
      "[Epoch:6, Iter:535] Loss: 0.542 | Acc: 72.400% \n",
      "[Epoch:6, Iter:536] Loss: 0.543 | Acc: 72.278% \n",
      "[Epoch:6, Iter:537] Loss: 0.545 | Acc: 72.324% \n",
      "[Epoch:6, Iter:538] Loss: 0.543 | Acc: 72.632% \n",
      "[Epoch:6, Iter:539] Loss: 0.542 | Acc: 72.564% \n",
      "[Epoch:6, Iter:540] Loss: 0.543 | Acc: 72.550% \n",
      "[Epoch:6, Iter:541] Loss: 0.543 | Acc: 72.780% \n",
      "[Epoch:6, Iter:542] Loss: 0.541 | Acc: 72.810% \n",
      "[Epoch:6, Iter:543] Loss: 0.544 | Acc: 72.558% \n",
      "[Epoch:6, Iter:544] Loss: 0.545 | Acc: 72.409% \n",
      "[Epoch:6, Iter:545] Loss: 0.544 | Acc: 72.533% \n",
      "[Epoch:6, Iter:546] Loss: 0.543 | Acc: 72.565% \n",
      "[Epoch:6, Iter:547] Loss: 0.542 | Acc: 72.596% \n",
      "[Epoch:6, Iter:548] Loss: 0.541 | Acc: 72.750% \n",
      "[Epoch:6, Iter:549] Loss: 0.540 | Acc: 72.735% \n",
      "[Epoch:6, Iter:550] Loss: 0.540 | Acc: 72.720% \n",
      "[Epoch:6, Iter:551] Loss: 0.541 | Acc: 72.706% \n",
      "[Epoch:6, Iter:552] Loss: 0.539 | Acc: 72.808% \n",
      "[Epoch:6, Iter:553] Loss: 0.540 | Acc: 72.830% \n",
      "[Epoch:6, Iter:554] Loss: 0.538 | Acc: 72.852% \n",
      "[Epoch:6, Iter:555] Loss: 0.541 | Acc: 72.655% \n",
      "[Epoch:6, Iter:556] Loss: 0.542 | Acc: 72.679% \n",
      "[Epoch:6, Iter:557] Loss: 0.542 | Acc: 72.561% \n",
      "[Epoch:6, Iter:558] Loss: 0.543 | Acc: 72.448% \n",
      "[Epoch:6, Iter:559] Loss: 0.542 | Acc: 72.407% \n",
      "[Epoch:6, Iter:560] Loss: 0.543 | Acc: 72.300% \n",
      "[Epoch:6, Iter:561] Loss: 0.545 | Acc: 72.131% \n",
      "[Epoch:6, Iter:562] Loss: 0.545 | Acc: 72.161% \n",
      "[Epoch:6, Iter:563] Loss: 0.544 | Acc: 72.286% \n",
      "[Epoch:6, Iter:564] Loss: 0.545 | Acc: 72.188% \n",
      "[Epoch:6, Iter:565] Loss: 0.545 | Acc: 72.215% \n",
      "[Epoch:6, Iter:566] Loss: 0.546 | Acc: 72.121% \n",
      "[Epoch:6, Iter:567] Loss: 0.545 | Acc: 72.179% \n",
      "[Epoch:6, Iter:568] Loss: 0.546 | Acc: 72.147% \n",
      "[Epoch:6, Iter:569] Loss: 0.546 | Acc: 72.145% \n",
      "[Epoch:6, Iter:570] Loss: 0.547 | Acc: 72.114% \n",
      "[Epoch:6, Iter:571] Loss: 0.546 | Acc: 72.141% \n",
      "[Epoch:6, Iter:572] Loss: 0.547 | Acc: 72.111% \n",
      "[Epoch:6, Iter:573] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:6, Iter:574] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:6, Iter:575] Loss: 0.547 | Acc: 72.080% \n",
      "[Epoch:6, Iter:576] Loss: 0.547 | Acc: 72.079% \n",
      "[Epoch:6, Iter:577] Loss: 0.544 | Acc: 72.286% \n",
      "[Epoch:6, Iter:578] Loss: 0.543 | Acc: 72.333% \n",
      "[Epoch:6, Iter:579] Loss: 0.542 | Acc: 72.405% \n",
      "[Epoch:6, Iter:580] Loss: 0.543 | Acc: 72.350% \n",
      "[Epoch:6, Iter:581] Loss: 0.544 | Acc: 72.247% \n",
      "[Epoch:6, Iter:582] Loss: 0.544 | Acc: 72.244% \n",
      "[Epoch:6, Iter:583] Loss: 0.544 | Acc: 72.169% \n",
      "[Epoch:6, Iter:584] Loss: 0.544 | Acc: 72.167% \n",
      "[Epoch:6, Iter:585] Loss: 0.544 | Acc: 72.212% \n",
      "[Epoch:6, Iter:586] Loss: 0.544 | Acc: 72.256% \n",
      "[Epoch:6, Iter:587] Loss: 0.543 | Acc: 72.276% \n",
      "[Epoch:6, Iter:588] Loss: 0.544 | Acc: 72.250% \n",
      "[Epoch:6, Iter:589] Loss: 0.545 | Acc: 72.202% \n",
      "[Epoch:6, Iter:590] Loss: 0.545 | Acc: 72.267% \n",
      "[Epoch:6, Iter:591] Loss: 0.545 | Acc: 72.286% \n",
      "[Epoch:6, Iter:592] Loss: 0.545 | Acc: 72.348% \n",
      "[Epoch:6, Iter:593] Loss: 0.544 | Acc: 72.409% \n",
      "[Epoch:6, Iter:594] Loss: 0.543 | Acc: 72.489% \n",
      "[Epoch:6, Iter:595] Loss: 0.543 | Acc: 72.505% \n",
      "[Epoch:6, Iter:596] Loss: 0.544 | Acc: 72.417% \n",
      "[Epoch:6, Iter:597] Loss: 0.543 | Acc: 72.454% \n",
      "[Epoch:6, Iter:598] Loss: 0.544 | Acc: 72.347% \n",
      "[Epoch:6, Iter:599] Loss: 0.545 | Acc: 72.364% \n",
      "[Epoch:6, Iter:600] Loss: 0.545 | Acc: 72.380% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.880%\n",
      "Training set's accuracy (after quantization) is: 72.240%\n",
      "Test set's accuracy (before quantization) is: 72.500%\n",
      "Test set's accuracy (after quantization) is: 71.300%\n",
      "Train Loss: 0.540 | Train Acc: 72.880% | Test Loss: 0.544 | Test Acc: 72.500% \n",
      "Quantized Train Loss: 0.548 | Quantized Train Acc: 72.240% | Quantized Test Loss: 0.550 | Quantized Test Acc: 71.300% \n",
      "\n",
      "Epoch: 7\n",
      "[Epoch:7, Iter:601] Loss: 0.594 | Acc: 70.000% \n",
      "[Epoch:7, Iter:602] Loss: 0.591 | Acc: 74.000% \n",
      "[Epoch:7, Iter:603] Loss: 0.571 | Acc: 74.000% \n",
      "[Epoch:7, Iter:604] Loss: 0.558 | Acc: 75.000% \n",
      "[Epoch:7, Iter:605] Loss: 0.553 | Acc: 75.200% \n",
      "[Epoch:7, Iter:606] Loss: 0.534 | Acc: 75.333% \n",
      "[Epoch:7, Iter:607] Loss: 0.529 | Acc: 74.857% \n",
      "[Epoch:7, Iter:608] Loss: 0.524 | Acc: 74.750% \n",
      "[Epoch:7, Iter:609] Loss: 0.517 | Acc: 75.111% \n",
      "[Epoch:7, Iter:610] Loss: 0.524 | Acc: 74.800% \n",
      "[Epoch:7, Iter:611] Loss: 0.523 | Acc: 74.909% \n",
      "[Epoch:7, Iter:612] Loss: 0.529 | Acc: 74.167% \n",
      "[Epoch:7, Iter:613] Loss: 0.525 | Acc: 74.000% \n",
      "[Epoch:7, Iter:614] Loss: 0.526 | Acc: 73.857% \n",
      "[Epoch:7, Iter:615] Loss: 0.529 | Acc: 73.333% \n",
      "[Epoch:7, Iter:616] Loss: 0.533 | Acc: 73.250% \n",
      "[Epoch:7, Iter:617] Loss: 0.535 | Acc: 72.706% \n",
      "[Epoch:7, Iter:618] Loss: 0.536 | Acc: 72.778% \n",
      "[Epoch:7, Iter:619] Loss: 0.543 | Acc: 72.000% \n",
      "[Epoch:7, Iter:620] Loss: 0.546 | Acc: 72.000% \n",
      "[Epoch:7, Iter:621] Loss: 0.543 | Acc: 72.190% \n",
      "[Epoch:7, Iter:622] Loss: 0.546 | Acc: 71.818% \n",
      "[Epoch:7, Iter:623] Loss: 0.543 | Acc: 71.913% \n",
      "[Epoch:7, Iter:624] Loss: 0.547 | Acc: 71.417% \n",
      "[Epoch:7, Iter:625] Loss: 0.547 | Acc: 71.600% \n",
      "[Epoch:7, Iter:626] Loss: 0.542 | Acc: 72.000% \n",
      "[Epoch:7, Iter:627] Loss: 0.547 | Acc: 71.630% \n",
      "[Epoch:7, Iter:628] Loss: 0.546 | Acc: 71.643% \n",
      "[Epoch:7, Iter:629] Loss: 0.548 | Acc: 71.448% \n",
      "[Epoch:7, Iter:630] Loss: 0.546 | Acc: 71.733% \n",
      "[Epoch:7, Iter:631] Loss: 0.548 | Acc: 71.806% \n",
      "[Epoch:7, Iter:632] Loss: 0.547 | Acc: 71.938% \n",
      "[Epoch:7, Iter:633] Loss: 0.545 | Acc: 72.000% \n",
      "[Epoch:7, Iter:634] Loss: 0.545 | Acc: 71.824% \n",
      "[Epoch:7, Iter:635] Loss: 0.545 | Acc: 71.829% \n",
      "[Epoch:7, Iter:636] Loss: 0.544 | Acc: 71.889% \n",
      "[Epoch:7, Iter:637] Loss: 0.545 | Acc: 71.892% \n",
      "[Epoch:7, Iter:638] Loss: 0.543 | Acc: 72.158% \n",
      "[Epoch:7, Iter:639] Loss: 0.542 | Acc: 72.205% \n",
      "[Epoch:7, Iter:640] Loss: 0.537 | Acc: 72.550% \n",
      "[Epoch:7, Iter:641] Loss: 0.539 | Acc: 72.390% \n",
      "[Epoch:7, Iter:642] Loss: 0.538 | Acc: 72.333% \n",
      "[Epoch:7, Iter:643] Loss: 0.539 | Acc: 72.233% \n",
      "[Epoch:7, Iter:644] Loss: 0.537 | Acc: 72.364% \n",
      "[Epoch:7, Iter:645] Loss: 0.539 | Acc: 72.400% \n",
      "[Epoch:7, Iter:646] Loss: 0.536 | Acc: 72.652% \n",
      "[Epoch:7, Iter:647] Loss: 0.535 | Acc: 72.766% \n",
      "[Epoch:7, Iter:648] Loss: 0.537 | Acc: 72.625% \n",
      "[Epoch:7, Iter:649] Loss: 0.536 | Acc: 72.694% \n",
      "[Epoch:7, Iter:650] Loss: 0.535 | Acc: 72.760% \n",
      "[Epoch:7, Iter:651] Loss: 0.532 | Acc: 73.020% \n",
      "[Epoch:7, Iter:652] Loss: 0.531 | Acc: 73.077% \n",
      "[Epoch:7, Iter:653] Loss: 0.531 | Acc: 73.170% \n",
      "[Epoch:7, Iter:654] Loss: 0.530 | Acc: 73.148% \n",
      "[Epoch:7, Iter:655] Loss: 0.531 | Acc: 73.055% \n",
      "[Epoch:7, Iter:656] Loss: 0.532 | Acc: 72.929% \n",
      "[Epoch:7, Iter:657] Loss: 0.531 | Acc: 73.088% \n",
      "[Epoch:7, Iter:658] Loss: 0.532 | Acc: 73.172% \n",
      "[Epoch:7, Iter:659] Loss: 0.533 | Acc: 73.085% \n",
      "[Epoch:7, Iter:660] Loss: 0.534 | Acc: 73.100% \n",
      "[Epoch:7, Iter:661] Loss: 0.534 | Acc: 72.984% \n",
      "[Epoch:7, Iter:662] Loss: 0.534 | Acc: 73.032% \n",
      "[Epoch:7, Iter:663] Loss: 0.535 | Acc: 73.079% \n",
      "[Epoch:7, Iter:664] Loss: 0.535 | Acc: 73.000% \n",
      "[Epoch:7, Iter:665] Loss: 0.535 | Acc: 73.015% \n",
      "[Epoch:7, Iter:666] Loss: 0.536 | Acc: 72.909% \n",
      "[Epoch:7, Iter:667] Loss: 0.536 | Acc: 72.925% \n",
      "[Epoch:7, Iter:668] Loss: 0.536 | Acc: 72.941% \n",
      "[Epoch:7, Iter:669] Loss: 0.539 | Acc: 72.754% \n",
      "[Epoch:7, Iter:670] Loss: 0.540 | Acc: 72.543% \n",
      "[Epoch:7, Iter:671] Loss: 0.540 | Acc: 72.423% \n",
      "[Epoch:7, Iter:672] Loss: 0.541 | Acc: 72.333% \n",
      "[Epoch:7, Iter:673] Loss: 0.542 | Acc: 72.301% \n",
      "[Epoch:7, Iter:674] Loss: 0.543 | Acc: 72.270% \n",
      "[Epoch:7, Iter:675] Loss: 0.543 | Acc: 72.373% \n",
      "[Epoch:7, Iter:676] Loss: 0.542 | Acc: 72.447% \n",
      "[Epoch:7, Iter:677] Loss: 0.543 | Acc: 72.468% \n",
      "[Epoch:7, Iter:678] Loss: 0.543 | Acc: 72.513% \n",
      "[Epoch:7, Iter:679] Loss: 0.542 | Acc: 72.557% \n",
      "[Epoch:7, Iter:680] Loss: 0.543 | Acc: 72.600% \n",
      "[Epoch:7, Iter:681] Loss: 0.542 | Acc: 72.716% \n",
      "[Epoch:7, Iter:682] Loss: 0.543 | Acc: 72.610% \n",
      "[Epoch:7, Iter:683] Loss: 0.543 | Acc: 72.651% \n",
      "[Epoch:7, Iter:684] Loss: 0.542 | Acc: 72.762% \n",
      "[Epoch:7, Iter:685] Loss: 0.542 | Acc: 72.776% \n",
      "[Epoch:7, Iter:686] Loss: 0.542 | Acc: 72.674% \n",
      "[Epoch:7, Iter:687] Loss: 0.542 | Acc: 72.759% \n",
      "[Epoch:7, Iter:688] Loss: 0.542 | Acc: 72.636% \n",
      "[Epoch:7, Iter:689] Loss: 0.542 | Acc: 72.697% \n",
      "[Epoch:7, Iter:690] Loss: 0.542 | Acc: 72.711% \n",
      "[Epoch:7, Iter:691] Loss: 0.542 | Acc: 72.725% \n",
      "[Epoch:7, Iter:692] Loss: 0.543 | Acc: 72.652% \n",
      "[Epoch:7, Iter:693] Loss: 0.543 | Acc: 72.753% \n",
      "[Epoch:7, Iter:694] Loss: 0.545 | Acc: 72.596% \n",
      "[Epoch:7, Iter:695] Loss: 0.544 | Acc: 72.674% \n",
      "[Epoch:7, Iter:696] Loss: 0.544 | Acc: 72.667% \n",
      "[Epoch:7, Iter:697] Loss: 0.544 | Acc: 72.660% \n",
      "[Epoch:7, Iter:698] Loss: 0.544 | Acc: 72.694% \n",
      "[Epoch:7, Iter:699] Loss: 0.545 | Acc: 72.687% \n",
      "[Epoch:7, Iter:700] Loss: 0.544 | Acc: 72.740% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.900%\n",
      "Training set's accuracy (after quantization) is: 72.320%\n",
      "Test set's accuracy (before quantization) is: 73.200%\n",
      "Test set's accuracy (after quantization) is: 71.700%\n",
      "Train Loss: 0.541 | Train Acc: 72.900% | Test Loss: 0.544 | Test Acc: 73.200% \n",
      "Quantized Train Loss: 0.548 | Quantized Train Acc: 72.320% | Quantized Test Loss: 0.550 | Quantized Test Acc: 71.700% \n",
      "\n",
      "Epoch: 8\n",
      "[Epoch:8, Iter:701] Loss: 0.658 | Acc: 62.000% \n",
      "[Epoch:8, Iter:702] Loss: 0.599 | Acc: 67.000% \n",
      "[Epoch:8, Iter:703] Loss: 0.609 | Acc: 66.667% \n",
      "[Epoch:8, Iter:704] Loss: 0.590 | Acc: 69.500% \n",
      "[Epoch:8, Iter:705] Loss: 0.581 | Acc: 70.800% \n",
      "[Epoch:8, Iter:706] Loss: 0.580 | Acc: 70.667% \n",
      "[Epoch:8, Iter:707] Loss: 0.576 | Acc: 70.857% \n",
      "[Epoch:8, Iter:708] Loss: 0.567 | Acc: 71.500% \n",
      "[Epoch:8, Iter:709] Loss: 0.564 | Acc: 72.222% \n",
      "[Epoch:8, Iter:710] Loss: 0.564 | Acc: 72.200% \n",
      "[Epoch:8, Iter:711] Loss: 0.561 | Acc: 72.545% \n",
      "[Epoch:8, Iter:712] Loss: 0.570 | Acc: 71.833% \n",
      "[Epoch:8, Iter:713] Loss: 0.559 | Acc: 72.462% \n",
      "[Epoch:8, Iter:714] Loss: 0.547 | Acc: 73.143% \n",
      "[Epoch:8, Iter:715] Loss: 0.544 | Acc: 73.200% \n",
      "[Epoch:8, Iter:716] Loss: 0.542 | Acc: 73.250% \n",
      "[Epoch:8, Iter:717] Loss: 0.543 | Acc: 73.176% \n",
      "[Epoch:8, Iter:718] Loss: 0.546 | Acc: 72.778% \n",
      "[Epoch:8, Iter:719] Loss: 0.544 | Acc: 72.737% \n",
      "[Epoch:8, Iter:720] Loss: 0.547 | Acc: 72.800% \n",
      "[Epoch:8, Iter:721] Loss: 0.547 | Acc: 72.857% \n",
      "[Epoch:8, Iter:722] Loss: 0.548 | Acc: 73.000% \n",
      "[Epoch:8, Iter:723] Loss: 0.547 | Acc: 73.130% \n",
      "[Epoch:8, Iter:724] Loss: 0.546 | Acc: 73.333% \n",
      "[Epoch:8, Iter:725] Loss: 0.544 | Acc: 73.280% \n",
      "[Epoch:8, Iter:726] Loss: 0.544 | Acc: 73.077% \n",
      "[Epoch:8, Iter:727] Loss: 0.542 | Acc: 73.259% \n",
      "[Epoch:8, Iter:728] Loss: 0.542 | Acc: 73.071% \n",
      "[Epoch:8, Iter:729] Loss: 0.542 | Acc: 72.828% \n",
      "[Epoch:8, Iter:730] Loss: 0.543 | Acc: 72.867% \n",
      "[Epoch:8, Iter:731] Loss: 0.540 | Acc: 73.097% \n",
      "[Epoch:8, Iter:732] Loss: 0.540 | Acc: 72.938% \n",
      "[Epoch:8, Iter:733] Loss: 0.538 | Acc: 72.970% \n",
      "[Epoch:8, Iter:734] Loss: 0.540 | Acc: 72.824% \n",
      "[Epoch:8, Iter:735] Loss: 0.538 | Acc: 72.971% \n",
      "[Epoch:8, Iter:736] Loss: 0.537 | Acc: 73.056% \n",
      "[Epoch:8, Iter:737] Loss: 0.538 | Acc: 72.919% \n",
      "[Epoch:8, Iter:738] Loss: 0.538 | Acc: 73.000% \n",
      "[Epoch:8, Iter:739] Loss: 0.539 | Acc: 72.718% \n",
      "[Epoch:8, Iter:740] Loss: 0.538 | Acc: 72.700% \n",
      "[Epoch:8, Iter:741] Loss: 0.538 | Acc: 72.780% \n",
      "[Epoch:8, Iter:742] Loss: 0.539 | Acc: 72.762% \n",
      "[Epoch:8, Iter:743] Loss: 0.537 | Acc: 72.977% \n",
      "[Epoch:8, Iter:744] Loss: 0.536 | Acc: 73.136% \n",
      "[Epoch:8, Iter:745] Loss: 0.538 | Acc: 73.022% \n",
      "[Epoch:8, Iter:746] Loss: 0.538 | Acc: 73.130% \n",
      "[Epoch:8, Iter:747] Loss: 0.537 | Acc: 73.234% \n",
      "[Epoch:8, Iter:748] Loss: 0.537 | Acc: 73.125% \n",
      "[Epoch:8, Iter:749] Loss: 0.536 | Acc: 73.224% \n",
      "[Epoch:8, Iter:750] Loss: 0.536 | Acc: 73.120% \n",
      "[Epoch:8, Iter:751] Loss: 0.538 | Acc: 73.059% \n",
      "[Epoch:8, Iter:752] Loss: 0.536 | Acc: 73.192% \n",
      "[Epoch:8, Iter:753] Loss: 0.534 | Acc: 73.321% \n",
      "[Epoch:8, Iter:754] Loss: 0.536 | Acc: 73.222% \n",
      "[Epoch:8, Iter:755] Loss: 0.537 | Acc: 73.127% \n",
      "[Epoch:8, Iter:756] Loss: 0.540 | Acc: 72.929% \n",
      "[Epoch:8, Iter:757] Loss: 0.538 | Acc: 73.053% \n",
      "[Epoch:8, Iter:758] Loss: 0.541 | Acc: 72.862% \n",
      "[Epoch:8, Iter:759] Loss: 0.542 | Acc: 72.746% \n",
      "[Epoch:8, Iter:760] Loss: 0.541 | Acc: 72.767% \n",
      "[Epoch:8, Iter:761] Loss: 0.542 | Acc: 72.754% \n",
      "[Epoch:8, Iter:762] Loss: 0.541 | Acc: 72.871% \n",
      "[Epoch:8, Iter:763] Loss: 0.541 | Acc: 72.857% \n",
      "[Epoch:8, Iter:764] Loss: 0.540 | Acc: 72.938% \n",
      "[Epoch:8, Iter:765] Loss: 0.542 | Acc: 72.831% \n",
      "[Epoch:8, Iter:766] Loss: 0.542 | Acc: 72.788% \n",
      "[Epoch:8, Iter:767] Loss: 0.542 | Acc: 72.716% \n",
      "[Epoch:8, Iter:768] Loss: 0.543 | Acc: 72.647% \n",
      "[Epoch:8, Iter:769] Loss: 0.541 | Acc: 72.783% \n",
      "[Epoch:8, Iter:770] Loss: 0.541 | Acc: 72.857% \n",
      "[Epoch:8, Iter:771] Loss: 0.543 | Acc: 72.761% \n",
      "[Epoch:8, Iter:772] Loss: 0.545 | Acc: 72.583% \n",
      "[Epoch:8, Iter:773] Loss: 0.545 | Acc: 72.548% \n",
      "[Epoch:8, Iter:774] Loss: 0.545 | Acc: 72.514% \n",
      "[Epoch:8, Iter:775] Loss: 0.547 | Acc: 72.427% \n",
      "[Epoch:8, Iter:776] Loss: 0.546 | Acc: 72.474% \n",
      "[Epoch:8, Iter:777] Loss: 0.546 | Acc: 72.442% \n",
      "[Epoch:8, Iter:778] Loss: 0.545 | Acc: 72.487% \n",
      "[Epoch:8, Iter:779] Loss: 0.545 | Acc: 72.380% \n",
      "[Epoch:8, Iter:780] Loss: 0.545 | Acc: 72.450% \n",
      "[Epoch:8, Iter:781] Loss: 0.545 | Acc: 72.444% \n",
      "[Epoch:8, Iter:782] Loss: 0.544 | Acc: 72.512% \n",
      "[Epoch:8, Iter:783] Loss: 0.544 | Acc: 72.482% \n",
      "[Epoch:8, Iter:784] Loss: 0.543 | Acc: 72.619% \n",
      "[Epoch:8, Iter:785] Loss: 0.544 | Acc: 72.518% \n",
      "[Epoch:8, Iter:786] Loss: 0.544 | Acc: 72.395% \n",
      "[Epoch:8, Iter:787] Loss: 0.545 | Acc: 72.437% \n",
      "[Epoch:8, Iter:788] Loss: 0.544 | Acc: 72.455% \n",
      "[Epoch:8, Iter:789] Loss: 0.545 | Acc: 72.404% \n",
      "[Epoch:8, Iter:790] Loss: 0.544 | Acc: 72.511% \n",
      "[Epoch:8, Iter:791] Loss: 0.544 | Acc: 72.549% \n",
      "[Epoch:8, Iter:792] Loss: 0.546 | Acc: 72.348% \n",
      "[Epoch:8, Iter:793] Loss: 0.544 | Acc: 72.516% \n",
      "[Epoch:8, Iter:794] Loss: 0.546 | Acc: 72.340% \n",
      "[Epoch:8, Iter:795] Loss: 0.545 | Acc: 72.379% \n",
      "[Epoch:8, Iter:796] Loss: 0.544 | Acc: 72.396% \n",
      "[Epoch:8, Iter:797] Loss: 0.545 | Acc: 72.351% \n",
      "[Epoch:8, Iter:798] Loss: 0.545 | Acc: 72.306% \n",
      "[Epoch:8, Iter:799] Loss: 0.546 | Acc: 72.242% \n",
      "[Epoch:8, Iter:800] Loss: 0.546 | Acc: 72.240% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.440%\n",
      "Training set's accuracy (after quantization) is: 71.920%\n",
      "Test set's accuracy (before quantization) is: 71.300%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.546 | Train Acc: 72.440% | Test Loss: 0.550 | Test Acc: 71.300% \n",
      "Quantized Train Loss: 0.556 | Quantized Train Acc: 71.920% | Quantized Test Loss: 0.558 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 9\n",
      "[Epoch:9, Iter:801] Loss: 0.578 | Acc: 70.000% \n",
      "[Epoch:9, Iter:802] Loss: 0.526 | Acc: 75.000% \n",
      "[Epoch:9, Iter:803] Loss: 0.557 | Acc: 72.667% \n",
      "[Epoch:9, Iter:804] Loss: 0.543 | Acc: 73.500% \n",
      "[Epoch:9, Iter:805] Loss: 0.558 | Acc: 73.600% \n",
      "[Epoch:9, Iter:806] Loss: 0.563 | Acc: 72.333% \n",
      "[Epoch:9, Iter:807] Loss: 0.562 | Acc: 72.000% \n",
      "[Epoch:9, Iter:808] Loss: 0.560 | Acc: 71.750% \n",
      "[Epoch:9, Iter:809] Loss: 0.561 | Acc: 72.667% \n",
      "[Epoch:9, Iter:810] Loss: 0.549 | Acc: 73.800% \n",
      "[Epoch:9, Iter:811] Loss: 0.540 | Acc: 74.000% \n",
      "[Epoch:9, Iter:812] Loss: 0.543 | Acc: 73.333% \n",
      "[Epoch:9, Iter:813] Loss: 0.545 | Acc: 72.923% \n",
      "[Epoch:9, Iter:814] Loss: 0.535 | Acc: 73.857% \n",
      "[Epoch:9, Iter:815] Loss: 0.542 | Acc: 73.733% \n",
      "[Epoch:9, Iter:816] Loss: 0.538 | Acc: 74.125% \n",
      "[Epoch:9, Iter:817] Loss: 0.541 | Acc: 73.647% \n",
      "[Epoch:9, Iter:818] Loss: 0.547 | Acc: 73.111% \n",
      "[Epoch:9, Iter:819] Loss: 0.547 | Acc: 73.263% \n",
      "[Epoch:9, Iter:820] Loss: 0.542 | Acc: 73.600% \n",
      "[Epoch:9, Iter:821] Loss: 0.540 | Acc: 73.714% \n",
      "[Epoch:9, Iter:822] Loss: 0.546 | Acc: 72.818% \n",
      "[Epoch:9, Iter:823] Loss: 0.544 | Acc: 72.783% \n",
      "[Epoch:9, Iter:824] Loss: 0.547 | Acc: 72.833% \n",
      "[Epoch:9, Iter:825] Loss: 0.549 | Acc: 72.880% \n",
      "[Epoch:9, Iter:826] Loss: 0.552 | Acc: 72.692% \n",
      "[Epoch:9, Iter:827] Loss: 0.553 | Acc: 72.519% \n",
      "[Epoch:9, Iter:828] Loss: 0.551 | Acc: 72.786% \n",
      "[Epoch:9, Iter:829] Loss: 0.549 | Acc: 73.103% \n",
      "[Epoch:9, Iter:830] Loss: 0.548 | Acc: 73.200% \n",
      "[Epoch:9, Iter:831] Loss: 0.546 | Acc: 73.355% \n",
      "[Epoch:9, Iter:832] Loss: 0.546 | Acc: 73.438% \n",
      "[Epoch:9, Iter:833] Loss: 0.546 | Acc: 73.455% \n",
      "[Epoch:9, Iter:834] Loss: 0.543 | Acc: 73.765% \n",
      "[Epoch:9, Iter:835] Loss: 0.541 | Acc: 74.057% \n",
      "[Epoch:9, Iter:836] Loss: 0.537 | Acc: 74.167% \n",
      "[Epoch:9, Iter:837] Loss: 0.532 | Acc: 74.541% \n",
      "[Epoch:9, Iter:838] Loss: 0.539 | Acc: 74.158% \n",
      "[Epoch:9, Iter:839] Loss: 0.538 | Acc: 74.103% \n",
      "[Epoch:9, Iter:840] Loss: 0.539 | Acc: 73.800% \n",
      "[Epoch:9, Iter:841] Loss: 0.538 | Acc: 74.000% \n",
      "[Epoch:9, Iter:842] Loss: 0.540 | Acc: 73.905% \n",
      "[Epoch:9, Iter:843] Loss: 0.539 | Acc: 73.907% \n",
      "[Epoch:9, Iter:844] Loss: 0.538 | Acc: 73.864% \n",
      "[Epoch:9, Iter:845] Loss: 0.540 | Acc: 73.644% \n",
      "[Epoch:9, Iter:846] Loss: 0.538 | Acc: 73.826% \n",
      "[Epoch:9, Iter:847] Loss: 0.539 | Acc: 73.702% \n",
      "[Epoch:9, Iter:848] Loss: 0.540 | Acc: 73.583% \n",
      "[Epoch:9, Iter:849] Loss: 0.541 | Acc: 73.469% \n",
      "[Epoch:9, Iter:850] Loss: 0.540 | Acc: 73.480% \n",
      "[Epoch:9, Iter:851] Loss: 0.541 | Acc: 73.451% \n",
      "[Epoch:9, Iter:852] Loss: 0.540 | Acc: 73.500% \n",
      "[Epoch:9, Iter:853] Loss: 0.539 | Acc: 73.585% \n",
      "[Epoch:9, Iter:854] Loss: 0.540 | Acc: 73.519% \n",
      "[Epoch:9, Iter:855] Loss: 0.542 | Acc: 73.418% \n",
      "[Epoch:9, Iter:856] Loss: 0.544 | Acc: 73.250% \n",
      "[Epoch:9, Iter:857] Loss: 0.546 | Acc: 73.158% \n",
      "[Epoch:9, Iter:858] Loss: 0.546 | Acc: 73.069% \n",
      "[Epoch:9, Iter:859] Loss: 0.545 | Acc: 73.186% \n",
      "[Epoch:9, Iter:860] Loss: 0.543 | Acc: 73.233% \n",
      "[Epoch:9, Iter:861] Loss: 0.543 | Acc: 73.180% \n",
      "[Epoch:9, Iter:862] Loss: 0.544 | Acc: 73.097% \n",
      "[Epoch:9, Iter:863] Loss: 0.545 | Acc: 72.952% \n",
      "[Epoch:9, Iter:864] Loss: 0.545 | Acc: 72.969% \n",
      "[Epoch:9, Iter:865] Loss: 0.547 | Acc: 72.892% \n",
      "[Epoch:9, Iter:866] Loss: 0.547 | Acc: 72.879% \n",
      "[Epoch:9, Iter:867] Loss: 0.547 | Acc: 72.836% \n",
      "[Epoch:9, Iter:868] Loss: 0.547 | Acc: 72.941% \n",
      "[Epoch:9, Iter:869] Loss: 0.546 | Acc: 72.841% \n",
      "[Epoch:9, Iter:870] Loss: 0.545 | Acc: 72.886% \n",
      "[Epoch:9, Iter:871] Loss: 0.546 | Acc: 72.789% \n",
      "[Epoch:9, Iter:872] Loss: 0.546 | Acc: 72.750% \n",
      "[Epoch:9, Iter:873] Loss: 0.545 | Acc: 72.904% \n",
      "[Epoch:9, Iter:874] Loss: 0.544 | Acc: 72.865% \n",
      "[Epoch:9, Iter:875] Loss: 0.543 | Acc: 72.880% \n",
      "[Epoch:9, Iter:876] Loss: 0.543 | Acc: 72.974% \n",
      "[Epoch:9, Iter:877] Loss: 0.543 | Acc: 72.961% \n",
      "[Epoch:9, Iter:878] Loss: 0.542 | Acc: 73.051% \n",
      "[Epoch:9, Iter:879] Loss: 0.542 | Acc: 73.063% \n",
      "[Epoch:9, Iter:880] Loss: 0.543 | Acc: 72.950% \n",
      "[Epoch:9, Iter:881] Loss: 0.543 | Acc: 72.988% \n",
      "[Epoch:9, Iter:882] Loss: 0.544 | Acc: 72.878% \n",
      "[Epoch:9, Iter:883] Loss: 0.544 | Acc: 72.916% \n",
      "[Epoch:9, Iter:884] Loss: 0.544 | Acc: 72.833% \n",
      "[Epoch:9, Iter:885] Loss: 0.545 | Acc: 72.800% \n",
      "[Epoch:9, Iter:886] Loss: 0.545 | Acc: 72.791% \n",
      "[Epoch:9, Iter:887] Loss: 0.544 | Acc: 72.897% \n",
      "[Epoch:9, Iter:888] Loss: 0.545 | Acc: 72.750% \n",
      "[Epoch:9, Iter:889] Loss: 0.546 | Acc: 72.697% \n",
      "[Epoch:9, Iter:890] Loss: 0.546 | Acc: 72.689% \n",
      "[Epoch:9, Iter:891] Loss: 0.546 | Acc: 72.681% \n",
      "[Epoch:9, Iter:892] Loss: 0.545 | Acc: 72.717% \n",
      "[Epoch:9, Iter:893] Loss: 0.547 | Acc: 72.624% \n",
      "[Epoch:9, Iter:894] Loss: 0.546 | Acc: 72.681% \n",
      "[Epoch:9, Iter:895] Loss: 0.547 | Acc: 72.653% \n",
      "[Epoch:9, Iter:896] Loss: 0.548 | Acc: 72.604% \n",
      "[Epoch:9, Iter:897] Loss: 0.548 | Acc: 72.619% \n",
      "[Epoch:9, Iter:898] Loss: 0.548 | Acc: 72.633% \n",
      "[Epoch:9, Iter:899] Loss: 0.548 | Acc: 72.646% \n",
      "[Epoch:9, Iter:900] Loss: 0.548 | Acc: 72.680% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.040%\n",
      "Training set's accuracy (after quantization) is: 72.180%\n",
      "Test set's accuracy (before quantization) is: 72.500%\n",
      "Test set's accuracy (after quantization) is: 71.200%\n",
      "Train Loss: 0.540 | Train Acc: 73.040% | Test Loss: 0.543 | Test Acc: 72.500% \n",
      "Quantized Train Loss: 0.548 | Quantized Train Acc: 72.180% | Quantized Test Loss: 0.551 | Quantized Test Acc: 71.200% \n",
      "\n",
      "Epoch: 10\n",
      "[Epoch:10, Iter:901] Loss: 0.510 | Acc: 72.000% \n",
      "[Epoch:10, Iter:902] Loss: 0.611 | Acc: 66.000% \n",
      "[Epoch:10, Iter:903] Loss: 0.611 | Acc: 65.333% \n",
      "[Epoch:10, Iter:904] Loss: 0.616 | Acc: 64.000% \n",
      "[Epoch:10, Iter:905] Loss: 0.603 | Acc: 65.600% \n",
      "[Epoch:10, Iter:906] Loss: 0.582 | Acc: 67.667% \n",
      "[Epoch:10, Iter:907] Loss: 0.580 | Acc: 67.143% \n",
      "[Epoch:10, Iter:908] Loss: 0.582 | Acc: 67.500% \n",
      "[Epoch:10, Iter:909] Loss: 0.583 | Acc: 68.222% \n",
      "[Epoch:10, Iter:910] Loss: 0.588 | Acc: 67.800% \n",
      "[Epoch:10, Iter:911] Loss: 0.587 | Acc: 68.364% \n",
      "[Epoch:10, Iter:912] Loss: 0.596 | Acc: 68.000% \n",
      "[Epoch:10, Iter:913] Loss: 0.594 | Acc: 68.308% \n",
      "[Epoch:10, Iter:914] Loss: 0.589 | Acc: 68.857% \n",
      "[Epoch:10, Iter:915] Loss: 0.584 | Acc: 69.067% \n",
      "[Epoch:10, Iter:916] Loss: 0.589 | Acc: 68.375% \n",
      "[Epoch:10, Iter:917] Loss: 0.588 | Acc: 68.353% \n",
      "[Epoch:10, Iter:918] Loss: 0.584 | Acc: 68.667% \n",
      "[Epoch:10, Iter:919] Loss: 0.581 | Acc: 68.632% \n",
      "[Epoch:10, Iter:920] Loss: 0.576 | Acc: 69.200% \n",
      "[Epoch:10, Iter:921] Loss: 0.573 | Acc: 69.714% \n",
      "[Epoch:10, Iter:922] Loss: 0.568 | Acc: 70.364% \n",
      "[Epoch:10, Iter:923] Loss: 0.562 | Acc: 70.870% \n",
      "[Epoch:10, Iter:924] Loss: 0.560 | Acc: 71.083% \n",
      "[Epoch:10, Iter:925] Loss: 0.560 | Acc: 71.040% \n",
      "[Epoch:10, Iter:926] Loss: 0.558 | Acc: 71.154% \n",
      "[Epoch:10, Iter:927] Loss: 0.562 | Acc: 71.185% \n",
      "[Epoch:10, Iter:928] Loss: 0.567 | Acc: 71.071% \n",
      "[Epoch:10, Iter:929] Loss: 0.563 | Acc: 71.379% \n",
      "[Epoch:10, Iter:930] Loss: 0.558 | Acc: 71.867% \n",
      "[Epoch:10, Iter:931] Loss: 0.554 | Acc: 72.000% \n",
      "[Epoch:10, Iter:932] Loss: 0.558 | Acc: 71.875% \n",
      "[Epoch:10, Iter:933] Loss: 0.555 | Acc: 72.000% \n",
      "[Epoch:10, Iter:934] Loss: 0.558 | Acc: 71.824% \n",
      "[Epoch:10, Iter:935] Loss: 0.558 | Acc: 71.771% \n",
      "[Epoch:10, Iter:936] Loss: 0.560 | Acc: 71.611% \n",
      "[Epoch:10, Iter:937] Loss: 0.561 | Acc: 71.405% \n",
      "[Epoch:10, Iter:938] Loss: 0.560 | Acc: 71.526% \n",
      "[Epoch:10, Iter:939] Loss: 0.560 | Acc: 71.385% \n",
      "[Epoch:10, Iter:940] Loss: 0.559 | Acc: 71.550% \n",
      "[Epoch:10, Iter:941] Loss: 0.558 | Acc: 71.659% \n",
      "[Epoch:10, Iter:942] Loss: 0.557 | Acc: 71.667% \n",
      "[Epoch:10, Iter:943] Loss: 0.553 | Acc: 71.953% \n",
      "[Epoch:10, Iter:944] Loss: 0.551 | Acc: 72.091% \n",
      "[Epoch:10, Iter:945] Loss: 0.552 | Acc: 71.956% \n",
      "[Epoch:10, Iter:946] Loss: 0.552 | Acc: 72.087% \n",
      "[Epoch:10, Iter:947] Loss: 0.553 | Acc: 72.043% \n",
      "[Epoch:10, Iter:948] Loss: 0.553 | Acc: 72.125% \n",
      "[Epoch:10, Iter:949] Loss: 0.553 | Acc: 72.163% \n",
      "[Epoch:10, Iter:950] Loss: 0.554 | Acc: 72.120% \n",
      "[Epoch:10, Iter:951] Loss: 0.553 | Acc: 72.275% \n",
      "[Epoch:10, Iter:952] Loss: 0.555 | Acc: 72.269% \n",
      "[Epoch:10, Iter:953] Loss: 0.553 | Acc: 72.453% \n",
      "[Epoch:10, Iter:954] Loss: 0.554 | Acc: 72.481% \n",
      "[Epoch:10, Iter:955] Loss: 0.551 | Acc: 72.691% \n",
      "[Epoch:10, Iter:956] Loss: 0.551 | Acc: 72.679% \n",
      "[Epoch:10, Iter:957] Loss: 0.550 | Acc: 72.772% \n",
      "[Epoch:10, Iter:958] Loss: 0.548 | Acc: 72.966% \n",
      "[Epoch:10, Iter:959] Loss: 0.551 | Acc: 72.881% \n",
      "[Epoch:10, Iter:960] Loss: 0.550 | Acc: 72.833% \n",
      "[Epoch:10, Iter:961] Loss: 0.551 | Acc: 72.787% \n",
      "[Epoch:10, Iter:962] Loss: 0.550 | Acc: 72.935% \n",
      "[Epoch:10, Iter:963] Loss: 0.549 | Acc: 72.921% \n",
      "[Epoch:10, Iter:964] Loss: 0.550 | Acc: 72.812% \n",
      "[Epoch:10, Iter:965] Loss: 0.551 | Acc: 72.800% \n",
      "[Epoch:10, Iter:966] Loss: 0.552 | Acc: 72.667% \n",
      "[Epoch:10, Iter:967] Loss: 0.551 | Acc: 72.746% \n",
      "[Epoch:10, Iter:968] Loss: 0.551 | Acc: 72.676% \n",
      "[Epoch:10, Iter:969] Loss: 0.550 | Acc: 72.580% \n",
      "[Epoch:10, Iter:970] Loss: 0.549 | Acc: 72.743% \n",
      "[Epoch:10, Iter:971] Loss: 0.546 | Acc: 72.789% \n",
      "[Epoch:10, Iter:972] Loss: 0.547 | Acc: 72.778% \n",
      "[Epoch:10, Iter:973] Loss: 0.545 | Acc: 72.904% \n",
      "[Epoch:10, Iter:974] Loss: 0.544 | Acc: 73.000% \n",
      "[Epoch:10, Iter:975] Loss: 0.543 | Acc: 73.013% \n",
      "[Epoch:10, Iter:976] Loss: 0.545 | Acc: 72.921% \n",
      "[Epoch:10, Iter:977] Loss: 0.544 | Acc: 72.909% \n",
      "[Epoch:10, Iter:978] Loss: 0.545 | Acc: 72.949% \n",
      "[Epoch:10, Iter:979] Loss: 0.543 | Acc: 73.038% \n",
      "[Epoch:10, Iter:980] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:10, Iter:981] Loss: 0.543 | Acc: 72.988% \n",
      "[Epoch:10, Iter:982] Loss: 0.542 | Acc: 73.073% \n",
      "[Epoch:10, Iter:983] Loss: 0.540 | Acc: 73.229% \n",
      "[Epoch:10, Iter:984] Loss: 0.539 | Acc: 73.214% \n",
      "[Epoch:10, Iter:985] Loss: 0.538 | Acc: 73.318% \n",
      "[Epoch:10, Iter:986] Loss: 0.538 | Acc: 73.279% \n",
      "[Epoch:10, Iter:987] Loss: 0.539 | Acc: 73.264% \n",
      "[Epoch:10, Iter:988] Loss: 0.539 | Acc: 73.273% \n",
      "[Epoch:10, Iter:989] Loss: 0.539 | Acc: 73.236% \n",
      "[Epoch:10, Iter:990] Loss: 0.541 | Acc: 73.111% \n",
      "[Epoch:10, Iter:991] Loss: 0.542 | Acc: 73.143% \n",
      "[Epoch:10, Iter:992] Loss: 0.543 | Acc: 73.022% \n",
      "[Epoch:10, Iter:993] Loss: 0.543 | Acc: 73.011% \n",
      "[Epoch:10, Iter:994] Loss: 0.544 | Acc: 72.957% \n",
      "[Epoch:10, Iter:995] Loss: 0.544 | Acc: 72.968% \n",
      "[Epoch:10, Iter:996] Loss: 0.545 | Acc: 72.896% \n",
      "[Epoch:10, Iter:997] Loss: 0.545 | Acc: 72.907% \n",
      "[Epoch:10, Iter:998] Loss: 0.546 | Acc: 72.918% \n",
      "[Epoch:10, Iter:999] Loss: 0.545 | Acc: 72.990% \n",
      "[Epoch:10, Iter:1000] Loss: 0.545 | Acc: 72.920% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.900%\n",
      "Training set's accuracy (after quantization) is: 72.020%\n",
      "Test set's accuracy (before quantization) is: 72.300%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.540 | Train Acc: 72.900% | Test Loss: 0.544 | Test Acc: 72.300% \n",
      "Quantized Train Loss: 0.549 | Quantized Train Acc: 72.020% | Quantized Test Loss: 0.552 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 11\n",
      "[Epoch:11, Iter:1001] Loss: 0.666 | Acc: 58.000% \n",
      "[Epoch:11, Iter:1002] Loss: 0.632 | Acc: 63.000% \n",
      "[Epoch:11, Iter:1003] Loss: 0.669 | Acc: 61.333% \n",
      "[Epoch:11, Iter:1004] Loss: 0.658 | Acc: 61.000% \n",
      "[Epoch:11, Iter:1005] Loss: 0.622 | Acc: 63.600% \n",
      "[Epoch:11, Iter:1006] Loss: 0.606 | Acc: 65.000% \n",
      "[Epoch:11, Iter:1007] Loss: 0.602 | Acc: 65.714% \n",
      "[Epoch:11, Iter:1008] Loss: 0.585 | Acc: 67.750% \n",
      "[Epoch:11, Iter:1009] Loss: 0.567 | Acc: 69.111% \n",
      "[Epoch:11, Iter:1010] Loss: 0.574 | Acc: 69.200% \n",
      "[Epoch:11, Iter:1011] Loss: 0.578 | Acc: 69.091% \n",
      "[Epoch:11, Iter:1012] Loss: 0.575 | Acc: 69.500% \n",
      "[Epoch:11, Iter:1013] Loss: 0.570 | Acc: 70.308% \n",
      "[Epoch:11, Iter:1014] Loss: 0.572 | Acc: 70.714% \n",
      "[Epoch:11, Iter:1015] Loss: 0.568 | Acc: 71.200% \n",
      "[Epoch:11, Iter:1016] Loss: 0.573 | Acc: 70.375% \n",
      "[Epoch:11, Iter:1017] Loss: 0.570 | Acc: 70.118% \n",
      "[Epoch:11, Iter:1018] Loss: 0.570 | Acc: 70.000% \n",
      "[Epoch:11, Iter:1019] Loss: 0.569 | Acc: 70.105% \n",
      "[Epoch:11, Iter:1020] Loss: 0.562 | Acc: 70.600% \n",
      "[Epoch:11, Iter:1021] Loss: 0.565 | Acc: 70.667% \n",
      "[Epoch:11, Iter:1022] Loss: 0.561 | Acc: 71.000% \n",
      "[Epoch:11, Iter:1023] Loss: 0.561 | Acc: 71.304% \n",
      "[Epoch:11, Iter:1024] Loss: 0.556 | Acc: 71.583% \n",
      "[Epoch:11, Iter:1025] Loss: 0.559 | Acc: 71.520% \n",
      "[Epoch:11, Iter:1026] Loss: 0.562 | Acc: 71.154% \n",
      "[Epoch:11, Iter:1027] Loss: 0.563 | Acc: 70.815% \n",
      "[Epoch:11, Iter:1028] Loss: 0.561 | Acc: 71.214% \n",
      "[Epoch:11, Iter:1029] Loss: 0.558 | Acc: 71.172% \n",
      "[Epoch:11, Iter:1030] Loss: 0.553 | Acc: 71.467% \n",
      "[Epoch:11, Iter:1031] Loss: 0.555 | Acc: 71.548% \n",
      "[Epoch:11, Iter:1032] Loss: 0.550 | Acc: 71.938% \n",
      "[Epoch:11, Iter:1033] Loss: 0.551 | Acc: 72.061% \n",
      "[Epoch:11, Iter:1034] Loss: 0.550 | Acc: 72.000% \n",
      "[Epoch:11, Iter:1035] Loss: 0.550 | Acc: 72.171% \n",
      "[Epoch:11, Iter:1036] Loss: 0.546 | Acc: 72.389% \n",
      "[Epoch:11, Iter:1037] Loss: 0.547 | Acc: 72.324% \n",
      "[Epoch:11, Iter:1038] Loss: 0.545 | Acc: 72.474% \n",
      "[Epoch:11, Iter:1039] Loss: 0.545 | Acc: 72.564% \n",
      "[Epoch:11, Iter:1040] Loss: 0.545 | Acc: 72.600% \n",
      "[Epoch:11, Iter:1041] Loss: 0.546 | Acc: 72.439% \n",
      "[Epoch:11, Iter:1042] Loss: 0.545 | Acc: 72.476% \n",
      "[Epoch:11, Iter:1043] Loss: 0.545 | Acc: 72.512% \n",
      "[Epoch:11, Iter:1044] Loss: 0.544 | Acc: 72.591% \n",
      "[Epoch:11, Iter:1045] Loss: 0.543 | Acc: 72.711% \n",
      "[Epoch:11, Iter:1046] Loss: 0.545 | Acc: 72.739% \n",
      "[Epoch:11, Iter:1047] Loss: 0.543 | Acc: 72.851% \n",
      "[Epoch:11, Iter:1048] Loss: 0.544 | Acc: 72.833% \n",
      "[Epoch:11, Iter:1049] Loss: 0.545 | Acc: 72.531% \n",
      "[Epoch:11, Iter:1050] Loss: 0.543 | Acc: 72.680% \n",
      "[Epoch:11, Iter:1051] Loss: 0.542 | Acc: 72.745% \n",
      "[Epoch:11, Iter:1052] Loss: 0.544 | Acc: 72.500% \n",
      "[Epoch:11, Iter:1053] Loss: 0.543 | Acc: 72.566% \n",
      "[Epoch:11, Iter:1054] Loss: 0.542 | Acc: 72.667% \n",
      "[Epoch:11, Iter:1055] Loss: 0.541 | Acc: 72.800% \n",
      "[Epoch:11, Iter:1056] Loss: 0.542 | Acc: 72.750% \n",
      "[Epoch:11, Iter:1057] Loss: 0.541 | Acc: 72.842% \n",
      "[Epoch:11, Iter:1058] Loss: 0.543 | Acc: 72.759% \n",
      "[Epoch:11, Iter:1059] Loss: 0.542 | Acc: 72.847% \n",
      "[Epoch:11, Iter:1060] Loss: 0.542 | Acc: 72.933% \n",
      "[Epoch:11, Iter:1061] Loss: 0.541 | Acc: 72.984% \n",
      "[Epoch:11, Iter:1062] Loss: 0.543 | Acc: 72.839% \n",
      "[Epoch:11, Iter:1063] Loss: 0.542 | Acc: 72.857% \n",
      "[Epoch:11, Iter:1064] Loss: 0.542 | Acc: 73.031% \n",
      "[Epoch:11, Iter:1065] Loss: 0.541 | Acc: 73.108% \n",
      "[Epoch:11, Iter:1066] Loss: 0.542 | Acc: 72.970% \n",
      "[Epoch:11, Iter:1067] Loss: 0.543 | Acc: 72.866% \n",
      "[Epoch:11, Iter:1068] Loss: 0.544 | Acc: 72.853% \n",
      "[Epoch:11, Iter:1069] Loss: 0.542 | Acc: 72.986% \n",
      "[Epoch:11, Iter:1070] Loss: 0.541 | Acc: 72.971% \n",
      "[Epoch:11, Iter:1071] Loss: 0.540 | Acc: 72.986% \n",
      "[Epoch:11, Iter:1072] Loss: 0.540 | Acc: 72.889% \n",
      "[Epoch:11, Iter:1073] Loss: 0.540 | Acc: 72.877% \n",
      "[Epoch:11, Iter:1074] Loss: 0.541 | Acc: 72.838% \n",
      "[Epoch:11, Iter:1075] Loss: 0.541 | Acc: 72.773% \n",
      "[Epoch:11, Iter:1076] Loss: 0.541 | Acc: 72.737% \n",
      "[Epoch:11, Iter:1077] Loss: 0.541 | Acc: 72.675% \n",
      "[Epoch:11, Iter:1078] Loss: 0.541 | Acc: 72.667% \n",
      "[Epoch:11, Iter:1079] Loss: 0.541 | Acc: 72.734% \n",
      "[Epoch:11, Iter:1080] Loss: 0.542 | Acc: 72.700% \n",
      "[Epoch:11, Iter:1081] Loss: 0.542 | Acc: 72.667% \n",
      "[Epoch:11, Iter:1082] Loss: 0.544 | Acc: 72.585% \n",
      "[Epoch:11, Iter:1083] Loss: 0.544 | Acc: 72.506% \n",
      "[Epoch:11, Iter:1084] Loss: 0.543 | Acc: 72.571% \n",
      "[Epoch:11, Iter:1085] Loss: 0.543 | Acc: 72.635% \n",
      "[Epoch:11, Iter:1086] Loss: 0.545 | Acc: 72.488% \n",
      "[Epoch:11, Iter:1087] Loss: 0.545 | Acc: 72.506% \n",
      "[Epoch:11, Iter:1088] Loss: 0.544 | Acc: 72.636% \n",
      "[Epoch:11, Iter:1089] Loss: 0.542 | Acc: 72.742% \n",
      "[Epoch:11, Iter:1090] Loss: 0.543 | Acc: 72.778% \n",
      "[Epoch:11, Iter:1091] Loss: 0.543 | Acc: 72.769% \n",
      "[Epoch:11, Iter:1092] Loss: 0.544 | Acc: 72.652% \n",
      "[Epoch:11, Iter:1093] Loss: 0.545 | Acc: 72.602% \n",
      "[Epoch:11, Iter:1094] Loss: 0.545 | Acc: 72.553% \n",
      "[Epoch:11, Iter:1095] Loss: 0.545 | Acc: 72.547% \n",
      "[Epoch:11, Iter:1096] Loss: 0.545 | Acc: 72.542% \n",
      "[Epoch:11, Iter:1097] Loss: 0.544 | Acc: 72.577% \n",
      "[Epoch:11, Iter:1098] Loss: 0.544 | Acc: 72.592% \n",
      "[Epoch:11, Iter:1099] Loss: 0.544 | Acc: 72.586% \n",
      "[Epoch:11, Iter:1100] Loss: 0.545 | Acc: 72.600% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.800%\n",
      "Training set's accuracy (after quantization) is: 71.840%\n",
      "Test set's accuracy (before quantization) is: 72.200%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.543 | Train Acc: 72.800% | Test Loss: 0.546 | Test Acc: 72.200% \n",
      "Quantized Train Loss: 0.554 | Quantized Train Acc: 71.840% | Quantized Test Loss: 0.557 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 12\n",
      "[Epoch:12, Iter:1101] Loss: 0.473 | Acc: 80.000% \n",
      "[Epoch:12, Iter:1102] Loss: 0.493 | Acc: 78.000% \n",
      "[Epoch:12, Iter:1103] Loss: 0.481 | Acc: 78.667% \n",
      "[Epoch:12, Iter:1104] Loss: 0.507 | Acc: 76.000% \n",
      "[Epoch:12, Iter:1105] Loss: 0.494 | Acc: 77.600% \n",
      "[Epoch:12, Iter:1106] Loss: 0.489 | Acc: 77.667% \n",
      "[Epoch:12, Iter:1107] Loss: 0.507 | Acc: 77.143% \n",
      "[Epoch:12, Iter:1108] Loss: 0.509 | Acc: 76.750% \n",
      "[Epoch:12, Iter:1109] Loss: 0.522 | Acc: 76.000% \n",
      "[Epoch:12, Iter:1110] Loss: 0.536 | Acc: 74.200% \n",
      "[Epoch:12, Iter:1111] Loss: 0.543 | Acc: 73.818% \n",
      "[Epoch:12, Iter:1112] Loss: 0.541 | Acc: 73.833% \n",
      "[Epoch:12, Iter:1113] Loss: 0.541 | Acc: 73.538% \n",
      "[Epoch:12, Iter:1114] Loss: 0.536 | Acc: 73.857% \n",
      "[Epoch:12, Iter:1115] Loss: 0.535 | Acc: 74.133% \n",
      "[Epoch:12, Iter:1116] Loss: 0.538 | Acc: 74.000% \n",
      "[Epoch:12, Iter:1117] Loss: 0.533 | Acc: 74.588% \n",
      "[Epoch:12, Iter:1118] Loss: 0.539 | Acc: 74.222% \n",
      "[Epoch:12, Iter:1119] Loss: 0.538 | Acc: 74.316% \n",
      "[Epoch:12, Iter:1120] Loss: 0.531 | Acc: 74.700% \n",
      "[Epoch:12, Iter:1121] Loss: 0.538 | Acc: 74.095% \n",
      "[Epoch:12, Iter:1122] Loss: 0.540 | Acc: 74.182% \n",
      "[Epoch:12, Iter:1123] Loss: 0.545 | Acc: 73.826% \n",
      "[Epoch:12, Iter:1124] Loss: 0.544 | Acc: 73.833% \n",
      "[Epoch:12, Iter:1125] Loss: 0.548 | Acc: 73.200% \n",
      "[Epoch:12, Iter:1126] Loss: 0.549 | Acc: 73.154% \n",
      "[Epoch:12, Iter:1127] Loss: 0.549 | Acc: 72.889% \n",
      "[Epoch:12, Iter:1128] Loss: 0.549 | Acc: 72.929% \n",
      "[Epoch:12, Iter:1129] Loss: 0.546 | Acc: 73.172% \n",
      "[Epoch:12, Iter:1130] Loss: 0.548 | Acc: 72.933% \n",
      "[Epoch:12, Iter:1131] Loss: 0.550 | Acc: 72.774% \n",
      "[Epoch:12, Iter:1132] Loss: 0.549 | Acc: 72.750% \n",
      "[Epoch:12, Iter:1133] Loss: 0.547 | Acc: 73.030% \n",
      "[Epoch:12, Iter:1134] Loss: 0.549 | Acc: 72.882% \n",
      "[Epoch:12, Iter:1135] Loss: 0.552 | Acc: 72.743% \n",
      "[Epoch:12, Iter:1136] Loss: 0.553 | Acc: 72.611% \n",
      "[Epoch:12, Iter:1137] Loss: 0.552 | Acc: 72.649% \n",
      "[Epoch:12, Iter:1138] Loss: 0.552 | Acc: 72.737% \n",
      "[Epoch:12, Iter:1139] Loss: 0.550 | Acc: 72.821% \n",
      "[Epoch:12, Iter:1140] Loss: 0.550 | Acc: 72.800% \n",
      "[Epoch:12, Iter:1141] Loss: 0.550 | Acc: 72.878% \n",
      "[Epoch:12, Iter:1142] Loss: 0.549 | Acc: 72.857% \n",
      "[Epoch:12, Iter:1143] Loss: 0.551 | Acc: 72.744% \n",
      "[Epoch:12, Iter:1144] Loss: 0.552 | Acc: 72.636% \n",
      "[Epoch:12, Iter:1145] Loss: 0.552 | Acc: 72.578% \n",
      "[Epoch:12, Iter:1146] Loss: 0.551 | Acc: 72.652% \n",
      "[Epoch:12, Iter:1147] Loss: 0.549 | Acc: 72.851% \n",
      "[Epoch:12, Iter:1148] Loss: 0.548 | Acc: 72.875% \n",
      "[Epoch:12, Iter:1149] Loss: 0.547 | Acc: 72.735% \n",
      "[Epoch:12, Iter:1150] Loss: 0.548 | Acc: 72.720% \n",
      "[Epoch:12, Iter:1151] Loss: 0.547 | Acc: 72.824% \n",
      "[Epoch:12, Iter:1152] Loss: 0.546 | Acc: 72.731% \n",
      "[Epoch:12, Iter:1153] Loss: 0.546 | Acc: 72.717% \n",
      "[Epoch:12, Iter:1154] Loss: 0.544 | Acc: 72.815% \n",
      "[Epoch:12, Iter:1155] Loss: 0.544 | Acc: 72.836% \n",
      "[Epoch:12, Iter:1156] Loss: 0.546 | Acc: 72.643% \n",
      "[Epoch:12, Iter:1157] Loss: 0.546 | Acc: 72.667% \n",
      "[Epoch:12, Iter:1158] Loss: 0.545 | Acc: 72.621% \n",
      "[Epoch:12, Iter:1159] Loss: 0.544 | Acc: 72.780% \n",
      "[Epoch:12, Iter:1160] Loss: 0.544 | Acc: 72.867% \n",
      "[Epoch:12, Iter:1161] Loss: 0.545 | Acc: 72.656% \n",
      "[Epoch:12, Iter:1162] Loss: 0.544 | Acc: 72.742% \n",
      "[Epoch:12, Iter:1163] Loss: 0.542 | Acc: 72.952% \n",
      "[Epoch:12, Iter:1164] Loss: 0.541 | Acc: 72.969% \n",
      "[Epoch:12, Iter:1165] Loss: 0.541 | Acc: 72.954% \n",
      "[Epoch:12, Iter:1166] Loss: 0.543 | Acc: 72.788% \n",
      "[Epoch:12, Iter:1167] Loss: 0.544 | Acc: 72.687% \n",
      "[Epoch:12, Iter:1168] Loss: 0.544 | Acc: 72.647% \n",
      "[Epoch:12, Iter:1169] Loss: 0.544 | Acc: 72.638% \n",
      "[Epoch:12, Iter:1170] Loss: 0.544 | Acc: 72.657% \n",
      "[Epoch:12, Iter:1171] Loss: 0.545 | Acc: 72.535% \n",
      "[Epoch:12, Iter:1172] Loss: 0.546 | Acc: 72.444% \n",
      "[Epoch:12, Iter:1173] Loss: 0.546 | Acc: 72.356% \n",
      "[Epoch:12, Iter:1174] Loss: 0.545 | Acc: 72.432% \n",
      "[Epoch:12, Iter:1175] Loss: 0.546 | Acc: 72.320% \n",
      "[Epoch:12, Iter:1176] Loss: 0.546 | Acc: 72.368% \n",
      "[Epoch:12, Iter:1177] Loss: 0.547 | Acc: 72.286% \n",
      "[Epoch:12, Iter:1178] Loss: 0.547 | Acc: 72.256% \n",
      "[Epoch:12, Iter:1179] Loss: 0.548 | Acc: 72.177% \n",
      "[Epoch:12, Iter:1180] Loss: 0.547 | Acc: 72.200% \n",
      "[Epoch:12, Iter:1181] Loss: 0.547 | Acc: 72.222% \n",
      "[Epoch:12, Iter:1182] Loss: 0.547 | Acc: 72.220% \n",
      "[Epoch:12, Iter:1183] Loss: 0.548 | Acc: 72.193% \n",
      "[Epoch:12, Iter:1184] Loss: 0.548 | Acc: 72.119% \n",
      "[Epoch:12, Iter:1185] Loss: 0.548 | Acc: 72.071% \n",
      "[Epoch:12, Iter:1186] Loss: 0.549 | Acc: 72.023% \n",
      "[Epoch:12, Iter:1187] Loss: 0.549 | Acc: 72.023% \n",
      "[Epoch:12, Iter:1188] Loss: 0.549 | Acc: 72.023% \n",
      "[Epoch:12, Iter:1189] Loss: 0.549 | Acc: 72.045% \n",
      "[Epoch:12, Iter:1190] Loss: 0.549 | Acc: 72.089% \n",
      "[Epoch:12, Iter:1191] Loss: 0.548 | Acc: 72.110% \n",
      "[Epoch:12, Iter:1192] Loss: 0.548 | Acc: 72.174% \n",
      "[Epoch:12, Iter:1193] Loss: 0.547 | Acc: 72.194% \n",
      "[Epoch:12, Iter:1194] Loss: 0.546 | Acc: 72.255% \n",
      "[Epoch:12, Iter:1195] Loss: 0.544 | Acc: 72.421% \n",
      "[Epoch:12, Iter:1196] Loss: 0.545 | Acc: 72.458% \n",
      "[Epoch:12, Iter:1197] Loss: 0.544 | Acc: 72.536% \n",
      "[Epoch:12, Iter:1198] Loss: 0.545 | Acc: 72.469% \n",
      "[Epoch:12, Iter:1199] Loss: 0.545 | Acc: 72.424% \n",
      "[Epoch:12, Iter:1200] Loss: 0.545 | Acc: 72.420% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.880%\n",
      "Training set's accuracy (after quantization) is: 71.840%\n",
      "Test set's accuracy (before quantization) is: 72.300%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.540 | Train Acc: 72.880% | Test Loss: 0.545 | Test Acc: 72.300% \n",
      "Quantized Train Loss: 0.552 | Quantized Train Acc: 71.840% | Quantized Test Loss: 0.554 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 13\n",
      "[Epoch:13, Iter:1201] Loss: 0.576 | Acc: 74.000% \n",
      "[Epoch:13, Iter:1202] Loss: 0.546 | Acc: 73.000% \n",
      "[Epoch:13, Iter:1203] Loss: 0.558 | Acc: 70.000% \n",
      "[Epoch:13, Iter:1204] Loss: 0.560 | Acc: 68.500% \n",
      "[Epoch:13, Iter:1205] Loss: 0.569 | Acc: 67.200% \n",
      "[Epoch:13, Iter:1206] Loss: 0.562 | Acc: 68.667% \n",
      "[Epoch:13, Iter:1207] Loss: 0.560 | Acc: 69.143% \n",
      "[Epoch:13, Iter:1208] Loss: 0.549 | Acc: 70.750% \n",
      "[Epoch:13, Iter:1209] Loss: 0.545 | Acc: 70.889% \n",
      "[Epoch:13, Iter:1210] Loss: 0.536 | Acc: 72.200% \n",
      "[Epoch:13, Iter:1211] Loss: 0.532 | Acc: 72.364% \n",
      "[Epoch:13, Iter:1212] Loss: 0.527 | Acc: 73.167% \n",
      "[Epoch:13, Iter:1213] Loss: 0.547 | Acc: 72.154% \n",
      "[Epoch:13, Iter:1214] Loss: 0.548 | Acc: 72.429% \n",
      "[Epoch:13, Iter:1215] Loss: 0.542 | Acc: 72.800% \n",
      "[Epoch:13, Iter:1216] Loss: 0.551 | Acc: 72.625% \n",
      "[Epoch:13, Iter:1217] Loss: 0.556 | Acc: 72.471% \n",
      "[Epoch:13, Iter:1218] Loss: 0.552 | Acc: 72.778% \n",
      "[Epoch:13, Iter:1219] Loss: 0.556 | Acc: 72.316% \n",
      "[Epoch:13, Iter:1220] Loss: 0.551 | Acc: 72.900% \n",
      "[Epoch:13, Iter:1221] Loss: 0.557 | Acc: 72.476% \n",
      "[Epoch:13, Iter:1222] Loss: 0.556 | Acc: 72.273% \n",
      "[Epoch:13, Iter:1223] Loss: 0.554 | Acc: 72.609% \n",
      "[Epoch:13, Iter:1224] Loss: 0.552 | Acc: 72.833% \n",
      "[Epoch:13, Iter:1225] Loss: 0.549 | Acc: 73.120% \n",
      "[Epoch:13, Iter:1226] Loss: 0.551 | Acc: 72.923% \n",
      "[Epoch:13, Iter:1227] Loss: 0.551 | Acc: 72.815% \n",
      "[Epoch:13, Iter:1228] Loss: 0.553 | Acc: 72.571% \n",
      "[Epoch:13, Iter:1229] Loss: 0.556 | Acc: 72.276% \n",
      "[Epoch:13, Iter:1230] Loss: 0.556 | Acc: 72.333% \n",
      "[Epoch:13, Iter:1231] Loss: 0.557 | Acc: 72.129% \n",
      "[Epoch:13, Iter:1232] Loss: 0.555 | Acc: 72.125% \n",
      "[Epoch:13, Iter:1233] Loss: 0.553 | Acc: 72.242% \n",
      "[Epoch:13, Iter:1234] Loss: 0.552 | Acc: 72.353% \n",
      "[Epoch:13, Iter:1235] Loss: 0.556 | Acc: 71.943% \n",
      "[Epoch:13, Iter:1236] Loss: 0.555 | Acc: 72.056% \n",
      "[Epoch:13, Iter:1237] Loss: 0.560 | Acc: 71.730% \n",
      "[Epoch:13, Iter:1238] Loss: 0.557 | Acc: 72.053% \n",
      "[Epoch:13, Iter:1239] Loss: 0.555 | Acc: 72.205% \n",
      "[Epoch:13, Iter:1240] Loss: 0.554 | Acc: 72.200% \n",
      "[Epoch:13, Iter:1241] Loss: 0.554 | Acc: 72.098% \n",
      "[Epoch:13, Iter:1242] Loss: 0.553 | Acc: 72.143% \n",
      "[Epoch:13, Iter:1243] Loss: 0.554 | Acc: 72.093% \n",
      "[Epoch:13, Iter:1244] Loss: 0.554 | Acc: 72.000% \n",
      "[Epoch:13, Iter:1245] Loss: 0.553 | Acc: 72.044% \n",
      "[Epoch:13, Iter:1246] Loss: 0.552 | Acc: 72.043% \n",
      "[Epoch:13, Iter:1247] Loss: 0.550 | Acc: 72.085% \n",
      "[Epoch:13, Iter:1248] Loss: 0.548 | Acc: 72.208% \n",
      "[Epoch:13, Iter:1249] Loss: 0.550 | Acc: 72.082% \n",
      "[Epoch:13, Iter:1250] Loss: 0.547 | Acc: 72.280% \n",
      "[Epoch:13, Iter:1251] Loss: 0.549 | Acc: 72.235% \n",
      "[Epoch:13, Iter:1252] Loss: 0.550 | Acc: 72.115% \n",
      "[Epoch:13, Iter:1253] Loss: 0.550 | Acc: 72.075% \n",
      "[Epoch:13, Iter:1254] Loss: 0.550 | Acc: 71.963% \n",
      "[Epoch:13, Iter:1255] Loss: 0.549 | Acc: 72.109% \n",
      "[Epoch:13, Iter:1256] Loss: 0.547 | Acc: 72.214% \n",
      "[Epoch:13, Iter:1257] Loss: 0.549 | Acc: 72.105% \n",
      "[Epoch:13, Iter:1258] Loss: 0.548 | Acc: 72.069% \n",
      "[Epoch:13, Iter:1259] Loss: 0.547 | Acc: 72.136% \n",
      "[Epoch:13, Iter:1260] Loss: 0.548 | Acc: 71.967% \n",
      "[Epoch:13, Iter:1261] Loss: 0.546 | Acc: 72.033% \n",
      "[Epoch:13, Iter:1262] Loss: 0.544 | Acc: 72.323% \n",
      "[Epoch:13, Iter:1263] Loss: 0.544 | Acc: 72.317% \n",
      "[Epoch:13, Iter:1264] Loss: 0.543 | Acc: 72.281% \n",
      "[Epoch:13, Iter:1265] Loss: 0.543 | Acc: 72.400% \n",
      "[Epoch:13, Iter:1266] Loss: 0.543 | Acc: 72.303% \n",
      "[Epoch:13, Iter:1267] Loss: 0.543 | Acc: 72.328% \n",
      "[Epoch:13, Iter:1268] Loss: 0.542 | Acc: 72.353% \n",
      "[Epoch:13, Iter:1269] Loss: 0.543 | Acc: 72.232% \n",
      "[Epoch:13, Iter:1270] Loss: 0.543 | Acc: 72.314% \n",
      "[Epoch:13, Iter:1271] Loss: 0.544 | Acc: 72.254% \n",
      "[Epoch:13, Iter:1272] Loss: 0.544 | Acc: 72.222% \n",
      "[Epoch:13, Iter:1273] Loss: 0.542 | Acc: 72.356% \n",
      "[Epoch:13, Iter:1274] Loss: 0.545 | Acc: 72.216% \n",
      "[Epoch:13, Iter:1275] Loss: 0.544 | Acc: 72.187% \n",
      "[Epoch:13, Iter:1276] Loss: 0.545 | Acc: 72.132% \n",
      "[Epoch:13, Iter:1277] Loss: 0.543 | Acc: 72.260% \n",
      "[Epoch:13, Iter:1278] Loss: 0.544 | Acc: 72.256% \n",
      "[Epoch:13, Iter:1279] Loss: 0.545 | Acc: 72.177% \n",
      "[Epoch:13, Iter:1280] Loss: 0.545 | Acc: 72.125% \n",
      "[Epoch:13, Iter:1281] Loss: 0.545 | Acc: 72.049% \n",
      "[Epoch:13, Iter:1282] Loss: 0.545 | Acc: 72.098% \n",
      "[Epoch:13, Iter:1283] Loss: 0.544 | Acc: 72.217% \n",
      "[Epoch:13, Iter:1284] Loss: 0.544 | Acc: 72.262% \n",
      "[Epoch:13, Iter:1285] Loss: 0.544 | Acc: 72.282% \n",
      "[Epoch:13, Iter:1286] Loss: 0.545 | Acc: 72.256% \n",
      "[Epoch:13, Iter:1287] Loss: 0.545 | Acc: 72.230% \n",
      "[Epoch:13, Iter:1288] Loss: 0.545 | Acc: 72.159% \n",
      "[Epoch:13, Iter:1289] Loss: 0.546 | Acc: 72.157% \n",
      "[Epoch:13, Iter:1290] Loss: 0.546 | Acc: 72.133% \n",
      "[Epoch:13, Iter:1291] Loss: 0.545 | Acc: 72.176% \n",
      "[Epoch:13, Iter:1292] Loss: 0.546 | Acc: 72.087% \n",
      "[Epoch:13, Iter:1293] Loss: 0.545 | Acc: 72.194% \n",
      "[Epoch:13, Iter:1294] Loss: 0.544 | Acc: 72.277% \n",
      "[Epoch:13, Iter:1295] Loss: 0.543 | Acc: 72.337% \n",
      "[Epoch:13, Iter:1296] Loss: 0.546 | Acc: 72.250% \n",
      "[Epoch:13, Iter:1297] Loss: 0.547 | Acc: 72.082% \n",
      "[Epoch:13, Iter:1298] Loss: 0.547 | Acc: 72.122% \n",
      "[Epoch:13, Iter:1299] Loss: 0.547 | Acc: 72.222% \n",
      "[Epoch:13, Iter:1300] Loss: 0.548 | Acc: 72.180% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.640%\n",
      "Training set's accuracy (after quantization) is: 71.780%\n",
      "Test set's accuracy (before quantization) is: 72.000%\n",
      "Test set's accuracy (after quantization) is: 71.200%\n",
      "Train Loss: 0.542 | Train Acc: 72.640% | Test Loss: 0.546 | Test Acc: 72.000% \n",
      "Quantized Train Loss: 0.552 | Quantized Train Acc: 71.780% | Quantized Test Loss: 0.555 | Quantized Test Acc: 71.200% \n",
      "\n",
      "Epoch: 14\n",
      "[Epoch:14, Iter:1301] Loss: 0.545 | Acc: 76.000% \n",
      "[Epoch:14, Iter:1302] Loss: 0.529 | Acc: 75.000% \n",
      "[Epoch:14, Iter:1303] Loss: 0.524 | Acc: 76.667% \n",
      "[Epoch:14, Iter:1304] Loss: 0.550 | Acc: 75.000% \n",
      "[Epoch:14, Iter:1305] Loss: 0.553 | Acc: 74.400% \n",
      "[Epoch:14, Iter:1306] Loss: 0.562 | Acc: 72.667% \n",
      "[Epoch:14, Iter:1307] Loss: 0.555 | Acc: 73.714% \n",
      "[Epoch:14, Iter:1308] Loss: 0.560 | Acc: 73.500% \n",
      "[Epoch:14, Iter:1309] Loss: 0.565 | Acc: 72.667% \n",
      "[Epoch:14, Iter:1310] Loss: 0.573 | Acc: 72.200% \n",
      "[Epoch:14, Iter:1311] Loss: 0.568 | Acc: 72.727% \n",
      "[Epoch:14, Iter:1312] Loss: 0.559 | Acc: 73.333% \n",
      "[Epoch:14, Iter:1313] Loss: 0.547 | Acc: 73.692% \n",
      "[Epoch:14, Iter:1314] Loss: 0.545 | Acc: 74.286% \n",
      "[Epoch:14, Iter:1315] Loss: 0.555 | Acc: 73.333% \n",
      "[Epoch:14, Iter:1316] Loss: 0.553 | Acc: 73.500% \n",
      "[Epoch:14, Iter:1317] Loss: 0.553 | Acc: 73.765% \n",
      "[Epoch:14, Iter:1318] Loss: 0.549 | Acc: 73.889% \n",
      "[Epoch:14, Iter:1319] Loss: 0.546 | Acc: 74.000% \n",
      "[Epoch:14, Iter:1320] Loss: 0.540 | Acc: 74.100% \n",
      "[Epoch:14, Iter:1321] Loss: 0.538 | Acc: 74.190% \n",
      "[Epoch:14, Iter:1322] Loss: 0.545 | Acc: 73.636% \n",
      "[Epoch:14, Iter:1323] Loss: 0.548 | Acc: 73.043% \n",
      "[Epoch:14, Iter:1324] Loss: 0.549 | Acc: 72.917% \n",
      "[Epoch:14, Iter:1325] Loss: 0.548 | Acc: 72.640% \n",
      "[Epoch:14, Iter:1326] Loss: 0.548 | Acc: 72.769% \n",
      "[Epoch:14, Iter:1327] Loss: 0.552 | Acc: 72.370% \n",
      "[Epoch:14, Iter:1328] Loss: 0.554 | Acc: 72.214% \n",
      "[Epoch:14, Iter:1329] Loss: 0.553 | Acc: 72.207% \n",
      "[Epoch:14, Iter:1330] Loss: 0.553 | Acc: 72.267% \n",
      "[Epoch:14, Iter:1331] Loss: 0.551 | Acc: 72.452% \n",
      "[Epoch:14, Iter:1332] Loss: 0.548 | Acc: 72.688% \n",
      "[Epoch:14, Iter:1333] Loss: 0.546 | Acc: 72.727% \n",
      "[Epoch:14, Iter:1334] Loss: 0.543 | Acc: 72.941% \n",
      "[Epoch:14, Iter:1335] Loss: 0.540 | Acc: 73.029% \n",
      "[Epoch:14, Iter:1336] Loss: 0.537 | Acc: 73.333% \n",
      "[Epoch:14, Iter:1337] Loss: 0.541 | Acc: 73.081% \n",
      "[Epoch:14, Iter:1338] Loss: 0.540 | Acc: 73.158% \n",
      "[Epoch:14, Iter:1339] Loss: 0.540 | Acc: 73.231% \n",
      "[Epoch:14, Iter:1340] Loss: 0.543 | Acc: 73.100% \n",
      "[Epoch:14, Iter:1341] Loss: 0.544 | Acc: 73.073% \n",
      "[Epoch:14, Iter:1342] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:14, Iter:1343] Loss: 0.542 | Acc: 72.977% \n",
      "[Epoch:14, Iter:1344] Loss: 0.541 | Acc: 73.091% \n",
      "[Epoch:14, Iter:1345] Loss: 0.541 | Acc: 73.244% \n",
      "[Epoch:14, Iter:1346] Loss: 0.540 | Acc: 73.217% \n",
      "[Epoch:14, Iter:1347] Loss: 0.541 | Acc: 73.021% \n",
      "[Epoch:14, Iter:1348] Loss: 0.540 | Acc: 73.125% \n",
      "[Epoch:14, Iter:1349] Loss: 0.539 | Acc: 73.143% \n",
      "[Epoch:14, Iter:1350] Loss: 0.541 | Acc: 73.160% \n",
      "[Epoch:14, Iter:1351] Loss: 0.541 | Acc: 73.137% \n",
      "[Epoch:14, Iter:1352] Loss: 0.542 | Acc: 73.000% \n",
      "[Epoch:14, Iter:1353] Loss: 0.542 | Acc: 72.943% \n",
      "[Epoch:14, Iter:1354] Loss: 0.544 | Acc: 72.889% \n",
      "[Epoch:14, Iter:1355] Loss: 0.543 | Acc: 72.836% \n",
      "[Epoch:14, Iter:1356] Loss: 0.543 | Acc: 72.929% \n",
      "[Epoch:14, Iter:1357] Loss: 0.545 | Acc: 72.667% \n",
      "[Epoch:14, Iter:1358] Loss: 0.543 | Acc: 72.828% \n",
      "[Epoch:14, Iter:1359] Loss: 0.544 | Acc: 72.712% \n",
      "[Epoch:14, Iter:1360] Loss: 0.546 | Acc: 72.533% \n",
      "[Epoch:14, Iter:1361] Loss: 0.546 | Acc: 72.426% \n",
      "[Epoch:14, Iter:1362] Loss: 0.546 | Acc: 72.484% \n",
      "[Epoch:14, Iter:1363] Loss: 0.546 | Acc: 72.413% \n",
      "[Epoch:14, Iter:1364] Loss: 0.547 | Acc: 72.469% \n",
      "[Epoch:14, Iter:1365] Loss: 0.546 | Acc: 72.431% \n",
      "[Epoch:14, Iter:1366] Loss: 0.545 | Acc: 72.455% \n",
      "[Epoch:14, Iter:1367] Loss: 0.546 | Acc: 72.448% \n",
      "[Epoch:14, Iter:1368] Loss: 0.546 | Acc: 72.382% \n",
      "[Epoch:14, Iter:1369] Loss: 0.545 | Acc: 72.493% \n",
      "[Epoch:14, Iter:1370] Loss: 0.545 | Acc: 72.429% \n",
      "[Epoch:14, Iter:1371] Loss: 0.545 | Acc: 72.366% \n",
      "[Epoch:14, Iter:1372] Loss: 0.544 | Acc: 72.389% \n",
      "[Epoch:14, Iter:1373] Loss: 0.543 | Acc: 72.466% \n",
      "[Epoch:14, Iter:1374] Loss: 0.544 | Acc: 72.432% \n",
      "[Epoch:14, Iter:1375] Loss: 0.544 | Acc: 72.320% \n",
      "[Epoch:14, Iter:1376] Loss: 0.544 | Acc: 72.395% \n",
      "[Epoch:14, Iter:1377] Loss: 0.544 | Acc: 72.390% \n",
      "[Epoch:14, Iter:1378] Loss: 0.545 | Acc: 72.282% \n",
      "[Epoch:14, Iter:1379] Loss: 0.545 | Acc: 72.329% \n",
      "[Epoch:14, Iter:1380] Loss: 0.544 | Acc: 72.375% \n",
      "[Epoch:14, Iter:1381] Loss: 0.544 | Acc: 72.494% \n",
      "[Epoch:14, Iter:1382] Loss: 0.544 | Acc: 72.415% \n",
      "[Epoch:14, Iter:1383] Loss: 0.545 | Acc: 72.361% \n",
      "[Epoch:14, Iter:1384] Loss: 0.544 | Acc: 72.476% \n",
      "[Epoch:14, Iter:1385] Loss: 0.544 | Acc: 72.471% \n",
      "[Epoch:14, Iter:1386] Loss: 0.544 | Acc: 72.465% \n",
      "[Epoch:14, Iter:1387] Loss: 0.545 | Acc: 72.414% \n",
      "[Epoch:14, Iter:1388] Loss: 0.545 | Acc: 72.409% \n",
      "[Epoch:14, Iter:1389] Loss: 0.545 | Acc: 72.337% \n",
      "[Epoch:14, Iter:1390] Loss: 0.546 | Acc: 72.289% \n",
      "[Epoch:14, Iter:1391] Loss: 0.545 | Acc: 72.308% \n",
      "[Epoch:14, Iter:1392] Loss: 0.544 | Acc: 72.413% \n",
      "[Epoch:14, Iter:1393] Loss: 0.545 | Acc: 72.366% \n",
      "[Epoch:14, Iter:1394] Loss: 0.544 | Acc: 72.383% \n",
      "[Epoch:14, Iter:1395] Loss: 0.544 | Acc: 72.379% \n",
      "[Epoch:14, Iter:1396] Loss: 0.545 | Acc: 72.375% \n",
      "[Epoch:14, Iter:1397] Loss: 0.545 | Acc: 72.412% \n",
      "[Epoch:14, Iter:1398] Loss: 0.544 | Acc: 72.449% \n",
      "[Epoch:14, Iter:1399] Loss: 0.545 | Acc: 72.384% \n",
      "[Epoch:14, Iter:1400] Loss: 0.545 | Acc: 72.400% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.740%\n",
      "Training set's accuracy (after quantization) is: 71.800%\n",
      "Test set's accuracy (before quantization) is: 71.700%\n",
      "Test set's accuracy (after quantization) is: 71.200%\n",
      "Train Loss: 0.543 | Train Acc: 72.740% | Test Loss: 0.546 | Test Acc: 71.700% \n",
      "Quantized Train Loss: 0.553 | Quantized Train Acc: 71.800% | Quantized Test Loss: 0.555 | Quantized Test Acc: 71.200% \n",
      "\n",
      "Epoch: 15\n",
      "[Epoch:15, Iter:1401] Loss: 0.530 | Acc: 70.000% \n",
      "[Epoch:15, Iter:1402] Loss: 0.546 | Acc: 71.000% \n",
      "[Epoch:15, Iter:1403] Loss: 0.511 | Acc: 75.333% \n",
      "[Epoch:15, Iter:1404] Loss: 0.514 | Acc: 75.000% \n",
      "[Epoch:15, Iter:1405] Loss: 0.520 | Acc: 74.400% \n",
      "[Epoch:15, Iter:1406] Loss: 0.518 | Acc: 75.000% \n",
      "[Epoch:15, Iter:1407] Loss: 0.518 | Acc: 74.857% \n",
      "[Epoch:15, Iter:1408] Loss: 0.512 | Acc: 75.500% \n",
      "[Epoch:15, Iter:1409] Loss: 0.512 | Acc: 75.111% \n",
      "[Epoch:15, Iter:1410] Loss: 0.522 | Acc: 73.200% \n",
      "[Epoch:15, Iter:1411] Loss: 0.528 | Acc: 73.636% \n",
      "[Epoch:15, Iter:1412] Loss: 0.523 | Acc: 74.167% \n",
      "[Epoch:15, Iter:1413] Loss: 0.516 | Acc: 74.769% \n",
      "[Epoch:15, Iter:1414] Loss: 0.522 | Acc: 74.143% \n",
      "[Epoch:15, Iter:1415] Loss: 0.526 | Acc: 73.867% \n",
      "[Epoch:15, Iter:1416] Loss: 0.524 | Acc: 73.750% \n",
      "[Epoch:15, Iter:1417] Loss: 0.527 | Acc: 73.412% \n",
      "[Epoch:15, Iter:1418] Loss: 0.524 | Acc: 73.889% \n",
      "[Epoch:15, Iter:1419] Loss: 0.521 | Acc: 74.000% \n",
      "[Epoch:15, Iter:1420] Loss: 0.520 | Acc: 74.400% \n",
      "[Epoch:15, Iter:1421] Loss: 0.519 | Acc: 74.571% \n",
      "[Epoch:15, Iter:1422] Loss: 0.522 | Acc: 74.545% \n",
      "[Epoch:15, Iter:1423] Loss: 0.520 | Acc: 74.522% \n",
      "[Epoch:15, Iter:1424] Loss: 0.518 | Acc: 74.750% \n",
      "[Epoch:15, Iter:1425] Loss: 0.518 | Acc: 74.960% \n",
      "[Epoch:15, Iter:1426] Loss: 0.518 | Acc: 74.923% \n",
      "[Epoch:15, Iter:1427] Loss: 0.513 | Acc: 75.481% \n",
      "[Epoch:15, Iter:1428] Loss: 0.512 | Acc: 75.500% \n",
      "[Epoch:15, Iter:1429] Loss: 0.513 | Acc: 75.517% \n",
      "[Epoch:15, Iter:1430] Loss: 0.516 | Acc: 75.133% \n",
      "[Epoch:15, Iter:1431] Loss: 0.518 | Acc: 74.839% \n",
      "[Epoch:15, Iter:1432] Loss: 0.521 | Acc: 74.500% \n",
      "[Epoch:15, Iter:1433] Loss: 0.522 | Acc: 74.303% \n",
      "[Epoch:15, Iter:1434] Loss: 0.524 | Acc: 73.882% \n",
      "[Epoch:15, Iter:1435] Loss: 0.526 | Acc: 73.714% \n",
      "[Epoch:15, Iter:1436] Loss: 0.526 | Acc: 73.667% \n",
      "[Epoch:15, Iter:1437] Loss: 0.526 | Acc: 73.784% \n",
      "[Epoch:15, Iter:1438] Loss: 0.531 | Acc: 73.579% \n",
      "[Epoch:15, Iter:1439] Loss: 0.531 | Acc: 73.436% \n",
      "[Epoch:15, Iter:1440] Loss: 0.532 | Acc: 73.250% \n",
      "[Epoch:15, Iter:1441] Loss: 0.531 | Acc: 73.366% \n",
      "[Epoch:15, Iter:1442] Loss: 0.536 | Acc: 73.143% \n",
      "[Epoch:15, Iter:1443] Loss: 0.537 | Acc: 72.977% \n",
      "[Epoch:15, Iter:1444] Loss: 0.539 | Acc: 72.818% \n",
      "[Epoch:15, Iter:1445] Loss: 0.541 | Acc: 72.578% \n",
      "[Epoch:15, Iter:1446] Loss: 0.538 | Acc: 72.739% \n",
      "[Epoch:15, Iter:1447] Loss: 0.538 | Acc: 72.851% \n",
      "[Epoch:15, Iter:1448] Loss: 0.540 | Acc: 72.667% \n",
      "[Epoch:15, Iter:1449] Loss: 0.541 | Acc: 72.612% \n",
      "[Epoch:15, Iter:1450] Loss: 0.540 | Acc: 72.760% \n",
      "[Epoch:15, Iter:1451] Loss: 0.538 | Acc: 72.824% \n",
      "[Epoch:15, Iter:1452] Loss: 0.541 | Acc: 72.692% \n",
      "[Epoch:15, Iter:1453] Loss: 0.541 | Acc: 72.679% \n",
      "[Epoch:15, Iter:1454] Loss: 0.539 | Acc: 72.926% \n",
      "[Epoch:15, Iter:1455] Loss: 0.537 | Acc: 73.055% \n",
      "[Epoch:15, Iter:1456] Loss: 0.536 | Acc: 73.143% \n",
      "[Epoch:15, Iter:1457] Loss: 0.541 | Acc: 72.772% \n",
      "[Epoch:15, Iter:1458] Loss: 0.542 | Acc: 72.655% \n",
      "[Epoch:15, Iter:1459] Loss: 0.542 | Acc: 72.576% \n",
      "[Epoch:15, Iter:1460] Loss: 0.542 | Acc: 72.500% \n",
      "[Epoch:15, Iter:1461] Loss: 0.540 | Acc: 72.623% \n",
      "[Epoch:15, Iter:1462] Loss: 0.542 | Acc: 72.387% \n",
      "[Epoch:15, Iter:1463] Loss: 0.543 | Acc: 72.413% \n",
      "[Epoch:15, Iter:1464] Loss: 0.543 | Acc: 72.438% \n",
      "[Epoch:15, Iter:1465] Loss: 0.542 | Acc: 72.400% \n",
      "[Epoch:15, Iter:1466] Loss: 0.543 | Acc: 72.364% \n",
      "[Epoch:15, Iter:1467] Loss: 0.541 | Acc: 72.507% \n",
      "[Epoch:15, Iter:1468] Loss: 0.544 | Acc: 72.353% \n",
      "[Epoch:15, Iter:1469] Loss: 0.543 | Acc: 72.464% \n",
      "[Epoch:15, Iter:1470] Loss: 0.545 | Acc: 72.286% \n",
      "[Epoch:15, Iter:1471] Loss: 0.544 | Acc: 72.366% \n",
      "[Epoch:15, Iter:1472] Loss: 0.544 | Acc: 72.444% \n",
      "[Epoch:15, Iter:1473] Loss: 0.542 | Acc: 72.548% \n",
      "[Epoch:15, Iter:1474] Loss: 0.542 | Acc: 72.595% \n",
      "[Epoch:15, Iter:1475] Loss: 0.541 | Acc: 72.640% \n",
      "[Epoch:15, Iter:1476] Loss: 0.541 | Acc: 72.632% \n",
      "[Epoch:15, Iter:1477] Loss: 0.541 | Acc: 72.649% \n",
      "[Epoch:15, Iter:1478] Loss: 0.541 | Acc: 72.564% \n",
      "[Epoch:15, Iter:1479] Loss: 0.542 | Acc: 72.506% \n",
      "[Epoch:15, Iter:1480] Loss: 0.543 | Acc: 72.475% \n",
      "[Epoch:15, Iter:1481] Loss: 0.543 | Acc: 72.494% \n",
      "[Epoch:15, Iter:1482] Loss: 0.543 | Acc: 72.488% \n",
      "[Epoch:15, Iter:1483] Loss: 0.543 | Acc: 72.434% \n",
      "[Epoch:15, Iter:1484] Loss: 0.543 | Acc: 72.500% \n",
      "[Epoch:15, Iter:1485] Loss: 0.544 | Acc: 72.471% \n",
      "[Epoch:15, Iter:1486] Loss: 0.545 | Acc: 72.395% \n",
      "[Epoch:15, Iter:1487] Loss: 0.545 | Acc: 72.322% \n",
      "[Epoch:15, Iter:1488] Loss: 0.545 | Acc: 72.318% \n",
      "[Epoch:15, Iter:1489] Loss: 0.545 | Acc: 72.292% \n",
      "[Epoch:15, Iter:1490] Loss: 0.545 | Acc: 72.289% \n",
      "[Epoch:15, Iter:1491] Loss: 0.545 | Acc: 72.308% \n",
      "[Epoch:15, Iter:1492] Loss: 0.545 | Acc: 72.261% \n",
      "[Epoch:15, Iter:1493] Loss: 0.545 | Acc: 72.280% \n",
      "[Epoch:15, Iter:1494] Loss: 0.546 | Acc: 72.191% \n",
      "[Epoch:15, Iter:1495] Loss: 0.546 | Acc: 72.126% \n",
      "[Epoch:15, Iter:1496] Loss: 0.546 | Acc: 72.167% \n",
      "[Epoch:15, Iter:1497] Loss: 0.546 | Acc: 72.206% \n",
      "[Epoch:15, Iter:1498] Loss: 0.546 | Acc: 72.143% \n",
      "[Epoch:15, Iter:1499] Loss: 0.547 | Acc: 72.020% \n",
      "[Epoch:15, Iter:1500] Loss: 0.547 | Acc: 72.060% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.160%\n",
      "Training set's accuracy (after quantization) is: 72.720%\n",
      "Test set's accuracy (before quantization) is: 72.800%\n",
      "Test set's accuracy (after quantization) is: 72.300%\n",
      "Train Loss: 0.552 | Train Acc: 72.160% | Test Loss: 0.556 | Test Acc: 72.800% \n",
      "Quantized Train Loss: 0.544 | Quantized Train Acc: 72.720% | Quantized Test Loss: 0.546 | Quantized Test Acc: 72.300% \n",
      "\n",
      "Epoch: 16\n",
      "[Epoch:16, Iter:1501] Loss: 0.488 | Acc: 74.000% \n",
      "[Epoch:16, Iter:1502] Loss: 0.550 | Acc: 69.000% \n",
      "[Epoch:16, Iter:1503] Loss: 0.522 | Acc: 74.000% \n",
      "[Epoch:16, Iter:1504] Loss: 0.519 | Acc: 76.000% \n",
      "[Epoch:16, Iter:1505] Loss: 0.553 | Acc: 73.200% \n",
      "[Epoch:16, Iter:1506] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:16, Iter:1507] Loss: 0.528 | Acc: 74.000% \n",
      "[Epoch:16, Iter:1508] Loss: 0.531 | Acc: 73.250% \n",
      "[Epoch:16, Iter:1509] Loss: 0.533 | Acc: 73.333% \n",
      "[Epoch:16, Iter:1510] Loss: 0.541 | Acc: 73.200% \n",
      "[Epoch:16, Iter:1511] Loss: 0.552 | Acc: 72.545% \n",
      "[Epoch:16, Iter:1512] Loss: 0.555 | Acc: 72.333% \n",
      "[Epoch:16, Iter:1513] Loss: 0.551 | Acc: 72.154% \n",
      "[Epoch:16, Iter:1514] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:16, Iter:1515] Loss: 0.552 | Acc: 71.467% \n",
      "[Epoch:16, Iter:1516] Loss: 0.552 | Acc: 71.625% \n",
      "[Epoch:16, Iter:1517] Loss: 0.554 | Acc: 71.529% \n",
      "[Epoch:16, Iter:1518] Loss: 0.551 | Acc: 71.889% \n",
      "[Epoch:16, Iter:1519] Loss: 0.554 | Acc: 71.789% \n",
      "[Epoch:16, Iter:1520] Loss: 0.554 | Acc: 71.900% \n",
      "[Epoch:16, Iter:1521] Loss: 0.557 | Acc: 71.619% \n",
      "[Epoch:16, Iter:1522] Loss: 0.558 | Acc: 71.727% \n",
      "[Epoch:16, Iter:1523] Loss: 0.555 | Acc: 71.913% \n",
      "[Epoch:16, Iter:1524] Loss: 0.559 | Acc: 71.667% \n",
      "[Epoch:16, Iter:1525] Loss: 0.564 | Acc: 70.960% \n",
      "[Epoch:16, Iter:1526] Loss: 0.563 | Acc: 71.000% \n",
      "[Epoch:16, Iter:1527] Loss: 0.564 | Acc: 70.815% \n",
      "[Epoch:16, Iter:1528] Loss: 0.564 | Acc: 70.857% \n",
      "[Epoch:16, Iter:1529] Loss: 0.566 | Acc: 70.828% \n",
      "[Epoch:16, Iter:1530] Loss: 0.569 | Acc: 70.867% \n",
      "[Epoch:16, Iter:1531] Loss: 0.566 | Acc: 71.161% \n",
      "[Epoch:16, Iter:1532] Loss: 0.568 | Acc: 71.125% \n",
      "[Epoch:16, Iter:1533] Loss: 0.565 | Acc: 71.152% \n",
      "[Epoch:16, Iter:1534] Loss: 0.567 | Acc: 70.941% \n",
      "[Epoch:16, Iter:1535] Loss: 0.564 | Acc: 71.371% \n",
      "[Epoch:16, Iter:1536] Loss: 0.565 | Acc: 71.333% \n",
      "[Epoch:16, Iter:1537] Loss: 0.565 | Acc: 71.351% \n",
      "[Epoch:16, Iter:1538] Loss: 0.564 | Acc: 71.263% \n",
      "[Epoch:16, Iter:1539] Loss: 0.562 | Acc: 71.385% \n",
      "[Epoch:16, Iter:1540] Loss: 0.562 | Acc: 71.450% \n",
      "[Epoch:16, Iter:1541] Loss: 0.560 | Acc: 71.610% \n",
      "[Epoch:16, Iter:1542] Loss: 0.559 | Acc: 71.714% \n",
      "[Epoch:16, Iter:1543] Loss: 0.560 | Acc: 71.814% \n",
      "[Epoch:16, Iter:1544] Loss: 0.557 | Acc: 71.955% \n",
      "[Epoch:16, Iter:1545] Loss: 0.555 | Acc: 72.044% \n",
      "[Epoch:16, Iter:1546] Loss: 0.552 | Acc: 72.304% \n",
      "[Epoch:16, Iter:1547] Loss: 0.551 | Acc: 72.511% \n",
      "[Epoch:16, Iter:1548] Loss: 0.552 | Acc: 72.375% \n",
      "[Epoch:16, Iter:1549] Loss: 0.551 | Acc: 72.408% \n",
      "[Epoch:16, Iter:1550] Loss: 0.550 | Acc: 72.480% \n",
      "[Epoch:16, Iter:1551] Loss: 0.549 | Acc: 72.667% \n",
      "[Epoch:16, Iter:1552] Loss: 0.547 | Acc: 72.731% \n",
      "[Epoch:16, Iter:1553] Loss: 0.549 | Acc: 72.528% \n",
      "[Epoch:16, Iter:1554] Loss: 0.547 | Acc: 72.667% \n",
      "[Epoch:16, Iter:1555] Loss: 0.548 | Acc: 72.400% \n",
      "[Epoch:16, Iter:1556] Loss: 0.547 | Acc: 72.464% \n",
      "[Epoch:16, Iter:1557] Loss: 0.546 | Acc: 72.596% \n",
      "[Epoch:16, Iter:1558] Loss: 0.547 | Acc: 72.448% \n",
      "[Epoch:16, Iter:1559] Loss: 0.546 | Acc: 72.508% \n",
      "[Epoch:16, Iter:1560] Loss: 0.547 | Acc: 72.400% \n",
      "[Epoch:16, Iter:1561] Loss: 0.547 | Acc: 72.328% \n",
      "[Epoch:16, Iter:1562] Loss: 0.547 | Acc: 72.323% \n",
      "[Epoch:16, Iter:1563] Loss: 0.546 | Acc: 72.349% \n",
      "[Epoch:16, Iter:1564] Loss: 0.547 | Acc: 72.406% \n",
      "[Epoch:16, Iter:1565] Loss: 0.545 | Acc: 72.646% \n",
      "[Epoch:16, Iter:1566] Loss: 0.545 | Acc: 72.727% \n",
      "[Epoch:16, Iter:1567] Loss: 0.545 | Acc: 72.627% \n",
      "[Epoch:16, Iter:1568] Loss: 0.546 | Acc: 72.618% \n",
      "[Epoch:16, Iter:1569] Loss: 0.546 | Acc: 72.609% \n",
      "[Epoch:16, Iter:1570] Loss: 0.547 | Acc: 72.571% \n",
      "[Epoch:16, Iter:1571] Loss: 0.546 | Acc: 72.535% \n",
      "[Epoch:16, Iter:1572] Loss: 0.547 | Acc: 72.417% \n",
      "[Epoch:16, Iter:1573] Loss: 0.546 | Acc: 72.521% \n",
      "[Epoch:16, Iter:1574] Loss: 0.547 | Acc: 72.486% \n",
      "[Epoch:16, Iter:1575] Loss: 0.548 | Acc: 72.453% \n",
      "[Epoch:16, Iter:1576] Loss: 0.549 | Acc: 72.263% \n",
      "[Epoch:16, Iter:1577] Loss: 0.550 | Acc: 72.182% \n",
      "[Epoch:16, Iter:1578] Loss: 0.551 | Acc: 72.026% \n",
      "[Epoch:16, Iter:1579] Loss: 0.552 | Acc: 71.873% \n",
      "[Epoch:16, Iter:1580] Loss: 0.551 | Acc: 71.975% \n",
      "[Epoch:16, Iter:1581] Loss: 0.551 | Acc: 71.901% \n",
      "[Epoch:16, Iter:1582] Loss: 0.550 | Acc: 71.927% \n",
      "[Epoch:16, Iter:1583] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:16, Iter:1584] Loss: 0.549 | Acc: 71.976% \n",
      "[Epoch:16, Iter:1585] Loss: 0.549 | Acc: 72.024% \n",
      "[Epoch:16, Iter:1586] Loss: 0.548 | Acc: 72.047% \n",
      "[Epoch:16, Iter:1587] Loss: 0.548 | Acc: 72.046% \n",
      "[Epoch:16, Iter:1588] Loss: 0.547 | Acc: 72.205% \n",
      "[Epoch:16, Iter:1589] Loss: 0.546 | Acc: 72.180% \n",
      "[Epoch:16, Iter:1590] Loss: 0.545 | Acc: 72.267% \n",
      "[Epoch:16, Iter:1591] Loss: 0.545 | Acc: 72.286% \n",
      "[Epoch:16, Iter:1592] Loss: 0.544 | Acc: 72.304% \n",
      "[Epoch:16, Iter:1593] Loss: 0.544 | Acc: 72.301% \n",
      "[Epoch:16, Iter:1594] Loss: 0.544 | Acc: 72.319% \n",
      "[Epoch:16, Iter:1595] Loss: 0.543 | Acc: 72.379% \n",
      "[Epoch:16, Iter:1596] Loss: 0.544 | Acc: 72.354% \n",
      "[Epoch:16, Iter:1597] Loss: 0.545 | Acc: 72.309% \n",
      "[Epoch:16, Iter:1598] Loss: 0.544 | Acc: 72.388% \n",
      "[Epoch:16, Iter:1599] Loss: 0.544 | Acc: 72.343% \n",
      "[Epoch:16, Iter:1600] Loss: 0.544 | Acc: 72.260% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.020%\n",
      "Training set's accuracy (after quantization) is: 72.260%\n",
      "Test set's accuracy (before quantization) is: 72.800%\n",
      "Test set's accuracy (after quantization) is: 71.400%\n",
      "Train Loss: 0.540 | Train Acc: 73.020% | Test Loss: 0.544 | Test Acc: 72.800% \n",
      "Quantized Train Loss: 0.548 | Quantized Train Acc: 72.260% | Quantized Test Loss: 0.550 | Quantized Test Acc: 71.400% \n",
      "\n",
      "Epoch: 17\n",
      "[Epoch:17, Iter:1601] Loss: 0.522 | Acc: 76.000% \n",
      "[Epoch:17, Iter:1602] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:17, Iter:1603] Loss: 0.564 | Acc: 72.000% \n",
      "[Epoch:17, Iter:1604] Loss: 0.543 | Acc: 73.500% \n",
      "[Epoch:17, Iter:1605] Loss: 0.549 | Acc: 73.200% \n",
      "[Epoch:17, Iter:1606] Loss: 0.553 | Acc: 72.667% \n",
      "[Epoch:17, Iter:1607] Loss: 0.551 | Acc: 72.571% \n",
      "[Epoch:17, Iter:1608] Loss: 0.550 | Acc: 72.750% \n",
      "[Epoch:17, Iter:1609] Loss: 0.551 | Acc: 72.444% \n",
      "[Epoch:17, Iter:1610] Loss: 0.550 | Acc: 72.400% \n",
      "[Epoch:17, Iter:1611] Loss: 0.545 | Acc: 72.727% \n",
      "[Epoch:17, Iter:1612] Loss: 0.538 | Acc: 73.333% \n",
      "[Epoch:17, Iter:1613] Loss: 0.536 | Acc: 73.538% \n",
      "[Epoch:17, Iter:1614] Loss: 0.545 | Acc: 72.857% \n",
      "[Epoch:17, Iter:1615] Loss: 0.541 | Acc: 72.800% \n",
      "[Epoch:17, Iter:1616] Loss: 0.540 | Acc: 72.625% \n",
      "[Epoch:17, Iter:1617] Loss: 0.537 | Acc: 73.059% \n",
      "[Epoch:17, Iter:1618] Loss: 0.530 | Acc: 73.667% \n",
      "[Epoch:17, Iter:1619] Loss: 0.529 | Acc: 73.579% \n",
      "[Epoch:17, Iter:1620] Loss: 0.527 | Acc: 73.700% \n",
      "[Epoch:17, Iter:1621] Loss: 0.527 | Acc: 73.524% \n",
      "[Epoch:17, Iter:1622] Loss: 0.528 | Acc: 73.273% \n",
      "[Epoch:17, Iter:1623] Loss: 0.523 | Acc: 73.565% \n",
      "[Epoch:17, Iter:1624] Loss: 0.520 | Acc: 73.750% \n",
      "[Epoch:17, Iter:1625] Loss: 0.521 | Acc: 73.760% \n",
      "[Epoch:17, Iter:1626] Loss: 0.517 | Acc: 74.154% \n",
      "[Epoch:17, Iter:1627] Loss: 0.517 | Acc: 74.148% \n",
      "[Epoch:17, Iter:1628] Loss: 0.518 | Acc: 74.071% \n",
      "[Epoch:17, Iter:1629] Loss: 0.516 | Acc: 74.207% \n",
      "[Epoch:17, Iter:1630] Loss: 0.518 | Acc: 74.067% \n",
      "[Epoch:17, Iter:1631] Loss: 0.517 | Acc: 74.323% \n",
      "[Epoch:17, Iter:1632] Loss: 0.519 | Acc: 74.062% \n",
      "[Epoch:17, Iter:1633] Loss: 0.522 | Acc: 73.818% \n",
      "[Epoch:17, Iter:1634] Loss: 0.522 | Acc: 73.588% \n",
      "[Epoch:17, Iter:1635] Loss: 0.523 | Acc: 73.600% \n",
      "[Epoch:17, Iter:1636] Loss: 0.524 | Acc: 73.611% \n",
      "[Epoch:17, Iter:1637] Loss: 0.525 | Acc: 73.622% \n",
      "[Epoch:17, Iter:1638] Loss: 0.528 | Acc: 73.474% \n",
      "[Epoch:17, Iter:1639] Loss: 0.528 | Acc: 73.385% \n",
      "[Epoch:17, Iter:1640] Loss: 0.527 | Acc: 73.550% \n",
      "[Epoch:17, Iter:1641] Loss: 0.527 | Acc: 73.512% \n",
      "[Epoch:17, Iter:1642] Loss: 0.527 | Acc: 73.286% \n",
      "[Epoch:17, Iter:1643] Loss: 0.530 | Acc: 73.163% \n",
      "[Epoch:17, Iter:1644] Loss: 0.531 | Acc: 73.000% \n",
      "[Epoch:17, Iter:1645] Loss: 0.528 | Acc: 73.378% \n",
      "[Epoch:17, Iter:1646] Loss: 0.525 | Acc: 73.522% \n",
      "[Epoch:17, Iter:1647] Loss: 0.523 | Acc: 73.660% \n",
      "[Epoch:17, Iter:1648] Loss: 0.522 | Acc: 73.792% \n",
      "[Epoch:17, Iter:1649] Loss: 0.525 | Acc: 73.673% \n",
      "[Epoch:17, Iter:1650] Loss: 0.526 | Acc: 73.720% \n",
      "[Epoch:17, Iter:1651] Loss: 0.526 | Acc: 73.608% \n",
      "[Epoch:17, Iter:1652] Loss: 0.525 | Acc: 73.654% \n",
      "[Epoch:17, Iter:1653] Loss: 0.524 | Acc: 73.736% \n",
      "[Epoch:17, Iter:1654] Loss: 0.526 | Acc: 73.667% \n",
      "[Epoch:17, Iter:1655] Loss: 0.525 | Acc: 73.600% \n",
      "[Epoch:17, Iter:1656] Loss: 0.526 | Acc: 73.500% \n",
      "[Epoch:17, Iter:1657] Loss: 0.528 | Acc: 73.368% \n",
      "[Epoch:17, Iter:1658] Loss: 0.527 | Acc: 73.483% \n",
      "[Epoch:17, Iter:1659] Loss: 0.531 | Acc: 73.220% \n",
      "[Epoch:17, Iter:1660] Loss: 0.531 | Acc: 73.100% \n",
      "[Epoch:17, Iter:1661] Loss: 0.531 | Acc: 73.082% \n",
      "[Epoch:17, Iter:1662] Loss: 0.529 | Acc: 73.258% \n",
      "[Epoch:17, Iter:1663] Loss: 0.528 | Acc: 73.397% \n",
      "[Epoch:17, Iter:1664] Loss: 0.529 | Acc: 73.375% \n",
      "[Epoch:17, Iter:1665] Loss: 0.532 | Acc: 73.169% \n",
      "[Epoch:17, Iter:1666] Loss: 0.532 | Acc: 73.152% \n",
      "[Epoch:17, Iter:1667] Loss: 0.534 | Acc: 73.045% \n",
      "[Epoch:17, Iter:1668] Loss: 0.534 | Acc: 73.059% \n",
      "[Epoch:17, Iter:1669] Loss: 0.534 | Acc: 73.101% \n",
      "[Epoch:17, Iter:1670] Loss: 0.535 | Acc: 73.086% \n",
      "[Epoch:17, Iter:1671] Loss: 0.538 | Acc: 73.014% \n",
      "[Epoch:17, Iter:1672] Loss: 0.538 | Acc: 73.028% \n",
      "[Epoch:17, Iter:1673] Loss: 0.536 | Acc: 73.205% \n",
      "[Epoch:17, Iter:1674] Loss: 0.538 | Acc: 73.081% \n",
      "[Epoch:17, Iter:1675] Loss: 0.538 | Acc: 73.147% \n",
      "[Epoch:17, Iter:1676] Loss: 0.539 | Acc: 73.105% \n",
      "[Epoch:17, Iter:1677] Loss: 0.538 | Acc: 73.247% \n",
      "[Epoch:17, Iter:1678] Loss: 0.538 | Acc: 73.256% \n",
      "[Epoch:17, Iter:1679] Loss: 0.538 | Acc: 73.139% \n",
      "[Epoch:17, Iter:1680] Loss: 0.538 | Acc: 73.075% \n",
      "[Epoch:17, Iter:1681] Loss: 0.537 | Acc: 73.259% \n",
      "[Epoch:17, Iter:1682] Loss: 0.538 | Acc: 73.220% \n",
      "[Epoch:17, Iter:1683] Loss: 0.539 | Acc: 73.181% \n",
      "[Epoch:17, Iter:1684] Loss: 0.539 | Acc: 73.214% \n",
      "[Epoch:17, Iter:1685] Loss: 0.539 | Acc: 73.247% \n",
      "[Epoch:17, Iter:1686] Loss: 0.539 | Acc: 73.209% \n",
      "[Epoch:17, Iter:1687] Loss: 0.540 | Acc: 73.241% \n",
      "[Epoch:17, Iter:1688] Loss: 0.542 | Acc: 73.182% \n",
      "[Epoch:17, Iter:1689] Loss: 0.542 | Acc: 73.191% \n",
      "[Epoch:17, Iter:1690] Loss: 0.543 | Acc: 73.156% \n",
      "[Epoch:17, Iter:1691] Loss: 0.542 | Acc: 73.055% \n",
      "[Epoch:17, Iter:1692] Loss: 0.542 | Acc: 73.022% \n",
      "[Epoch:17, Iter:1693] Loss: 0.543 | Acc: 72.989% \n",
      "[Epoch:17, Iter:1694] Loss: 0.542 | Acc: 73.085% \n",
      "[Epoch:17, Iter:1695] Loss: 0.543 | Acc: 73.032% \n",
      "[Epoch:17, Iter:1696] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:17, Iter:1697] Loss: 0.543 | Acc: 72.928% \n",
      "[Epoch:17, Iter:1698] Loss: 0.543 | Acc: 72.878% \n",
      "[Epoch:17, Iter:1699] Loss: 0.543 | Acc: 72.889% \n",
      "[Epoch:17, Iter:1700] Loss: 0.542 | Acc: 72.880% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.120%\n",
      "Training set's accuracy (after quantization) is: 72.940%\n",
      "Test set's accuracy (before quantization) is: 72.800%\n",
      "Test set's accuracy (after quantization) is: 72.900%\n",
      "Train Loss: 0.552 | Train Acc: 72.120% | Test Loss: 0.556 | Test Acc: 72.800% \n",
      "Quantized Train Loss: 0.543 | Quantized Train Acc: 72.940% | Quantized Test Loss: 0.545 | Quantized Test Acc: 72.900% \n",
      "\n",
      "Epoch: 18\n",
      "[Epoch:18, Iter:1701] Loss: 0.559 | Acc: 70.000% \n",
      "[Epoch:18, Iter:1702] Loss: 0.492 | Acc: 76.000% \n",
      "[Epoch:18, Iter:1703] Loss: 0.576 | Acc: 70.667% \n",
      "[Epoch:18, Iter:1704] Loss: 0.579 | Acc: 69.000% \n",
      "[Epoch:18, Iter:1705] Loss: 0.591 | Acc: 68.400% \n",
      "[Epoch:18, Iter:1706] Loss: 0.575 | Acc: 69.667% \n",
      "[Epoch:18, Iter:1707] Loss: 0.570 | Acc: 70.286% \n",
      "[Epoch:18, Iter:1708] Loss: 0.587 | Acc: 69.500% \n",
      "[Epoch:18, Iter:1709] Loss: 0.585 | Acc: 69.556% \n",
      "[Epoch:18, Iter:1710] Loss: 0.586 | Acc: 69.200% \n",
      "[Epoch:18, Iter:1711] Loss: 0.592 | Acc: 69.273% \n",
      "[Epoch:18, Iter:1712] Loss: 0.592 | Acc: 69.667% \n",
      "[Epoch:18, Iter:1713] Loss: 0.586 | Acc: 70.615% \n",
      "[Epoch:18, Iter:1714] Loss: 0.575 | Acc: 71.000% \n",
      "[Epoch:18, Iter:1715] Loss: 0.569 | Acc: 71.600% \n",
      "[Epoch:18, Iter:1716] Loss: 0.559 | Acc: 72.125% \n",
      "[Epoch:18, Iter:1717] Loss: 0.559 | Acc: 72.118% \n",
      "[Epoch:18, Iter:1718] Loss: 0.561 | Acc: 71.778% \n",
      "[Epoch:18, Iter:1719] Loss: 0.556 | Acc: 72.105% \n",
      "[Epoch:18, Iter:1720] Loss: 0.556 | Acc: 72.100% \n",
      "[Epoch:18, Iter:1721] Loss: 0.551 | Acc: 72.571% \n",
      "[Epoch:18, Iter:1722] Loss: 0.550 | Acc: 72.545% \n",
      "[Epoch:18, Iter:1723] Loss: 0.551 | Acc: 72.783% \n",
      "[Epoch:18, Iter:1724] Loss: 0.548 | Acc: 73.000% \n",
      "[Epoch:18, Iter:1725] Loss: 0.550 | Acc: 72.800% \n",
      "[Epoch:18, Iter:1726] Loss: 0.545 | Acc: 73.000% \n",
      "[Epoch:18, Iter:1727] Loss: 0.546 | Acc: 72.741% \n",
      "[Epoch:18, Iter:1728] Loss: 0.546 | Acc: 72.786% \n",
      "[Epoch:18, Iter:1729] Loss: 0.543 | Acc: 73.103% \n",
      "[Epoch:18, Iter:1730] Loss: 0.540 | Acc: 73.200% \n",
      "[Epoch:18, Iter:1731] Loss: 0.542 | Acc: 73.226% \n",
      "[Epoch:18, Iter:1732] Loss: 0.542 | Acc: 73.000% \n",
      "[Epoch:18, Iter:1733] Loss: 0.543 | Acc: 72.970% \n",
      "[Epoch:18, Iter:1734] Loss: 0.544 | Acc: 72.765% \n",
      "[Epoch:18, Iter:1735] Loss: 0.546 | Acc: 72.514% \n",
      "[Epoch:18, Iter:1736] Loss: 0.548 | Acc: 72.444% \n",
      "[Epoch:18, Iter:1737] Loss: 0.547 | Acc: 72.757% \n",
      "[Epoch:18, Iter:1738] Loss: 0.546 | Acc: 72.947% \n",
      "[Epoch:18, Iter:1739] Loss: 0.548 | Acc: 72.667% \n",
      "[Epoch:18, Iter:1740] Loss: 0.548 | Acc: 72.550% \n",
      "[Epoch:18, Iter:1741] Loss: 0.545 | Acc: 72.829% \n",
      "[Epoch:18, Iter:1742] Loss: 0.546 | Acc: 72.714% \n",
      "[Epoch:18, Iter:1743] Loss: 0.544 | Acc: 72.884% \n",
      "[Epoch:18, Iter:1744] Loss: 0.546 | Acc: 72.818% \n",
      "[Epoch:18, Iter:1745] Loss: 0.546 | Acc: 72.667% \n",
      "[Epoch:18, Iter:1746] Loss: 0.545 | Acc: 72.696% \n",
      "[Epoch:18, Iter:1747] Loss: 0.544 | Acc: 72.809% \n",
      "[Epoch:18, Iter:1748] Loss: 0.548 | Acc: 72.583% \n",
      "[Epoch:18, Iter:1749] Loss: 0.548 | Acc: 72.612% \n",
      "[Epoch:18, Iter:1750] Loss: 0.548 | Acc: 72.600% \n",
      "[Epoch:18, Iter:1751] Loss: 0.549 | Acc: 72.549% \n",
      "[Epoch:18, Iter:1752] Loss: 0.548 | Acc: 72.538% \n",
      "[Epoch:18, Iter:1753] Loss: 0.548 | Acc: 72.453% \n",
      "[Epoch:18, Iter:1754] Loss: 0.547 | Acc: 72.593% \n",
      "[Epoch:18, Iter:1755] Loss: 0.546 | Acc: 72.618% \n",
      "[Epoch:18, Iter:1756] Loss: 0.545 | Acc: 72.643% \n",
      "[Epoch:18, Iter:1757] Loss: 0.547 | Acc: 72.596% \n",
      "[Epoch:18, Iter:1758] Loss: 0.547 | Acc: 72.586% \n",
      "[Epoch:18, Iter:1759] Loss: 0.547 | Acc: 72.508% \n",
      "[Epoch:18, Iter:1760] Loss: 0.546 | Acc: 72.600% \n",
      "[Epoch:18, Iter:1761] Loss: 0.548 | Acc: 72.393% \n",
      "[Epoch:18, Iter:1762] Loss: 0.548 | Acc: 72.419% \n",
      "[Epoch:18, Iter:1763] Loss: 0.547 | Acc: 72.540% \n",
      "[Epoch:18, Iter:1764] Loss: 0.547 | Acc: 72.594% \n",
      "[Epoch:18, Iter:1765] Loss: 0.544 | Acc: 72.738% \n",
      "[Epoch:18, Iter:1766] Loss: 0.544 | Acc: 72.667% \n",
      "[Epoch:18, Iter:1767] Loss: 0.544 | Acc: 72.746% \n",
      "[Epoch:18, Iter:1768] Loss: 0.543 | Acc: 72.853% \n",
      "[Epoch:18, Iter:1769] Loss: 0.546 | Acc: 72.667% \n",
      "[Epoch:18, Iter:1770] Loss: 0.546 | Acc: 72.629% \n",
      "[Epoch:18, Iter:1771] Loss: 0.546 | Acc: 72.620% \n",
      "[Epoch:18, Iter:1772] Loss: 0.547 | Acc: 72.556% \n",
      "[Epoch:18, Iter:1773] Loss: 0.548 | Acc: 72.411% \n",
      "[Epoch:18, Iter:1774] Loss: 0.549 | Acc: 72.351% \n",
      "[Epoch:18, Iter:1775] Loss: 0.548 | Acc: 72.453% \n",
      "[Epoch:18, Iter:1776] Loss: 0.548 | Acc: 72.553% \n",
      "[Epoch:18, Iter:1777] Loss: 0.548 | Acc: 72.571% \n",
      "[Epoch:18, Iter:1778] Loss: 0.548 | Acc: 72.590% \n",
      "[Epoch:18, Iter:1779] Loss: 0.547 | Acc: 72.582% \n",
      "[Epoch:18, Iter:1780] Loss: 0.547 | Acc: 72.525% \n",
      "[Epoch:18, Iter:1781] Loss: 0.547 | Acc: 72.568% \n",
      "[Epoch:18, Iter:1782] Loss: 0.547 | Acc: 72.585% \n",
      "[Epoch:18, Iter:1783] Loss: 0.547 | Acc: 72.578% \n",
      "[Epoch:18, Iter:1784] Loss: 0.546 | Acc: 72.643% \n",
      "[Epoch:18, Iter:1785] Loss: 0.545 | Acc: 72.729% \n",
      "[Epoch:18, Iter:1786] Loss: 0.544 | Acc: 72.791% \n",
      "[Epoch:18, Iter:1787] Loss: 0.544 | Acc: 72.874% \n",
      "[Epoch:18, Iter:1788] Loss: 0.543 | Acc: 72.932% \n",
      "[Epoch:18, Iter:1789] Loss: 0.543 | Acc: 72.944% \n",
      "[Epoch:18, Iter:1790] Loss: 0.543 | Acc: 73.022% \n",
      "[Epoch:18, Iter:1791] Loss: 0.542 | Acc: 73.099% \n",
      "[Epoch:18, Iter:1792] Loss: 0.543 | Acc: 73.065% \n",
      "[Epoch:18, Iter:1793] Loss: 0.544 | Acc: 72.989% \n",
      "[Epoch:18, Iter:1794] Loss: 0.545 | Acc: 72.915% \n",
      "[Epoch:18, Iter:1795] Loss: 0.544 | Acc: 72.821% \n",
      "[Epoch:18, Iter:1796] Loss: 0.544 | Acc: 72.958% \n",
      "[Epoch:18, Iter:1797] Loss: 0.544 | Acc: 72.928% \n",
      "[Epoch:18, Iter:1798] Loss: 0.545 | Acc: 72.918% \n",
      "[Epoch:18, Iter:1799] Loss: 0.546 | Acc: 72.828% \n",
      "[Epoch:18, Iter:1800] Loss: 0.545 | Acc: 72.900% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.300%\n",
      "Training set's accuracy (after quantization) is: 72.540%\n",
      "Test set's accuracy (before quantization) is: 73.100%\n",
      "Test set's accuracy (after quantization) is: 72.200%\n",
      "Train Loss: 0.540 | Train Acc: 73.300% | Test Loss: 0.544 | Test Acc: 73.100% \n",
      "Quantized Train Loss: 0.545 | Quantized Train Acc: 72.540% | Quantized Test Loss: 0.548 | Quantized Test Acc: 72.200% \n",
      "\n",
      "Epoch: 19\n",
      "[Epoch:19, Iter:1801] Loss: 0.606 | Acc: 68.000% \n",
      "[Epoch:19, Iter:1802] Loss: 0.584 | Acc: 68.000% \n",
      "[Epoch:19, Iter:1803] Loss: 0.551 | Acc: 71.333% \n",
      "[Epoch:19, Iter:1804] Loss: 0.542 | Acc: 72.000% \n",
      "[Epoch:19, Iter:1805] Loss: 0.545 | Acc: 71.600% \n",
      "[Epoch:19, Iter:1806] Loss: 0.547 | Acc: 71.667% \n",
      "[Epoch:19, Iter:1807] Loss: 0.538 | Acc: 72.571% \n",
      "[Epoch:19, Iter:1808] Loss: 0.538 | Acc: 72.500% \n",
      "[Epoch:19, Iter:1809] Loss: 0.520 | Acc: 74.222% \n",
      "[Epoch:19, Iter:1810] Loss: 0.528 | Acc: 73.200% \n",
      "[Epoch:19, Iter:1811] Loss: 0.524 | Acc: 73.818% \n",
      "[Epoch:19, Iter:1812] Loss: 0.526 | Acc: 74.000% \n",
      "[Epoch:19, Iter:1813] Loss: 0.527 | Acc: 74.308% \n",
      "[Epoch:19, Iter:1814] Loss: 0.520 | Acc: 74.286% \n",
      "[Epoch:19, Iter:1815] Loss: 0.517 | Acc: 74.667% \n",
      "[Epoch:19, Iter:1816] Loss: 0.515 | Acc: 75.000% \n",
      "[Epoch:19, Iter:1817] Loss: 0.513 | Acc: 74.824% \n",
      "[Epoch:19, Iter:1818] Loss: 0.515 | Acc: 74.444% \n",
      "[Epoch:19, Iter:1819] Loss: 0.513 | Acc: 74.632% \n",
      "[Epoch:19, Iter:1820] Loss: 0.518 | Acc: 74.200% \n",
      "[Epoch:19, Iter:1821] Loss: 0.517 | Acc: 74.381% \n",
      "[Epoch:19, Iter:1822] Loss: 0.519 | Acc: 74.364% \n",
      "[Epoch:19, Iter:1823] Loss: 0.520 | Acc: 74.348% \n",
      "[Epoch:19, Iter:1824] Loss: 0.522 | Acc: 74.167% \n",
      "[Epoch:19, Iter:1825] Loss: 0.525 | Acc: 74.000% \n",
      "[Epoch:19, Iter:1826] Loss: 0.527 | Acc: 73.923% \n",
      "[Epoch:19, Iter:1827] Loss: 0.525 | Acc: 74.296% \n",
      "[Epoch:19, Iter:1828] Loss: 0.525 | Acc: 74.286% \n",
      "[Epoch:19, Iter:1829] Loss: 0.528 | Acc: 74.069% \n",
      "[Epoch:19, Iter:1830] Loss: 0.526 | Acc: 74.200% \n",
      "[Epoch:19, Iter:1831] Loss: 0.525 | Acc: 74.452% \n",
      "[Epoch:19, Iter:1832] Loss: 0.532 | Acc: 74.000% \n",
      "[Epoch:19, Iter:1833] Loss: 0.532 | Acc: 73.939% \n",
      "[Epoch:19, Iter:1834] Loss: 0.531 | Acc: 74.059% \n",
      "[Epoch:19, Iter:1835] Loss: 0.534 | Acc: 74.057% \n",
      "[Epoch:19, Iter:1836] Loss: 0.536 | Acc: 74.000% \n",
      "[Epoch:19, Iter:1837] Loss: 0.538 | Acc: 73.946% \n",
      "[Epoch:19, Iter:1838] Loss: 0.536 | Acc: 73.895% \n",
      "[Epoch:19, Iter:1839] Loss: 0.535 | Acc: 74.051% \n",
      "[Epoch:19, Iter:1840] Loss: 0.534 | Acc: 74.250% \n",
      "[Epoch:19, Iter:1841] Loss: 0.532 | Acc: 74.390% \n",
      "[Epoch:19, Iter:1842] Loss: 0.531 | Acc: 74.429% \n",
      "[Epoch:19, Iter:1843] Loss: 0.533 | Acc: 74.186% \n",
      "[Epoch:19, Iter:1844] Loss: 0.535 | Acc: 73.818% \n",
      "[Epoch:19, Iter:1845] Loss: 0.537 | Acc: 73.778% \n",
      "[Epoch:19, Iter:1846] Loss: 0.536 | Acc: 73.739% \n",
      "[Epoch:19, Iter:1847] Loss: 0.537 | Acc: 73.745% \n",
      "[Epoch:19, Iter:1848] Loss: 0.539 | Acc: 73.500% \n",
      "[Epoch:19, Iter:1849] Loss: 0.539 | Acc: 73.551% \n",
      "[Epoch:19, Iter:1850] Loss: 0.542 | Acc: 73.520% \n",
      "[Epoch:19, Iter:1851] Loss: 0.541 | Acc: 73.647% \n",
      "[Epoch:19, Iter:1852] Loss: 0.542 | Acc: 73.500% \n",
      "[Epoch:19, Iter:1853] Loss: 0.541 | Acc: 73.623% \n",
      "[Epoch:19, Iter:1854] Loss: 0.543 | Acc: 73.519% \n",
      "[Epoch:19, Iter:1855] Loss: 0.543 | Acc: 73.418% \n",
      "[Epoch:19, Iter:1856] Loss: 0.541 | Acc: 73.607% \n",
      "[Epoch:19, Iter:1857] Loss: 0.543 | Acc: 73.439% \n",
      "[Epoch:19, Iter:1858] Loss: 0.542 | Acc: 73.483% \n",
      "[Epoch:19, Iter:1859] Loss: 0.543 | Acc: 73.254% \n",
      "[Epoch:19, Iter:1860] Loss: 0.543 | Acc: 73.167% \n",
      "[Epoch:19, Iter:1861] Loss: 0.542 | Acc: 73.246% \n",
      "[Epoch:19, Iter:1862] Loss: 0.541 | Acc: 73.226% \n",
      "[Epoch:19, Iter:1863] Loss: 0.541 | Acc: 73.270% \n",
      "[Epoch:19, Iter:1864] Loss: 0.541 | Acc: 73.344% \n",
      "[Epoch:19, Iter:1865] Loss: 0.542 | Acc: 73.385% \n",
      "[Epoch:19, Iter:1866] Loss: 0.542 | Acc: 73.273% \n",
      "[Epoch:19, Iter:1867] Loss: 0.541 | Acc: 73.284% \n",
      "[Epoch:19, Iter:1868] Loss: 0.542 | Acc: 73.265% \n",
      "[Epoch:19, Iter:1869] Loss: 0.543 | Acc: 73.188% \n",
      "[Epoch:19, Iter:1870] Loss: 0.542 | Acc: 73.286% \n",
      "[Epoch:19, Iter:1871] Loss: 0.543 | Acc: 73.183% \n",
      "[Epoch:19, Iter:1872] Loss: 0.544 | Acc: 73.083% \n",
      "[Epoch:19, Iter:1873] Loss: 0.544 | Acc: 73.096% \n",
      "[Epoch:19, Iter:1874] Loss: 0.545 | Acc: 73.054% \n",
      "[Epoch:19, Iter:1875] Loss: 0.544 | Acc: 73.040% \n",
      "[Epoch:19, Iter:1876] Loss: 0.546 | Acc: 72.921% \n",
      "[Epoch:19, Iter:1877] Loss: 0.545 | Acc: 72.935% \n",
      "[Epoch:19, Iter:1878] Loss: 0.543 | Acc: 73.051% \n",
      "[Epoch:19, Iter:1879] Loss: 0.543 | Acc: 73.038% \n",
      "[Epoch:19, Iter:1880] Loss: 0.543 | Acc: 73.075% \n",
      "[Epoch:19, Iter:1881] Loss: 0.544 | Acc: 73.037% \n",
      "[Epoch:19, Iter:1882] Loss: 0.544 | Acc: 72.976% \n",
      "[Epoch:19, Iter:1883] Loss: 0.545 | Acc: 72.892% \n",
      "[Epoch:19, Iter:1884] Loss: 0.545 | Acc: 72.976% \n",
      "[Epoch:19, Iter:1885] Loss: 0.544 | Acc: 73.059% \n",
      "[Epoch:19, Iter:1886] Loss: 0.545 | Acc: 72.953% \n",
      "[Epoch:19, Iter:1887] Loss: 0.544 | Acc: 73.034% \n",
      "[Epoch:19, Iter:1888] Loss: 0.545 | Acc: 73.000% \n",
      "[Epoch:19, Iter:1889] Loss: 0.545 | Acc: 73.011% \n",
      "[Epoch:19, Iter:1890] Loss: 0.545 | Acc: 72.956% \n",
      "[Epoch:19, Iter:1891] Loss: 0.545 | Acc: 72.945% \n",
      "[Epoch:19, Iter:1892] Loss: 0.545 | Acc: 72.935% \n",
      "[Epoch:19, Iter:1893] Loss: 0.544 | Acc: 72.989% \n",
      "[Epoch:19, Iter:1894] Loss: 0.544 | Acc: 72.979% \n",
      "[Epoch:19, Iter:1895] Loss: 0.545 | Acc: 72.905% \n",
      "[Epoch:19, Iter:1896] Loss: 0.545 | Acc: 72.812% \n",
      "[Epoch:19, Iter:1897] Loss: 0.545 | Acc: 72.866% \n",
      "[Epoch:19, Iter:1898] Loss: 0.545 | Acc: 72.857% \n",
      "[Epoch:19, Iter:1899] Loss: 0.545 | Acc: 72.828% \n",
      "[Epoch:19, Iter:1900] Loss: 0.545 | Acc: 72.800% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.040%\n",
      "Training set's accuracy (after quantization) is: 72.500%\n",
      "Test set's accuracy (before quantization) is: 72.900%\n",
      "Test set's accuracy (after quantization) is: 72.200%\n",
      "Train Loss: 0.540 | Train Acc: 73.040% | Test Loss: 0.543 | Test Acc: 72.900% \n",
      "Quantized Train Loss: 0.545 | Quantized Train Acc: 72.500% | Quantized Test Loss: 0.548 | Quantized Test Acc: 72.200% \n",
      "\n",
      "Epoch: 20\n",
      "[Epoch:20, Iter:1901] Loss: 0.566 | Acc: 78.000% \n",
      "[Epoch:20, Iter:1902] Loss: 0.510 | Acc: 82.000% \n",
      "[Epoch:20, Iter:1903] Loss: 0.560 | Acc: 77.333% \n",
      "[Epoch:20, Iter:1904] Loss: 0.557 | Acc: 76.000% \n",
      "[Epoch:20, Iter:1905] Loss: 0.552 | Acc: 75.200% \n",
      "[Epoch:20, Iter:1906] Loss: 0.571 | Acc: 73.000% \n",
      "[Epoch:20, Iter:1907] Loss: 0.570 | Acc: 73.143% \n",
      "[Epoch:20, Iter:1908] Loss: 0.556 | Acc: 74.000% \n",
      "[Epoch:20, Iter:1909] Loss: 0.551 | Acc: 74.444% \n",
      "[Epoch:20, Iter:1910] Loss: 0.546 | Acc: 74.600% \n",
      "[Epoch:20, Iter:1911] Loss: 0.545 | Acc: 73.818% \n",
      "[Epoch:20, Iter:1912] Loss: 0.547 | Acc: 73.667% \n",
      "[Epoch:20, Iter:1913] Loss: 0.551 | Acc: 72.615% \n",
      "[Epoch:20, Iter:1914] Loss: 0.552 | Acc: 72.714% \n",
      "[Epoch:20, Iter:1915] Loss: 0.551 | Acc: 73.067% \n",
      "[Epoch:20, Iter:1916] Loss: 0.551 | Acc: 72.875% \n",
      "[Epoch:20, Iter:1917] Loss: 0.555 | Acc: 72.706% \n",
      "[Epoch:20, Iter:1918] Loss: 0.555 | Acc: 72.667% \n",
      "[Epoch:20, Iter:1919] Loss: 0.550 | Acc: 73.263% \n",
      "[Epoch:20, Iter:1920] Loss: 0.547 | Acc: 73.600% \n",
      "[Epoch:20, Iter:1921] Loss: 0.545 | Acc: 73.714% \n",
      "[Epoch:20, Iter:1922] Loss: 0.547 | Acc: 73.364% \n",
      "[Epoch:20, Iter:1923] Loss: 0.552 | Acc: 72.522% \n",
      "[Epoch:20, Iter:1924] Loss: 0.552 | Acc: 72.417% \n",
      "[Epoch:20, Iter:1925] Loss: 0.551 | Acc: 72.560% \n",
      "[Epoch:20, Iter:1926] Loss: 0.550 | Acc: 72.615% \n",
      "[Epoch:20, Iter:1927] Loss: 0.549 | Acc: 72.593% \n",
      "[Epoch:20, Iter:1928] Loss: 0.549 | Acc: 72.571% \n",
      "[Epoch:20, Iter:1929] Loss: 0.550 | Acc: 72.414% \n",
      "[Epoch:20, Iter:1930] Loss: 0.546 | Acc: 72.667% \n",
      "[Epoch:20, Iter:1931] Loss: 0.546 | Acc: 72.645% \n",
      "[Epoch:20, Iter:1932] Loss: 0.545 | Acc: 72.688% \n",
      "[Epoch:20, Iter:1933] Loss: 0.544 | Acc: 72.788% \n",
      "[Epoch:20, Iter:1934] Loss: 0.546 | Acc: 72.529% \n",
      "[Epoch:20, Iter:1935] Loss: 0.547 | Acc: 72.457% \n",
      "[Epoch:20, Iter:1936] Loss: 0.547 | Acc: 72.444% \n",
      "[Epoch:20, Iter:1937] Loss: 0.547 | Acc: 72.541% \n",
      "[Epoch:20, Iter:1938] Loss: 0.547 | Acc: 72.526% \n",
      "[Epoch:20, Iter:1939] Loss: 0.545 | Acc: 72.769% \n",
      "[Epoch:20, Iter:1940] Loss: 0.545 | Acc: 72.750% \n",
      "[Epoch:20, Iter:1941] Loss: 0.547 | Acc: 72.780% \n",
      "[Epoch:20, Iter:1942] Loss: 0.547 | Acc: 72.667% \n",
      "[Epoch:20, Iter:1943] Loss: 0.548 | Acc: 72.651% \n",
      "[Epoch:20, Iter:1944] Loss: 0.547 | Acc: 72.500% \n",
      "[Epoch:20, Iter:1945] Loss: 0.548 | Acc: 72.489% \n",
      "[Epoch:20, Iter:1946] Loss: 0.546 | Acc: 72.609% \n",
      "[Epoch:20, Iter:1947] Loss: 0.544 | Acc: 72.681% \n",
      "[Epoch:20, Iter:1948] Loss: 0.544 | Acc: 72.583% \n",
      "[Epoch:20, Iter:1949] Loss: 0.542 | Acc: 72.653% \n",
      "[Epoch:20, Iter:1950] Loss: 0.541 | Acc: 72.720% \n",
      "[Epoch:20, Iter:1951] Loss: 0.542 | Acc: 72.627% \n",
      "[Epoch:20, Iter:1952] Loss: 0.544 | Acc: 72.654% \n",
      "[Epoch:20, Iter:1953] Loss: 0.544 | Acc: 72.604% \n",
      "[Epoch:20, Iter:1954] Loss: 0.543 | Acc: 72.630% \n",
      "[Epoch:20, Iter:1955] Loss: 0.544 | Acc: 72.509% \n",
      "[Epoch:20, Iter:1956] Loss: 0.543 | Acc: 72.536% \n",
      "[Epoch:20, Iter:1957] Loss: 0.543 | Acc: 72.632% \n",
      "[Epoch:20, Iter:1958] Loss: 0.543 | Acc: 72.724% \n",
      "[Epoch:20, Iter:1959] Loss: 0.544 | Acc: 72.712% \n",
      "[Epoch:20, Iter:1960] Loss: 0.541 | Acc: 72.800% \n",
      "[Epoch:20, Iter:1961] Loss: 0.543 | Acc: 72.721% \n",
      "[Epoch:20, Iter:1962] Loss: 0.543 | Acc: 72.581% \n",
      "[Epoch:20, Iter:1963] Loss: 0.545 | Acc: 72.571% \n",
      "[Epoch:20, Iter:1964] Loss: 0.544 | Acc: 72.625% \n",
      "[Epoch:20, Iter:1965] Loss: 0.545 | Acc: 72.646% \n",
      "[Epoch:20, Iter:1966] Loss: 0.546 | Acc: 72.636% \n",
      "[Epoch:20, Iter:1967] Loss: 0.547 | Acc: 72.537% \n",
      "[Epoch:20, Iter:1968] Loss: 0.547 | Acc: 72.588% \n",
      "[Epoch:20, Iter:1969] Loss: 0.547 | Acc: 72.609% \n",
      "[Epoch:20, Iter:1970] Loss: 0.546 | Acc: 72.657% \n",
      "[Epoch:20, Iter:1971] Loss: 0.546 | Acc: 72.648% \n",
      "[Epoch:20, Iter:1972] Loss: 0.546 | Acc: 72.806% \n",
      "[Epoch:20, Iter:1973] Loss: 0.544 | Acc: 72.849% \n",
      "[Epoch:20, Iter:1974] Loss: 0.545 | Acc: 72.865% \n",
      "[Epoch:20, Iter:1975] Loss: 0.544 | Acc: 72.907% \n",
      "[Epoch:20, Iter:1976] Loss: 0.543 | Acc: 72.947% \n",
      "[Epoch:20, Iter:1977] Loss: 0.547 | Acc: 72.727% \n",
      "[Epoch:20, Iter:1978] Loss: 0.545 | Acc: 72.846% \n",
      "[Epoch:20, Iter:1979] Loss: 0.545 | Acc: 72.785% \n",
      "[Epoch:20, Iter:1980] Loss: 0.546 | Acc: 72.775% \n",
      "[Epoch:20, Iter:1981] Loss: 0.546 | Acc: 72.716% \n",
      "[Epoch:20, Iter:1982] Loss: 0.545 | Acc: 72.805% \n",
      "[Epoch:20, Iter:1983] Loss: 0.545 | Acc: 72.867% \n",
      "[Epoch:20, Iter:1984] Loss: 0.545 | Acc: 72.810% \n",
      "[Epoch:20, Iter:1985] Loss: 0.544 | Acc: 72.894% \n",
      "[Epoch:20, Iter:1986] Loss: 0.544 | Acc: 72.837% \n",
      "[Epoch:20, Iter:1987] Loss: 0.543 | Acc: 72.874% \n",
      "[Epoch:20, Iter:1988] Loss: 0.542 | Acc: 72.955% \n",
      "[Epoch:20, Iter:1989] Loss: 0.543 | Acc: 72.899% \n",
      "[Epoch:20, Iter:1990] Loss: 0.543 | Acc: 72.933% \n",
      "[Epoch:20, Iter:1991] Loss: 0.543 | Acc: 72.923% \n",
      "[Epoch:20, Iter:1992] Loss: 0.543 | Acc: 72.891% \n",
      "[Epoch:20, Iter:1993] Loss: 0.543 | Acc: 72.839% \n",
      "[Epoch:20, Iter:1994] Loss: 0.543 | Acc: 72.830% \n",
      "[Epoch:20, Iter:1995] Loss: 0.542 | Acc: 72.947% \n",
      "[Epoch:20, Iter:1996] Loss: 0.543 | Acc: 72.979% \n",
      "[Epoch:20, Iter:1997] Loss: 0.543 | Acc: 72.928% \n",
      "[Epoch:20, Iter:1998] Loss: 0.544 | Acc: 72.898% \n",
      "[Epoch:20, Iter:1999] Loss: 0.544 | Acc: 72.848% \n",
      "[Epoch:20, Iter:2000] Loss: 0.545 | Acc: 72.740% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.600%\n",
      "Training set's accuracy (after quantization) is: 71.800%\n",
      "Test set's accuracy (before quantization) is: 71.500%\n",
      "Test set's accuracy (after quantization) is: 71.000%\n",
      "Train Loss: 0.547 | Train Acc: 72.600% | Test Loss: 0.551 | Test Acc: 71.500% \n",
      "Quantized Train Loss: 0.551 | Quantized Train Acc: 71.800% | Quantized Test Loss: 0.553 | Quantized Test Acc: 71.000% \n",
      "\n",
      "Epoch: 21\n",
      "[Epoch:21, Iter:2001] Loss: 0.666 | Acc: 62.000% \n",
      "[Epoch:21, Iter:2002] Loss: 0.665 | Acc: 63.000% \n",
      "[Epoch:21, Iter:2003] Loss: 0.596 | Acc: 69.333% \n",
      "[Epoch:21, Iter:2004] Loss: 0.582 | Acc: 70.000% \n",
      "[Epoch:21, Iter:2005] Loss: 0.583 | Acc: 70.800% \n",
      "[Epoch:21, Iter:2006] Loss: 0.579 | Acc: 71.667% \n",
      "[Epoch:21, Iter:2007] Loss: 0.575 | Acc: 71.143% \n",
      "[Epoch:21, Iter:2008] Loss: 0.579 | Acc: 71.750% \n",
      "[Epoch:21, Iter:2009] Loss: 0.580 | Acc: 71.556% \n",
      "[Epoch:21, Iter:2010] Loss: 0.570 | Acc: 72.200% \n",
      "[Epoch:21, Iter:2011] Loss: 0.587 | Acc: 70.364% \n",
      "[Epoch:21, Iter:2012] Loss: 0.587 | Acc: 70.000% \n",
      "[Epoch:21, Iter:2013] Loss: 0.586 | Acc: 69.846% \n",
      "[Epoch:21, Iter:2014] Loss: 0.581 | Acc: 70.143% \n",
      "[Epoch:21, Iter:2015] Loss: 0.573 | Acc: 70.933% \n",
      "[Epoch:21, Iter:2016] Loss: 0.576 | Acc: 70.750% \n",
      "[Epoch:21, Iter:2017] Loss: 0.571 | Acc: 71.294% \n",
      "[Epoch:21, Iter:2018] Loss: 0.579 | Acc: 71.000% \n",
      "[Epoch:21, Iter:2019] Loss: 0.574 | Acc: 71.474% \n",
      "[Epoch:21, Iter:2020] Loss: 0.575 | Acc: 71.300% \n",
      "[Epoch:21, Iter:2021] Loss: 0.582 | Acc: 70.571% \n",
      "[Epoch:21, Iter:2022] Loss: 0.580 | Acc: 70.727% \n",
      "[Epoch:21, Iter:2023] Loss: 0.577 | Acc: 70.957% \n",
      "[Epoch:21, Iter:2024] Loss: 0.578 | Acc: 70.583% \n",
      "[Epoch:21, Iter:2025] Loss: 0.574 | Acc: 71.120% \n",
      "[Epoch:21, Iter:2026] Loss: 0.575 | Acc: 71.000% \n",
      "[Epoch:21, Iter:2027] Loss: 0.574 | Acc: 70.963% \n",
      "[Epoch:21, Iter:2028] Loss: 0.572 | Acc: 70.714% \n",
      "[Epoch:21, Iter:2029] Loss: 0.569 | Acc: 70.759% \n",
      "[Epoch:21, Iter:2030] Loss: 0.569 | Acc: 70.933% \n",
      "[Epoch:21, Iter:2031] Loss: 0.570 | Acc: 70.645% \n",
      "[Epoch:21, Iter:2032] Loss: 0.570 | Acc: 70.562% \n",
      "[Epoch:21, Iter:2033] Loss: 0.568 | Acc: 70.545% \n",
      "[Epoch:21, Iter:2034] Loss: 0.567 | Acc: 70.706% \n",
      "[Epoch:21, Iter:2035] Loss: 0.565 | Acc: 70.800% \n",
      "[Epoch:21, Iter:2036] Loss: 0.565 | Acc: 70.667% \n",
      "[Epoch:21, Iter:2037] Loss: 0.567 | Acc: 70.486% \n",
      "[Epoch:21, Iter:2038] Loss: 0.565 | Acc: 70.632% \n",
      "[Epoch:21, Iter:2039] Loss: 0.564 | Acc: 70.769% \n",
      "[Epoch:21, Iter:2040] Loss: 0.567 | Acc: 70.650% \n",
      "[Epoch:21, Iter:2041] Loss: 0.565 | Acc: 70.780% \n",
      "[Epoch:21, Iter:2042] Loss: 0.564 | Acc: 70.762% \n",
      "[Epoch:21, Iter:2043] Loss: 0.563 | Acc: 70.930% \n",
      "[Epoch:21, Iter:2044] Loss: 0.562 | Acc: 70.955% \n",
      "[Epoch:21, Iter:2045] Loss: 0.563 | Acc: 70.978% \n",
      "[Epoch:21, Iter:2046] Loss: 0.564 | Acc: 70.957% \n",
      "[Epoch:21, Iter:2047] Loss: 0.564 | Acc: 70.936% \n",
      "[Epoch:21, Iter:2048] Loss: 0.563 | Acc: 70.958% \n",
      "[Epoch:21, Iter:2049] Loss: 0.562 | Acc: 70.980% \n",
      "[Epoch:21, Iter:2050] Loss: 0.562 | Acc: 70.920% \n",
      "[Epoch:21, Iter:2051] Loss: 0.562 | Acc: 70.902% \n",
      "[Epoch:21, Iter:2052] Loss: 0.561 | Acc: 70.885% \n",
      "[Epoch:21, Iter:2053] Loss: 0.562 | Acc: 70.755% \n",
      "[Epoch:21, Iter:2054] Loss: 0.562 | Acc: 70.778% \n",
      "[Epoch:21, Iter:2055] Loss: 0.563 | Acc: 70.618% \n",
      "[Epoch:21, Iter:2056] Loss: 0.563 | Acc: 70.750% \n",
      "[Epoch:21, Iter:2057] Loss: 0.564 | Acc: 70.596% \n",
      "[Epoch:21, Iter:2058] Loss: 0.565 | Acc: 70.552% \n",
      "[Epoch:21, Iter:2059] Loss: 0.564 | Acc: 70.678% \n",
      "[Epoch:21, Iter:2060] Loss: 0.563 | Acc: 70.700% \n",
      "[Epoch:21, Iter:2061] Loss: 0.562 | Acc: 70.852% \n",
      "[Epoch:21, Iter:2062] Loss: 0.561 | Acc: 70.968% \n",
      "[Epoch:21, Iter:2063] Loss: 0.559 | Acc: 71.143% \n",
      "[Epoch:21, Iter:2064] Loss: 0.559 | Acc: 71.125% \n",
      "[Epoch:21, Iter:2065] Loss: 0.559 | Acc: 71.108% \n",
      "[Epoch:21, Iter:2066] Loss: 0.557 | Acc: 71.242% \n",
      "[Epoch:21, Iter:2067] Loss: 0.556 | Acc: 71.284% \n",
      "[Epoch:21, Iter:2068] Loss: 0.554 | Acc: 71.412% \n",
      "[Epoch:21, Iter:2069] Loss: 0.554 | Acc: 71.420% \n",
      "[Epoch:21, Iter:2070] Loss: 0.555 | Acc: 71.314% \n",
      "[Epoch:21, Iter:2071] Loss: 0.553 | Acc: 71.380% \n",
      "[Epoch:21, Iter:2072] Loss: 0.551 | Acc: 71.583% \n",
      "[Epoch:21, Iter:2073] Loss: 0.552 | Acc: 71.534% \n",
      "[Epoch:21, Iter:2074] Loss: 0.552 | Acc: 71.595% \n",
      "[Epoch:21, Iter:2075] Loss: 0.552 | Acc: 71.547% \n",
      "[Epoch:21, Iter:2076] Loss: 0.551 | Acc: 71.553% \n",
      "[Epoch:21, Iter:2077] Loss: 0.549 | Acc: 71.714% \n",
      "[Epoch:21, Iter:2078] Loss: 0.549 | Acc: 71.744% \n",
      "[Epoch:21, Iter:2079] Loss: 0.550 | Acc: 71.722% \n",
      "[Epoch:21, Iter:2080] Loss: 0.551 | Acc: 71.700% \n",
      "[Epoch:21, Iter:2081] Loss: 0.551 | Acc: 71.778% \n",
      "[Epoch:21, Iter:2082] Loss: 0.550 | Acc: 71.829% \n",
      "[Epoch:21, Iter:2083] Loss: 0.551 | Acc: 71.807% \n",
      "[Epoch:21, Iter:2084] Loss: 0.549 | Acc: 71.905% \n",
      "[Epoch:21, Iter:2085] Loss: 0.548 | Acc: 71.953% \n",
      "[Epoch:21, Iter:2086] Loss: 0.550 | Acc: 71.814% \n",
      "[Epoch:21, Iter:2087] Loss: 0.550 | Acc: 71.862% \n",
      "[Epoch:21, Iter:2088] Loss: 0.549 | Acc: 71.909% \n",
      "[Epoch:21, Iter:2089] Loss: 0.548 | Acc: 71.978% \n",
      "[Epoch:21, Iter:2090] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2091] Loss: 0.550 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2092] Loss: 0.548 | Acc: 72.065% \n",
      "[Epoch:21, Iter:2093] Loss: 0.549 | Acc: 72.065% \n",
      "[Epoch:21, Iter:2094] Loss: 0.549 | Acc: 72.128% \n",
      "[Epoch:21, Iter:2095] Loss: 0.547 | Acc: 72.189% \n",
      "[Epoch:21, Iter:2096] Loss: 0.546 | Acc: 72.208% \n",
      "[Epoch:21, Iter:2097] Loss: 0.546 | Acc: 72.289% \n",
      "[Epoch:21, Iter:2098] Loss: 0.545 | Acc: 72.327% \n",
      "[Epoch:21, Iter:2099] Loss: 0.545 | Acc: 72.242% \n",
      "[Epoch:21, Iter:2100] Loss: 0.547 | Acc: 72.160% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.500%\n",
      "Training set's accuracy (after quantization) is: 72.000%\n",
      "Test set's accuracy (before quantization) is: 71.600%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.547 | Train Acc: 72.500% | Test Loss: 0.549 | Test Acc: 71.600% \n",
      "Quantized Train Loss: 0.549 | Quantized Train Acc: 72.000% | Quantized Test Loss: 0.552 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 22\n",
      "[Epoch:22, Iter:2101] Loss: 0.492 | Acc: 78.000% \n",
      "[Epoch:22, Iter:2102] Loss: 0.490 | Acc: 75.000% \n",
      "[Epoch:22, Iter:2103] Loss: 0.508 | Acc: 74.667% \n",
      "[Epoch:22, Iter:2104] Loss: 0.500 | Acc: 75.500% \n",
      "[Epoch:22, Iter:2105] Loss: 0.491 | Acc: 75.600% \n",
      "[Epoch:22, Iter:2106] Loss: 0.518 | Acc: 74.667% \n",
      "[Epoch:22, Iter:2107] Loss: 0.517 | Acc: 74.857% \n",
      "[Epoch:22, Iter:2108] Loss: 0.523 | Acc: 74.750% \n",
      "[Epoch:22, Iter:2109] Loss: 0.520 | Acc: 75.333% \n",
      "[Epoch:22, Iter:2110] Loss: 0.553 | Acc: 72.600% \n",
      "[Epoch:22, Iter:2111] Loss: 0.554 | Acc: 71.818% \n",
      "[Epoch:22, Iter:2112] Loss: 0.551 | Acc: 72.167% \n",
      "[Epoch:22, Iter:2113] Loss: 0.549 | Acc: 71.538% \n",
      "[Epoch:22, Iter:2114] Loss: 0.549 | Acc: 71.857% \n",
      "[Epoch:22, Iter:2115] Loss: 0.549 | Acc: 71.600% \n",
      "[Epoch:22, Iter:2116] Loss: 0.546 | Acc: 71.875% \n",
      "[Epoch:22, Iter:2117] Loss: 0.546 | Acc: 72.235% \n",
      "[Epoch:22, Iter:2118] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:22, Iter:2119] Loss: 0.547 | Acc: 72.316% \n",
      "[Epoch:22, Iter:2120] Loss: 0.547 | Acc: 72.400% \n",
      "[Epoch:22, Iter:2121] Loss: 0.544 | Acc: 72.762% \n",
      "[Epoch:22, Iter:2122] Loss: 0.540 | Acc: 73.182% \n",
      "[Epoch:22, Iter:2123] Loss: 0.537 | Acc: 73.391% \n",
      "[Epoch:22, Iter:2124] Loss: 0.538 | Acc: 73.167% \n",
      "[Epoch:22, Iter:2125] Loss: 0.538 | Acc: 73.280% \n",
      "[Epoch:22, Iter:2126] Loss: 0.538 | Acc: 73.231% \n",
      "[Epoch:22, Iter:2127] Loss: 0.539 | Acc: 73.259% \n",
      "[Epoch:22, Iter:2128] Loss: 0.541 | Acc: 73.286% \n",
      "[Epoch:22, Iter:2129] Loss: 0.538 | Acc: 73.448% \n",
      "[Epoch:22, Iter:2130] Loss: 0.538 | Acc: 73.467% \n",
      "[Epoch:22, Iter:2131] Loss: 0.537 | Acc: 73.355% \n",
      "[Epoch:22, Iter:2132] Loss: 0.535 | Acc: 73.562% \n",
      "[Epoch:22, Iter:2133] Loss: 0.540 | Acc: 73.212% \n",
      "[Epoch:22, Iter:2134] Loss: 0.539 | Acc: 73.353% \n",
      "[Epoch:22, Iter:2135] Loss: 0.541 | Acc: 73.200% \n",
      "[Epoch:22, Iter:2136] Loss: 0.541 | Acc: 73.278% \n",
      "[Epoch:22, Iter:2137] Loss: 0.543 | Acc: 73.027% \n",
      "[Epoch:22, Iter:2138] Loss: 0.544 | Acc: 72.947% \n",
      "[Epoch:22, Iter:2139] Loss: 0.545 | Acc: 72.872% \n",
      "[Epoch:22, Iter:2140] Loss: 0.545 | Acc: 72.800% \n",
      "[Epoch:22, Iter:2141] Loss: 0.547 | Acc: 72.537% \n",
      "[Epoch:22, Iter:2142] Loss: 0.548 | Acc: 72.524% \n",
      "[Epoch:22, Iter:2143] Loss: 0.547 | Acc: 72.651% \n",
      "[Epoch:22, Iter:2144] Loss: 0.547 | Acc: 72.682% \n",
      "[Epoch:22, Iter:2145] Loss: 0.549 | Acc: 72.667% \n",
      "[Epoch:22, Iter:2146] Loss: 0.548 | Acc: 72.913% \n",
      "[Epoch:22, Iter:2147] Loss: 0.547 | Acc: 72.936% \n",
      "[Epoch:22, Iter:2148] Loss: 0.547 | Acc: 72.917% \n",
      "[Epoch:22, Iter:2149] Loss: 0.547 | Acc: 72.735% \n",
      "[Epoch:22, Iter:2150] Loss: 0.546 | Acc: 72.840% \n",
      "[Epoch:22, Iter:2151] Loss: 0.545 | Acc: 72.941% \n",
      "[Epoch:22, Iter:2152] Loss: 0.545 | Acc: 72.923% \n",
      "[Epoch:22, Iter:2153] Loss: 0.546 | Acc: 72.792% \n",
      "[Epoch:22, Iter:2154] Loss: 0.545 | Acc: 72.815% \n",
      "[Epoch:22, Iter:2155] Loss: 0.544 | Acc: 72.764% \n",
      "[Epoch:22, Iter:2156] Loss: 0.545 | Acc: 72.679% \n",
      "[Epoch:22, Iter:2157] Loss: 0.544 | Acc: 72.702% \n",
      "[Epoch:22, Iter:2158] Loss: 0.546 | Acc: 72.483% \n",
      "[Epoch:22, Iter:2159] Loss: 0.546 | Acc: 72.576% \n",
      "[Epoch:22, Iter:2160] Loss: 0.545 | Acc: 72.633% \n",
      "[Epoch:22, Iter:2161] Loss: 0.542 | Acc: 72.754% \n",
      "[Epoch:22, Iter:2162] Loss: 0.541 | Acc: 72.903% \n",
      "[Epoch:22, Iter:2163] Loss: 0.540 | Acc: 72.984% \n",
      "[Epoch:22, Iter:2164] Loss: 0.543 | Acc: 72.812% \n",
      "[Epoch:22, Iter:2165] Loss: 0.544 | Acc: 72.708% \n",
      "[Epoch:22, Iter:2166] Loss: 0.544 | Acc: 72.848% \n",
      "[Epoch:22, Iter:2167] Loss: 0.546 | Acc: 72.866% \n",
      "[Epoch:22, Iter:2168] Loss: 0.546 | Acc: 72.824% \n",
      "[Epoch:22, Iter:2169] Loss: 0.545 | Acc: 72.928% \n",
      "[Epoch:22, Iter:2170] Loss: 0.544 | Acc: 72.971% \n",
      "[Epoch:22, Iter:2171] Loss: 0.546 | Acc: 72.789% \n",
      "[Epoch:22, Iter:2172] Loss: 0.546 | Acc: 72.806% \n",
      "[Epoch:22, Iter:2173] Loss: 0.545 | Acc: 72.822% \n",
      "[Epoch:22, Iter:2174] Loss: 0.545 | Acc: 72.919% \n",
      "[Epoch:22, Iter:2175] Loss: 0.544 | Acc: 72.987% \n",
      "[Epoch:22, Iter:2176] Loss: 0.546 | Acc: 72.842% \n",
      "[Epoch:22, Iter:2177] Loss: 0.546 | Acc: 72.779% \n",
      "[Epoch:22, Iter:2178] Loss: 0.547 | Acc: 72.769% \n",
      "[Epoch:22, Iter:2179] Loss: 0.548 | Acc: 72.709% \n",
      "[Epoch:22, Iter:2180] Loss: 0.548 | Acc: 72.775% \n",
      "[Epoch:22, Iter:2181] Loss: 0.548 | Acc: 72.691% \n",
      "[Epoch:22, Iter:2182] Loss: 0.549 | Acc: 72.634% \n",
      "[Epoch:22, Iter:2183] Loss: 0.550 | Acc: 72.458% \n",
      "[Epoch:22, Iter:2184] Loss: 0.549 | Acc: 72.571% \n",
      "[Epoch:22, Iter:2185] Loss: 0.549 | Acc: 72.471% \n",
      "[Epoch:22, Iter:2186] Loss: 0.547 | Acc: 72.558% \n",
      "[Epoch:22, Iter:2187] Loss: 0.549 | Acc: 72.506% \n",
      "[Epoch:22, Iter:2188] Loss: 0.549 | Acc: 72.500% \n",
      "[Epoch:22, Iter:2189] Loss: 0.548 | Acc: 72.449% \n",
      "[Epoch:22, Iter:2190] Loss: 0.549 | Acc: 72.422% \n",
      "[Epoch:22, Iter:2191] Loss: 0.549 | Acc: 72.352% \n",
      "[Epoch:22, Iter:2192] Loss: 0.548 | Acc: 72.391% \n",
      "[Epoch:22, Iter:2193] Loss: 0.548 | Acc: 72.344% \n",
      "[Epoch:22, Iter:2194] Loss: 0.547 | Acc: 72.404% \n",
      "[Epoch:22, Iter:2195] Loss: 0.547 | Acc: 72.400% \n",
      "[Epoch:22, Iter:2196] Loss: 0.546 | Acc: 72.458% \n",
      "[Epoch:22, Iter:2197] Loss: 0.547 | Acc: 72.412% \n",
      "[Epoch:22, Iter:2198] Loss: 0.547 | Acc: 72.388% \n",
      "[Epoch:22, Iter:2199] Loss: 0.547 | Acc: 72.465% \n",
      "[Epoch:22, Iter:2200] Loss: 0.547 | Acc: 72.420% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.800%\n",
      "Training set's accuracy (after quantization) is: 72.580%\n",
      "Test set's accuracy (before quantization) is: 72.600%\n",
      "Test set's accuracy (after quantization) is: 72.100%\n",
      "Train Loss: 0.541 | Train Acc: 72.800% | Test Loss: 0.544 | Test Acc: 72.600% \n",
      "Quantized Train Loss: 0.546 | Quantized Train Acc: 72.580% | Quantized Test Loss: 0.548 | Quantized Test Acc: 72.100% \n",
      "\n",
      "Epoch: 23\n",
      "[Epoch:23, Iter:2201] Loss: 0.538 | Acc: 74.000% \n",
      "[Epoch:23, Iter:2202] Loss: 0.514 | Acc: 76.000% \n",
      "[Epoch:23, Iter:2203] Loss: 0.567 | Acc: 74.000% \n",
      "[Epoch:23, Iter:2204] Loss: 0.557 | Acc: 75.500% \n",
      "[Epoch:23, Iter:2205] Loss: 0.550 | Acc: 76.400% \n",
      "[Epoch:23, Iter:2206] Loss: 0.542 | Acc: 76.667% \n",
      "[Epoch:23, Iter:2207] Loss: 0.559 | Acc: 75.143% \n",
      "[Epoch:23, Iter:2208] Loss: 0.566 | Acc: 75.250% \n",
      "[Epoch:23, Iter:2209] Loss: 0.561 | Acc: 75.333% \n",
      "[Epoch:23, Iter:2210] Loss: 0.549 | Acc: 76.200% \n",
      "[Epoch:23, Iter:2211] Loss: 0.553 | Acc: 75.818% \n",
      "[Epoch:23, Iter:2212] Loss: 0.552 | Acc: 75.833% \n",
      "[Epoch:23, Iter:2213] Loss: 0.550 | Acc: 75.692% \n",
      "[Epoch:23, Iter:2214] Loss: 0.544 | Acc: 76.000% \n",
      "[Epoch:23, Iter:2215] Loss: 0.537 | Acc: 76.133% \n",
      "[Epoch:23, Iter:2216] Loss: 0.534 | Acc: 76.000% \n",
      "[Epoch:23, Iter:2217] Loss: 0.530 | Acc: 75.647% \n",
      "[Epoch:23, Iter:2218] Loss: 0.533 | Acc: 74.778% \n",
      "[Epoch:23, Iter:2219] Loss: 0.536 | Acc: 74.632% \n",
      "[Epoch:23, Iter:2220] Loss: 0.539 | Acc: 74.400% \n",
      "[Epoch:23, Iter:2221] Loss: 0.543 | Acc: 74.190% \n",
      "[Epoch:23, Iter:2222] Loss: 0.540 | Acc: 74.364% \n",
      "[Epoch:23, Iter:2223] Loss: 0.538 | Acc: 74.522% \n",
      "[Epoch:23, Iter:2224] Loss: 0.536 | Acc: 74.417% \n",
      "[Epoch:23, Iter:2225] Loss: 0.537 | Acc: 74.160% \n",
      "[Epoch:23, Iter:2226] Loss: 0.539 | Acc: 74.154% \n",
      "[Epoch:23, Iter:2227] Loss: 0.541 | Acc: 73.926% \n",
      "[Epoch:23, Iter:2228] Loss: 0.535 | Acc: 74.357% \n",
      "[Epoch:23, Iter:2229] Loss: 0.538 | Acc: 74.138% \n",
      "[Epoch:23, Iter:2230] Loss: 0.536 | Acc: 74.200% \n",
      "[Epoch:23, Iter:2231] Loss: 0.535 | Acc: 74.194% \n",
      "[Epoch:23, Iter:2232] Loss: 0.533 | Acc: 74.312% \n",
      "[Epoch:23, Iter:2233] Loss: 0.535 | Acc: 74.121% \n",
      "[Epoch:23, Iter:2234] Loss: 0.534 | Acc: 74.235% \n",
      "[Epoch:23, Iter:2235] Loss: 0.534 | Acc: 74.057% \n",
      "[Epoch:23, Iter:2236] Loss: 0.534 | Acc: 74.222% \n",
      "[Epoch:23, Iter:2237] Loss: 0.532 | Acc: 74.378% \n",
      "[Epoch:23, Iter:2238] Loss: 0.529 | Acc: 74.579% \n",
      "[Epoch:23, Iter:2239] Loss: 0.532 | Acc: 74.410% \n",
      "[Epoch:23, Iter:2240] Loss: 0.534 | Acc: 74.250% \n",
      "[Epoch:23, Iter:2241] Loss: 0.534 | Acc: 74.049% \n",
      "[Epoch:23, Iter:2242] Loss: 0.536 | Acc: 73.619% \n",
      "[Epoch:23, Iter:2243] Loss: 0.539 | Acc: 73.209% \n",
      "[Epoch:23, Iter:2244] Loss: 0.537 | Acc: 73.409% \n",
      "[Epoch:23, Iter:2245] Loss: 0.536 | Acc: 73.511% \n",
      "[Epoch:23, Iter:2246] Loss: 0.535 | Acc: 73.565% \n",
      "[Epoch:23, Iter:2247] Loss: 0.533 | Acc: 73.702% \n",
      "[Epoch:23, Iter:2248] Loss: 0.534 | Acc: 73.667% \n",
      "[Epoch:23, Iter:2249] Loss: 0.534 | Acc: 73.714% \n",
      "[Epoch:23, Iter:2250] Loss: 0.536 | Acc: 73.560% \n",
      "[Epoch:23, Iter:2251] Loss: 0.539 | Acc: 73.412% \n",
      "[Epoch:23, Iter:2252] Loss: 0.538 | Acc: 73.423% \n",
      "[Epoch:23, Iter:2253] Loss: 0.537 | Acc: 73.434% \n",
      "[Epoch:23, Iter:2254] Loss: 0.537 | Acc: 73.407% \n",
      "[Epoch:23, Iter:2255] Loss: 0.538 | Acc: 73.309% \n",
      "[Epoch:23, Iter:2256] Loss: 0.538 | Acc: 73.250% \n",
      "[Epoch:23, Iter:2257] Loss: 0.537 | Acc: 73.263% \n",
      "[Epoch:23, Iter:2258] Loss: 0.537 | Acc: 73.241% \n",
      "[Epoch:23, Iter:2259] Loss: 0.538 | Acc: 73.186% \n",
      "[Epoch:23, Iter:2260] Loss: 0.538 | Acc: 73.233% \n",
      "[Epoch:23, Iter:2261] Loss: 0.538 | Acc: 73.213% \n",
      "[Epoch:23, Iter:2262] Loss: 0.538 | Acc: 73.194% \n",
      "[Epoch:23, Iter:2263] Loss: 0.537 | Acc: 73.238% \n",
      "[Epoch:23, Iter:2264] Loss: 0.538 | Acc: 73.250% \n",
      "[Epoch:23, Iter:2265] Loss: 0.540 | Acc: 73.077% \n",
      "[Epoch:23, Iter:2266] Loss: 0.540 | Acc: 72.939% \n",
      "[Epoch:23, Iter:2267] Loss: 0.540 | Acc: 73.045% \n",
      "[Epoch:23, Iter:2268] Loss: 0.540 | Acc: 73.088% \n",
      "[Epoch:23, Iter:2269] Loss: 0.538 | Acc: 73.159% \n",
      "[Epoch:23, Iter:2270] Loss: 0.541 | Acc: 73.057% \n",
      "[Epoch:23, Iter:2271] Loss: 0.540 | Acc: 73.070% \n",
      "[Epoch:23, Iter:2272] Loss: 0.541 | Acc: 73.083% \n",
      "[Epoch:23, Iter:2273] Loss: 0.542 | Acc: 73.041% \n",
      "[Epoch:23, Iter:2274] Loss: 0.542 | Acc: 73.081% \n",
      "[Epoch:23, Iter:2275] Loss: 0.544 | Acc: 72.987% \n",
      "[Epoch:23, Iter:2276] Loss: 0.544 | Acc: 72.921% \n",
      "[Epoch:23, Iter:2277] Loss: 0.543 | Acc: 72.961% \n",
      "[Epoch:23, Iter:2278] Loss: 0.543 | Acc: 72.949% \n",
      "[Epoch:23, Iter:2279] Loss: 0.544 | Acc: 72.911% \n",
      "[Epoch:23, Iter:2280] Loss: 0.544 | Acc: 72.850% \n",
      "[Epoch:23, Iter:2281] Loss: 0.544 | Acc: 72.914% \n",
      "[Epoch:23, Iter:2282] Loss: 0.543 | Acc: 72.902% \n",
      "[Epoch:23, Iter:2283] Loss: 0.544 | Acc: 72.795% \n",
      "[Epoch:23, Iter:2284] Loss: 0.544 | Acc: 72.810% \n",
      "[Epoch:23, Iter:2285] Loss: 0.543 | Acc: 72.894% \n",
      "[Epoch:23, Iter:2286] Loss: 0.544 | Acc: 72.977% \n",
      "[Epoch:23, Iter:2287] Loss: 0.545 | Acc: 72.874% \n",
      "[Epoch:23, Iter:2288] Loss: 0.545 | Acc: 72.773% \n",
      "[Epoch:23, Iter:2289] Loss: 0.546 | Acc: 72.697% \n",
      "[Epoch:23, Iter:2290] Loss: 0.545 | Acc: 72.800% \n",
      "[Epoch:23, Iter:2291] Loss: 0.546 | Acc: 72.835% \n",
      "[Epoch:23, Iter:2292] Loss: 0.545 | Acc: 72.870% \n",
      "[Epoch:23, Iter:2293] Loss: 0.545 | Acc: 72.925% \n",
      "[Epoch:23, Iter:2294] Loss: 0.544 | Acc: 72.957% \n",
      "[Epoch:23, Iter:2295] Loss: 0.546 | Acc: 72.884% \n",
      "[Epoch:23, Iter:2296] Loss: 0.545 | Acc: 72.938% \n",
      "[Epoch:23, Iter:2297] Loss: 0.545 | Acc: 72.948% \n",
      "[Epoch:23, Iter:2298] Loss: 0.545 | Acc: 72.918% \n",
      "[Epoch:23, Iter:2299] Loss: 0.545 | Acc: 73.010% \n",
      "[Epoch:23, Iter:2300] Loss: 0.545 | Acc: 72.960% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.140%\n",
      "Training set's accuracy (after quantization) is: 72.600%\n",
      "Test set's accuracy (before quantization) is: 72.900%\n",
      "Test set's accuracy (after quantization) is: 73.300%\n",
      "Train Loss: 0.550 | Train Acc: 72.140% | Test Loss: 0.553 | Test Acc: 72.900% \n",
      "Quantized Train Loss: 0.542 | Quantized Train Acc: 72.600% | Quantized Test Loss: 0.544 | Quantized Test Acc: 73.300% \n",
      "\n",
      "Epoch: 24\n",
      "[Epoch:24, Iter:2301] Loss: 0.594 | Acc: 64.000% \n",
      "[Epoch:24, Iter:2302] Loss: 0.505 | Acc: 73.000% \n",
      "[Epoch:24, Iter:2303] Loss: 0.494 | Acc: 75.333% \n",
      "[Epoch:24, Iter:2304] Loss: 0.496 | Acc: 75.000% \n",
      "[Epoch:24, Iter:2305] Loss: 0.481 | Acc: 75.200% \n",
      "[Epoch:24, Iter:2306] Loss: 0.476 | Acc: 75.667% \n",
      "[Epoch:24, Iter:2307] Loss: 0.492 | Acc: 75.429% \n",
      "[Epoch:24, Iter:2308] Loss: 0.491 | Acc: 75.500% \n",
      "[Epoch:24, Iter:2309] Loss: 0.506 | Acc: 74.222% \n",
      "[Epoch:24, Iter:2310] Loss: 0.505 | Acc: 75.000% \n",
      "[Epoch:24, Iter:2311] Loss: 0.509 | Acc: 74.000% \n",
      "[Epoch:24, Iter:2312] Loss: 0.518 | Acc: 72.833% \n",
      "[Epoch:24, Iter:2313] Loss: 0.528 | Acc: 71.846% \n",
      "[Epoch:24, Iter:2314] Loss: 0.529 | Acc: 71.571% \n",
      "[Epoch:24, Iter:2315] Loss: 0.527 | Acc: 71.867% \n",
      "[Epoch:24, Iter:2316] Loss: 0.538 | Acc: 71.375% \n",
      "[Epoch:24, Iter:2317] Loss: 0.544 | Acc: 71.176% \n",
      "[Epoch:24, Iter:2318] Loss: 0.541 | Acc: 71.333% \n",
      "[Epoch:24, Iter:2319] Loss: 0.541 | Acc: 71.579% \n",
      "[Epoch:24, Iter:2320] Loss: 0.537 | Acc: 71.900% \n",
      "[Epoch:24, Iter:2321] Loss: 0.535 | Acc: 72.286% \n",
      "[Epoch:24, Iter:2322] Loss: 0.535 | Acc: 72.545% \n",
      "[Epoch:24, Iter:2323] Loss: 0.537 | Acc: 72.609% \n",
      "[Epoch:24, Iter:2324] Loss: 0.536 | Acc: 72.917% \n",
      "[Epoch:24, Iter:2325] Loss: 0.532 | Acc: 73.200% \n",
      "[Epoch:24, Iter:2326] Loss: 0.535 | Acc: 73.077% \n",
      "[Epoch:24, Iter:2327] Loss: 0.535 | Acc: 72.889% \n",
      "[Epoch:24, Iter:2328] Loss: 0.534 | Acc: 72.929% \n",
      "[Epoch:24, Iter:2329] Loss: 0.533 | Acc: 72.828% \n",
      "[Epoch:24, Iter:2330] Loss: 0.534 | Acc: 73.000% \n",
      "[Epoch:24, Iter:2331] Loss: 0.538 | Acc: 72.710% \n",
      "[Epoch:24, Iter:2332] Loss: 0.542 | Acc: 72.188% \n",
      "[Epoch:24, Iter:2333] Loss: 0.543 | Acc: 72.121% \n",
      "[Epoch:24, Iter:2334] Loss: 0.541 | Acc: 72.294% \n",
      "[Epoch:24, Iter:2335] Loss: 0.541 | Acc: 72.171% \n",
      "[Epoch:24, Iter:2336] Loss: 0.539 | Acc: 72.333% \n",
      "[Epoch:24, Iter:2337] Loss: 0.543 | Acc: 72.216% \n",
      "[Epoch:24, Iter:2338] Loss: 0.543 | Acc: 72.158% \n",
      "[Epoch:24, Iter:2339] Loss: 0.542 | Acc: 72.256% \n",
      "[Epoch:24, Iter:2340] Loss: 0.543 | Acc: 72.150% \n",
      "[Epoch:24, Iter:2341] Loss: 0.539 | Acc: 72.488% \n",
      "[Epoch:24, Iter:2342] Loss: 0.541 | Acc: 72.333% \n",
      "[Epoch:24, Iter:2343] Loss: 0.543 | Acc: 72.186% \n",
      "[Epoch:24, Iter:2344] Loss: 0.544 | Acc: 72.136% \n",
      "[Epoch:24, Iter:2345] Loss: 0.541 | Acc: 72.311% \n",
      "[Epoch:24, Iter:2346] Loss: 0.547 | Acc: 72.217% \n",
      "[Epoch:24, Iter:2347] Loss: 0.546 | Acc: 72.298% \n",
      "[Epoch:24, Iter:2348] Loss: 0.548 | Acc: 72.083% \n",
      "[Epoch:24, Iter:2349] Loss: 0.547 | Acc: 72.286% \n",
      "[Epoch:24, Iter:2350] Loss: 0.547 | Acc: 72.200% \n",
      "[Epoch:24, Iter:2351] Loss: 0.546 | Acc: 72.196% \n",
      "[Epoch:24, Iter:2352] Loss: 0.547 | Acc: 72.077% \n",
      "[Epoch:24, Iter:2353] Loss: 0.546 | Acc: 72.189% \n",
      "[Epoch:24, Iter:2354] Loss: 0.545 | Acc: 72.222% \n",
      "[Epoch:24, Iter:2355] Loss: 0.545 | Acc: 72.218% \n",
      "[Epoch:24, Iter:2356] Loss: 0.547 | Acc: 72.071% \n",
      "[Epoch:24, Iter:2357] Loss: 0.547 | Acc: 72.175% \n",
      "[Epoch:24, Iter:2358] Loss: 0.548 | Acc: 72.172% \n",
      "[Epoch:24, Iter:2359] Loss: 0.547 | Acc: 72.407% \n",
      "[Epoch:24, Iter:2360] Loss: 0.547 | Acc: 72.367% \n",
      "[Epoch:24, Iter:2361] Loss: 0.549 | Acc: 72.295% \n",
      "[Epoch:24, Iter:2362] Loss: 0.549 | Acc: 72.419% \n",
      "[Epoch:24, Iter:2363] Loss: 0.548 | Acc: 72.413% \n",
      "[Epoch:24, Iter:2364] Loss: 0.549 | Acc: 72.375% \n",
      "[Epoch:24, Iter:2365] Loss: 0.548 | Acc: 72.431% \n",
      "[Epoch:24, Iter:2366] Loss: 0.548 | Acc: 72.485% \n",
      "[Epoch:24, Iter:2367] Loss: 0.548 | Acc: 72.358% \n",
      "[Epoch:24, Iter:2368] Loss: 0.548 | Acc: 72.382% \n",
      "[Epoch:24, Iter:2369] Loss: 0.548 | Acc: 72.348% \n",
      "[Epoch:24, Iter:2370] Loss: 0.548 | Acc: 72.400% \n",
      "[Epoch:24, Iter:2371] Loss: 0.546 | Acc: 72.451% \n",
      "[Epoch:24, Iter:2372] Loss: 0.547 | Acc: 72.333% \n",
      "[Epoch:24, Iter:2373] Loss: 0.547 | Acc: 72.301% \n",
      "[Epoch:24, Iter:2374] Loss: 0.547 | Acc: 72.351% \n",
      "[Epoch:24, Iter:2375] Loss: 0.547 | Acc: 72.320% \n",
      "[Epoch:24, Iter:2376] Loss: 0.547 | Acc: 72.395% \n",
      "[Epoch:24, Iter:2377] Loss: 0.546 | Acc: 72.468% \n",
      "[Epoch:24, Iter:2378] Loss: 0.546 | Acc: 72.513% \n",
      "[Epoch:24, Iter:2379] Loss: 0.546 | Acc: 72.481% \n",
      "[Epoch:24, Iter:2380] Loss: 0.546 | Acc: 72.450% \n",
      "[Epoch:24, Iter:2381] Loss: 0.546 | Acc: 72.469% \n",
      "[Epoch:24, Iter:2382] Loss: 0.547 | Acc: 72.390% \n",
      "[Epoch:24, Iter:2383] Loss: 0.546 | Acc: 72.506% \n",
      "[Epoch:24, Iter:2384] Loss: 0.546 | Acc: 72.571% \n",
      "[Epoch:24, Iter:2385] Loss: 0.546 | Acc: 72.565% \n",
      "[Epoch:24, Iter:2386] Loss: 0.545 | Acc: 72.674% \n",
      "[Epoch:24, Iter:2387] Loss: 0.545 | Acc: 72.667% \n",
      "[Epoch:24, Iter:2388] Loss: 0.546 | Acc: 72.568% \n",
      "[Epoch:24, Iter:2389] Loss: 0.546 | Acc: 72.517% \n",
      "[Epoch:24, Iter:2390] Loss: 0.547 | Acc: 72.489% \n",
      "[Epoch:24, Iter:2391] Loss: 0.548 | Acc: 72.462% \n",
      "[Epoch:24, Iter:2392] Loss: 0.547 | Acc: 72.478% \n",
      "[Epoch:24, Iter:2393] Loss: 0.547 | Acc: 72.495% \n",
      "[Epoch:24, Iter:2394] Loss: 0.547 | Acc: 72.532% \n",
      "[Epoch:24, Iter:2395] Loss: 0.546 | Acc: 72.611% \n",
      "[Epoch:24, Iter:2396] Loss: 0.546 | Acc: 72.562% \n",
      "[Epoch:24, Iter:2397] Loss: 0.545 | Acc: 72.598% \n",
      "[Epoch:24, Iter:2398] Loss: 0.545 | Acc: 72.673% \n",
      "[Epoch:24, Iter:2399] Loss: 0.545 | Acc: 72.646% \n",
      "[Epoch:24, Iter:2400] Loss: 0.546 | Acc: 72.560% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.580%\n",
      "Training set's accuracy (after quantization) is: 72.580%\n",
      "Test set's accuracy (before quantization) is: 72.700%\n",
      "Test set's accuracy (after quantization) is: 72.200%\n",
      "Train Loss: 0.541 | Train Acc: 72.580% | Test Loss: 0.545 | Test Acc: 72.700% \n",
      "Quantized Train Loss: 0.546 | Quantized Train Acc: 72.580% | Quantized Test Loss: 0.548 | Quantized Test Acc: 72.200% \n",
      "\n",
      "Epoch: 25\n",
      "[Epoch:25, Iter:2401] Loss: 0.646 | Acc: 64.000% \n",
      "[Epoch:25, Iter:2402] Loss: 0.622 | Acc: 63.000% \n",
      "[Epoch:25, Iter:2403] Loss: 0.585 | Acc: 66.667% \n",
      "[Epoch:25, Iter:2404] Loss: 0.557 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2405] Loss: 0.556 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2406] Loss: 0.543 | Acc: 71.000% \n",
      "[Epoch:25, Iter:2407] Loss: 0.546 | Acc: 71.143% \n",
      "[Epoch:25, Iter:2408] Loss: 0.564 | Acc: 69.750% \n",
      "[Epoch:25, Iter:2409] Loss: 0.562 | Acc: 69.778% \n",
      "[Epoch:25, Iter:2410] Loss: 0.569 | Acc: 69.000% \n",
      "[Epoch:25, Iter:2411] Loss: 0.568 | Acc: 69.455% \n",
      "[Epoch:25, Iter:2412] Loss: 0.563 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2413] Loss: 0.567 | Acc: 69.692% \n",
      "[Epoch:25, Iter:2414] Loss: 0.566 | Acc: 69.286% \n",
      "[Epoch:25, Iter:2415] Loss: 0.563 | Acc: 69.600% \n",
      "[Epoch:25, Iter:2416] Loss: 0.568 | Acc: 69.750% \n",
      "[Epoch:25, Iter:2417] Loss: 0.567 | Acc: 69.412% \n",
      "[Epoch:25, Iter:2418] Loss: 0.567 | Acc: 69.333% \n",
      "[Epoch:25, Iter:2419] Loss: 0.568 | Acc: 69.158% \n",
      "[Epoch:25, Iter:2420] Loss: 0.568 | Acc: 69.500% \n",
      "[Epoch:25, Iter:2421] Loss: 0.564 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2422] Loss: 0.562 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2423] Loss: 0.560 | Acc: 69.913% \n",
      "[Epoch:25, Iter:2424] Loss: 0.560 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2425] Loss: 0.560 | Acc: 70.240% \n",
      "[Epoch:25, Iter:2426] Loss: 0.563 | Acc: 69.923% \n",
      "[Epoch:25, Iter:2427] Loss: 0.562 | Acc: 70.148% \n",
      "[Epoch:25, Iter:2428] Loss: 0.561 | Acc: 70.143% \n",
      "[Epoch:25, Iter:2429] Loss: 0.564 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2430] Loss: 0.562 | Acc: 70.067% \n",
      "[Epoch:25, Iter:2431] Loss: 0.560 | Acc: 70.194% \n",
      "[Epoch:25, Iter:2432] Loss: 0.560 | Acc: 70.250% \n",
      "[Epoch:25, Iter:2433] Loss: 0.559 | Acc: 70.424% \n",
      "[Epoch:25, Iter:2434] Loss: 0.558 | Acc: 70.471% \n",
      "[Epoch:25, Iter:2435] Loss: 0.561 | Acc: 70.343% \n",
      "[Epoch:25, Iter:2436] Loss: 0.558 | Acc: 70.500% \n",
      "[Epoch:25, Iter:2437] Loss: 0.559 | Acc: 70.541% \n",
      "[Epoch:25, Iter:2438] Loss: 0.556 | Acc: 70.895% \n",
      "[Epoch:25, Iter:2439] Loss: 0.557 | Acc: 70.821% \n",
      "[Epoch:25, Iter:2440] Loss: 0.556 | Acc: 70.750% \n",
      "[Epoch:25, Iter:2441] Loss: 0.557 | Acc: 70.927% \n",
      "[Epoch:25, Iter:2442] Loss: 0.558 | Acc: 70.857% \n",
      "[Epoch:25, Iter:2443] Loss: 0.558 | Acc: 70.930% \n",
      "[Epoch:25, Iter:2444] Loss: 0.558 | Acc: 70.818% \n",
      "[Epoch:25, Iter:2445] Loss: 0.557 | Acc: 70.933% \n",
      "[Epoch:25, Iter:2446] Loss: 0.557 | Acc: 70.957% \n",
      "[Epoch:25, Iter:2447] Loss: 0.558 | Acc: 70.851% \n",
      "[Epoch:25, Iter:2448] Loss: 0.556 | Acc: 71.125% \n",
      "[Epoch:25, Iter:2449] Loss: 0.555 | Acc: 71.102% \n",
      "[Epoch:25, Iter:2450] Loss: 0.558 | Acc: 70.960% \n",
      "[Epoch:25, Iter:2451] Loss: 0.557 | Acc: 70.980% \n",
      "[Epoch:25, Iter:2452] Loss: 0.557 | Acc: 71.115% \n",
      "[Epoch:25, Iter:2453] Loss: 0.558 | Acc: 71.057% \n",
      "[Epoch:25, Iter:2454] Loss: 0.556 | Acc: 71.222% \n",
      "[Epoch:25, Iter:2455] Loss: 0.559 | Acc: 71.091% \n",
      "[Epoch:25, Iter:2456] Loss: 0.558 | Acc: 71.179% \n",
      "[Epoch:25, Iter:2457] Loss: 0.556 | Acc: 71.298% \n",
      "[Epoch:25, Iter:2458] Loss: 0.555 | Acc: 71.517% \n",
      "[Epoch:25, Iter:2459] Loss: 0.554 | Acc: 71.559% \n",
      "[Epoch:25, Iter:2460] Loss: 0.552 | Acc: 71.733% \n",
      "[Epoch:25, Iter:2461] Loss: 0.551 | Acc: 71.836% \n",
      "[Epoch:25, Iter:2462] Loss: 0.553 | Acc: 71.774% \n",
      "[Epoch:25, Iter:2463] Loss: 0.553 | Acc: 71.778% \n",
      "[Epoch:25, Iter:2464] Loss: 0.553 | Acc: 71.844% \n",
      "[Epoch:25, Iter:2465] Loss: 0.553 | Acc: 71.877% \n",
      "[Epoch:25, Iter:2466] Loss: 0.552 | Acc: 71.879% \n",
      "[Epoch:25, Iter:2467] Loss: 0.552 | Acc: 71.881% \n",
      "[Epoch:25, Iter:2468] Loss: 0.554 | Acc: 71.588% \n",
      "[Epoch:25, Iter:2469] Loss: 0.553 | Acc: 71.681% \n",
      "[Epoch:25, Iter:2470] Loss: 0.552 | Acc: 71.829% \n",
      "[Epoch:25, Iter:2471] Loss: 0.552 | Acc: 71.859% \n",
      "[Epoch:25, Iter:2472] Loss: 0.552 | Acc: 71.833% \n",
      "[Epoch:25, Iter:2473] Loss: 0.552 | Acc: 71.945% \n",
      "[Epoch:25, Iter:2474] Loss: 0.551 | Acc: 71.865% \n",
      "[Epoch:25, Iter:2475] Loss: 0.552 | Acc: 71.867% \n",
      "[Epoch:25, Iter:2476] Loss: 0.552 | Acc: 71.868% \n",
      "[Epoch:25, Iter:2477] Loss: 0.552 | Acc: 71.844% \n",
      "[Epoch:25, Iter:2478] Loss: 0.551 | Acc: 71.949% \n",
      "[Epoch:25, Iter:2479] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:25, Iter:2480] Loss: 0.551 | Acc: 71.975% \n",
      "[Epoch:25, Iter:2481] Loss: 0.550 | Acc: 72.099% \n",
      "[Epoch:25, Iter:2482] Loss: 0.549 | Acc: 72.049% \n",
      "[Epoch:25, Iter:2483] Loss: 0.550 | Acc: 72.048% \n",
      "[Epoch:25, Iter:2484] Loss: 0.547 | Acc: 72.238% \n",
      "[Epoch:25, Iter:2485] Loss: 0.549 | Acc: 72.212% \n",
      "[Epoch:25, Iter:2486] Loss: 0.549 | Acc: 72.279% \n",
      "[Epoch:25, Iter:2487] Loss: 0.548 | Acc: 72.322% \n",
      "[Epoch:25, Iter:2488] Loss: 0.547 | Acc: 72.386% \n",
      "[Epoch:25, Iter:2489] Loss: 0.548 | Acc: 72.315% \n",
      "[Epoch:25, Iter:2490] Loss: 0.547 | Acc: 72.333% \n",
      "[Epoch:25, Iter:2491] Loss: 0.548 | Acc: 72.308% \n",
      "[Epoch:25, Iter:2492] Loss: 0.546 | Acc: 72.478% \n",
      "[Epoch:25, Iter:2493] Loss: 0.546 | Acc: 72.538% \n",
      "[Epoch:25, Iter:2494] Loss: 0.545 | Acc: 72.596% \n",
      "[Epoch:25, Iter:2495] Loss: 0.545 | Acc: 72.632% \n",
      "[Epoch:25, Iter:2496] Loss: 0.543 | Acc: 72.688% \n",
      "[Epoch:25, Iter:2497] Loss: 0.544 | Acc: 72.680% \n",
      "[Epoch:25, Iter:2498] Loss: 0.544 | Acc: 72.612% \n",
      "[Epoch:25, Iter:2499] Loss: 0.544 | Acc: 72.626% \n",
      "[Epoch:25, Iter:2500] Loss: 0.545 | Acc: 72.560% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.060%\n",
      "Training set's accuracy (after quantization) is: 72.120%\n",
      "Test set's accuracy (before quantization) is: 71.400%\n",
      "Test set's accuracy (after quantization) is: 71.100%\n",
      "Train Loss: 0.551 | Train Acc: 72.060% | Test Loss: 0.554 | Test Acc: 71.400% \n",
      "Quantized Train Loss: 0.548 | Quantized Train Acc: 72.120% | Quantized Test Loss: 0.551 | Quantized Test Acc: 71.100% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fec7941a6b41ff9bee3f30a14d2aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td></td></tr><tr><td>batch_gradient</td><td></td></tr><tr><td>quantized_accuracy</td><td></td></tr><tr><td>quantized_test_accuracy</td><td></td></tr><tr><td>quantized_test_loss</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>weight_distance</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>72.06</td></tr><tr><td>batch_gradient</td><td>0.10386</td></tr><tr><td>quantized_accuracy</td><td>72.12</td></tr><tr><td>quantized_test_accuracy</td><td>71.10001</td></tr><tr><td>quantized_test_loss</td><td>0.55075</td></tr><tr><td>test_accuracy</td><td>71.4</td></tr><tr><td>test_loss</td><td>0.55438</td></tr><tr><td>weight_distance</td><td>0.21251</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ASkewSGD</strong> at: <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/sa3gxbth' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/sa3gxbth</a><br/> View project at: <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250711_035346-sa3gxbth\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ASkewSGD\n",
    "net, optimizer = init(project_name=\"LogisticRegression_binary\", opt_name=\"ASkewSGD\", batch_size=BATCH_SIZE, architecture=\"MLP\", dataset_name=\"LogisticRegression\", lr=LR)\n",
    "\n",
    "lr_decay_epochs = [30]\n",
    "for decay_epoch in lr_decay_epochs:\n",
    "    if pre_epoch > decay_epoch:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] *= 0.5\n",
    "\n",
    "# Train\n",
    "for epoch in range(pre_epoch, EPOCH):\n",
    "    print(\"\\nEpoch: %d\" % (epoch + 1))\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    if epoch in lr_decay_epochs:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] *= 0.5\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        if epoch < ANNEAL_EPOCH_AS:\n",
    "            epsilon=1\n",
    "        else:\n",
    "            epsilon =  DECAY_CONST ** ((epoch - ANNEAL_EPOCH_AS)+(i/len(trainloader)))\n",
    "        # prepare dataset\n",
    "        length = len(trainloader)\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # forward & backward\n",
    "        outputs = net(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            if param_group[\"tag\"] == \"weights\":\n",
    "                for idx, p in enumerate(param_group[\"params\"]):\n",
    "                    constr = epsilon - (p.data**2 - 1) ** 2\n",
    "                    Kx = -4 * (p.data**2 - 1) * p.data\n",
    "                    direct_grad = torch.logical_or(\n",
    "                        torch.logical_or(constr >= 0, Kx == 0),\n",
    "                        torch.logical_and(\n",
    "                            constr < 0, (-Kx * p.grad.data) >= -alpha * constr\n",
    "                        ),\n",
    "                    )\n",
    "                    p.grad.data[direct_grad] = p.grad.data[direct_grad]\n",
    "                    p.grad.data[~direct_grad] = (torch.clip(\n",
    "                        alpha * constr / Kx,\n",
    "                        -1,\n",
    "                        1,\n",
    "                    ))[~direct_grad]\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        sum_loss += loss.item()\n",
    "        predicted = torch.where(outputs.data>0.5, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        print(\n",
    "            \"[Epoch:%d, Iter:%d] Loss: %.03f | Acc: %.3f%% \"\n",
    "            % (\n",
    "                epoch + 1,\n",
    "                (i + 1 + (epoch) * length),\n",
    "                sum_loss / (i + 1),\n",
    "                100.0 * correct / total,\n",
    "            )\n",
    "        )\n",
    "    print(\"Waiting Test...\")\n",
    "    # calculate weight_dist and batch_gradient\n",
    "    model_copy=copy.deepcopy(net)\n",
    "    weight_dist=torch.norm(w_star-model_copy.fc1.weight.cpu()).item()\n",
    "    model_copy.to(device)\n",
    "    model_copy.train()\n",
    "    weights = [p for name, p in net.named_parameters() if 'bias' not in name]\n",
    "    bias = [p for name, p in net.named_parameters() if 'bias' in name]\n",
    "    parameters = [{\"params\": weights, \"tag\": \"weights\"}, {\"params\": bias, \"tag\": \"bias\"}]\n",
    "    optimizerx = optim.SGD(parameters, lr=LR)\n",
    "    grad=torch.zeros(model_copy.fc1.weight.shape, device=device)\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_copy(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        grad += model_copy.fc1.weight.grad\n",
    "        optimizerx.step()\n",
    "        model_copy.zero_grad()\n",
    "    grad = grad / len(trainloader)\n",
    "    batch_gradient = torch.norm(grad).item()\n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_acc = net.evaluate(trainloader, criterion, device, eval=False, qt=False)\n",
    "        qtrain_loss, qtrain_acc = net.evaluate(trainloader, criterion, device, eval=False, qt=True)\n",
    "        test_loss, test_acc=net.evaluate(testloader, criterion, device, eval=True, qt=False)\n",
    "        qtest_loss, qtest_acc=net.evaluate(testloader, criterion, device, eval=True, qt=True)\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"test_loss\": test_loss,\n",
    "                \"quantized_test_loss\": qtest_loss,\n",
    "                \"accuracy\": train_acc,\n",
    "                \"quantized_accuracy\": qtrain_acc,\n",
    "                \"test_accuracy\": test_acc,\n",
    "                \"quantized_test_accuracy\": qtest_acc,\n",
    "                \"weight_distance\": weight_dist,\n",
    "                \"batch_gradient\": batch_gradient,\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            \"Train Loss: %.03f | Train Acc: %.3f%% | Test Loss: %.03f | Test Acc: %.3f%% \"\n",
    "            % (\n",
    "                train_loss,\n",
    "                train_acc,\n",
    "                test_loss,\n",
    "                test_acc,\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Quantized Train Loss: %.03f | Quantized Train Acc: %.3f%% | Quantized Test Loss: %.03f | Quantized Test Acc: %.3f%% \"\n",
    "            % (\n",
    "                qtrain_loss,\n",
    "                qtrain_acc,\n",
    "                qtest_loss,\n",
    "                qtest_acc,\n",
    "            )\n",
    "        )\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.21.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lenovo\\Desktop\\CUHK\\Research-2025\\exp\\wandb\\run-20250711_035428-1509m89r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/1509m89r' target=\"_blank\">Deterministic BinaryConnect</a></strong> to <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/1509m89r' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/1509m89r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "[epoch:1, iter:1] Loss: 0.739 | Acc: 36.000% \n",
      "[epoch:1, iter:2] Loss: 1.095 | Acc: 53.000% \n",
      "[epoch:1, iter:3] Loss: 1.355 | Acc: 42.000% \n",
      "[epoch:1, iter:4] Loss: 1.455 | Acc: 48.000% \n",
      "[epoch:1, iter:5] Loss: 1.475 | Acc: 52.000% \n",
      "[epoch:1, iter:6] Loss: 1.341 | Acc: 48.667% \n",
      "[epoch:1, iter:7] Loss: 1.232 | Acc: 52.286% \n",
      "[epoch:1, iter:8] Loss: 1.153 | Acc: 49.750% \n",
      "[epoch:1, iter:9] Loss: 1.082 | Acc: 48.889% \n",
      "[epoch:1, iter:10] Loss: 1.070 | Acc: 50.200% \n",
      "[epoch:1, iter:11] Loss: 1.072 | Acc: 49.091% \n",
      "[epoch:1, iter:12] Loss: 1.062 | Acc: 50.667% \n",
      "[epoch:1, iter:13] Loss: 1.028 | Acc: 50.923% \n",
      "[epoch:1, iter:14] Loss: 0.996 | Acc: 52.429% \n",
      "[epoch:1, iter:15] Loss: 0.968 | Acc: 51.200% \n",
      "[epoch:1, iter:16] Loss: 0.954 | Acc: 52.375% \n",
      "[epoch:1, iter:17] Loss: 0.948 | Acc: 50.824% \n",
      "[epoch:1, iter:18] Loss: 0.929 | Acc: 51.889% \n",
      "[epoch:1, iter:19] Loss: 0.913 | Acc: 52.421% \n",
      "[epoch:1, iter:20] Loss: 0.898 | Acc: 51.900% \n",
      "[epoch:1, iter:21] Loss: 0.879 | Acc: 51.048% \n",
      "[epoch:1, iter:22] Loss: 0.866 | Acc: 51.455% \n",
      "[epoch:1, iter:23] Loss: 0.852 | Acc: 52.000% \n",
      "[epoch:1, iter:24] Loss: 0.838 | Acc: 53.000% \n",
      "[epoch:1, iter:25] Loss: 0.829 | Acc: 53.520% \n",
      "[epoch:1, iter:26] Loss: 0.821 | Acc: 53.000% \n",
      "[epoch:1, iter:27] Loss: 0.809 | Acc: 52.074% \n",
      "[epoch:1, iter:28] Loss: 0.801 | Acc: 52.286% \n",
      "[epoch:1, iter:29] Loss: 0.790 | Acc: 53.034% \n",
      "[epoch:1, iter:30] Loss: 0.790 | Acc: 53.400% \n",
      "[epoch:1, iter:31] Loss: 0.784 | Acc: 53.032% \n",
      "[epoch:1, iter:32] Loss: 0.776 | Acc: 52.750% \n",
      "[epoch:1, iter:33] Loss: 0.772 | Acc: 52.303% \n",
      "[epoch:1, iter:34] Loss: 0.767 | Acc: 51.706% \n",
      "[epoch:1, iter:35] Loss: 0.763 | Acc: 51.371% \n",
      "[epoch:1, iter:36] Loss: 0.757 | Acc: 50.889% \n",
      "[epoch:1, iter:37] Loss: 0.755 | Acc: 51.243% \n",
      "[epoch:1, iter:38] Loss: 0.751 | Acc: 51.211% \n",
      "[epoch:1, iter:39] Loss: 0.744 | Acc: 51.641% \n",
      "[epoch:1, iter:40] Loss: 0.739 | Acc: 51.900% \n",
      "[epoch:1, iter:41] Loss: 0.731 | Acc: 52.634% \n",
      "[epoch:1, iter:42] Loss: 0.743 | Acc: 52.762% \n",
      "[epoch:1, iter:43] Loss: 0.738 | Acc: 52.279% \n",
      "[epoch:1, iter:44] Loss: 0.733 | Acc: 52.227% \n",
      "[epoch:1, iter:45] Loss: 0.729 | Acc: 52.578% \n",
      "[epoch:1, iter:46] Loss: 0.729 | Acc: 52.913% \n",
      "[epoch:1, iter:47] Loss: 0.725 | Acc: 52.681% \n",
      "[epoch:1, iter:48] Loss: 0.720 | Acc: 53.208% \n",
      "[epoch:1, iter:49] Loss: 0.718 | Acc: 53.429% \n",
      "[epoch:1, iter:50] Loss: 0.714 | Acc: 53.840% \n",
      "[epoch:1, iter:51] Loss: 0.709 | Acc: 54.275% \n",
      "[epoch:1, iter:52] Loss: 0.706 | Acc: 54.654% \n",
      "[epoch:1, iter:53] Loss: 0.706 | Acc: 54.415% \n",
      "[epoch:1, iter:54] Loss: 0.702 | Acc: 54.185% \n",
      "[epoch:1, iter:55] Loss: 0.699 | Acc: 54.109% \n",
      "[epoch:1, iter:56] Loss: 0.695 | Acc: 54.357% \n",
      "[epoch:1, iter:57] Loss: 0.691 | Acc: 54.842% \n",
      "[epoch:1, iter:58] Loss: 0.699 | Acc: 55.069% \n",
      "[epoch:1, iter:59] Loss: 0.697 | Acc: 55.322% \n",
      "[epoch:1, iter:60] Loss: 0.696 | Acc: 55.467% \n",
      "[epoch:1, iter:61] Loss: 0.692 | Acc: 55.607% \n",
      "[epoch:1, iter:62] Loss: 0.690 | Acc: 55.742% \n",
      "[epoch:1, iter:63] Loss: 0.687 | Acc: 55.937% \n",
      "[epoch:1, iter:64] Loss: 0.684 | Acc: 56.188% \n",
      "[epoch:1, iter:65] Loss: 0.682 | Acc: 56.338% \n",
      "[epoch:1, iter:66] Loss: 0.679 | Acc: 56.606% \n",
      "[epoch:1, iter:67] Loss: 0.675 | Acc: 57.045% \n",
      "[epoch:1, iter:68] Loss: 0.676 | Acc: 57.147% \n",
      "[epoch:1, iter:69] Loss: 0.674 | Acc: 57.449% \n",
      "[epoch:1, iter:70] Loss: 0.672 | Acc: 57.629% \n",
      "[epoch:1, iter:71] Loss: 0.669 | Acc: 57.803% \n",
      "[epoch:1, iter:72] Loss: 0.669 | Acc: 57.944% \n",
      "[epoch:1, iter:73] Loss: 0.671 | Acc: 57.973% \n",
      "[epoch:1, iter:74] Loss: 0.673 | Acc: 57.649% \n",
      "[epoch:1, iter:75] Loss: 0.673 | Acc: 57.813% \n",
      "[epoch:1, iter:76] Loss: 0.671 | Acc: 58.026% \n",
      "[epoch:1, iter:77] Loss: 0.670 | Acc: 58.208% \n",
      "[epoch:1, iter:78] Loss: 0.669 | Acc: 58.410% \n",
      "[epoch:1, iter:79] Loss: 0.670 | Acc: 58.481% \n",
      "[epoch:1, iter:80] Loss: 0.668 | Acc: 58.650% \n",
      "[epoch:1, iter:81] Loss: 0.668 | Acc: 58.765% \n",
      "[epoch:1, iter:82] Loss: 0.666 | Acc: 58.976% \n",
      "[epoch:1, iter:83] Loss: 0.663 | Acc: 59.181% \n",
      "[epoch:1, iter:84] Loss: 0.665 | Acc: 59.238% \n",
      "[epoch:1, iter:85] Loss: 0.664 | Acc: 59.271% \n",
      "[epoch:1, iter:86] Loss: 0.664 | Acc: 59.279% \n",
      "[epoch:1, iter:87] Loss: 0.662 | Acc: 59.379% \n",
      "[epoch:1, iter:88] Loss: 0.661 | Acc: 59.432% \n",
      "[epoch:1, iter:89] Loss: 0.660 | Acc: 59.618% \n",
      "[epoch:1, iter:90] Loss: 0.658 | Acc: 59.667% \n",
      "[epoch:1, iter:91] Loss: 0.655 | Acc: 59.868% \n",
      "[epoch:1, iter:92] Loss: 0.654 | Acc: 59.848% \n",
      "[epoch:1, iter:93] Loss: 0.652 | Acc: 59.914% \n",
      "[epoch:1, iter:94] Loss: 0.650 | Acc: 60.064% \n",
      "[epoch:1, iter:95] Loss: 0.649 | Acc: 60.126% \n",
      "[epoch:1, iter:96] Loss: 0.648 | Acc: 60.125% \n",
      "[epoch:1, iter:97] Loss: 0.646 | Acc: 60.289% \n",
      "[epoch:1, iter:98] Loss: 0.645 | Acc: 60.388% \n",
      "[epoch:1, iter:99] Loss: 0.644 | Acc: 60.424% \n",
      "[epoch:1, iter:100] Loss: 0.642 | Acc: 60.540% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.400%\n",
      "Training set's accuracy (after quantization) is: 72.900%\n",
      "Test set's accuracy (before quantization) is: 69.600%\n",
      "Test set's accuracy (after quantization) is: 73.400%\n",
      "Train Loss: 0.579 | Train Acc: 69.400% | Test Loss: 0.580 | Test Acc: 69.600% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.900% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.400% \n",
      "\n",
      "Epoch: 2\n",
      "[epoch:2, iter:101] Loss: 0.512 | Acc: 72.000% \n",
      "[epoch:2, iter:102] Loss: 0.554 | Acc: 72.000% \n",
      "[epoch:2, iter:103] Loss: 0.563 | Acc: 72.667% \n",
      "[epoch:2, iter:104] Loss: 0.559 | Acc: 70.000% \n",
      "[epoch:2, iter:105] Loss: 0.559 | Acc: 69.200% \n",
      "[epoch:2, iter:106] Loss: 0.559 | Acc: 69.000% \n",
      "[epoch:2, iter:107] Loss: 0.551 | Acc: 68.286% \n",
      "[epoch:2, iter:108] Loss: 0.539 | Acc: 68.750% \n",
      "[epoch:2, iter:109] Loss: 0.532 | Acc: 69.556% \n",
      "[epoch:2, iter:110] Loss: 0.530 | Acc: 70.200% \n",
      "[epoch:2, iter:111] Loss: 0.558 | Acc: 70.000% \n",
      "[epoch:2, iter:112] Loss: 0.556 | Acc: 70.333% \n",
      "[epoch:2, iter:113] Loss: 0.573 | Acc: 70.154% \n",
      "[epoch:2, iter:114] Loss: 0.569 | Acc: 70.000% \n",
      "[epoch:2, iter:115] Loss: 0.564 | Acc: 70.133% \n",
      "[epoch:2, iter:116] Loss: 0.569 | Acc: 69.375% \n",
      "[epoch:2, iter:117] Loss: 0.567 | Acc: 69.647% \n",
      "[epoch:2, iter:118] Loss: 0.560 | Acc: 70.111% \n",
      "[epoch:2, iter:119] Loss: 0.560 | Acc: 70.316% \n",
      "[epoch:2, iter:120] Loss: 0.559 | Acc: 69.900% \n",
      "[epoch:2, iter:121] Loss: 0.557 | Acc: 69.905% \n",
      "[epoch:2, iter:122] Loss: 0.561 | Acc: 69.727% \n",
      "[epoch:2, iter:123] Loss: 0.560 | Acc: 69.652% \n",
      "[epoch:2, iter:124] Loss: 0.555 | Acc: 70.000% \n",
      "[epoch:2, iter:125] Loss: 0.562 | Acc: 69.680% \n",
      "[epoch:2, iter:126] Loss: 0.558 | Acc: 69.923% \n",
      "[epoch:2, iter:127] Loss: 0.560 | Acc: 69.704% \n",
      "[epoch:2, iter:128] Loss: 0.558 | Acc: 69.786% \n",
      "[epoch:2, iter:129] Loss: 0.556 | Acc: 69.655% \n",
      "[epoch:2, iter:130] Loss: 0.557 | Acc: 69.733% \n",
      "[epoch:2, iter:131] Loss: 0.554 | Acc: 69.742% \n",
      "[epoch:2, iter:132] Loss: 0.557 | Acc: 69.500% \n",
      "[epoch:2, iter:133] Loss: 0.558 | Acc: 69.394% \n",
      "[epoch:2, iter:134] Loss: 0.557 | Acc: 69.647% \n",
      "[epoch:2, iter:135] Loss: 0.557 | Acc: 69.429% \n",
      "[epoch:2, iter:136] Loss: 0.556 | Acc: 69.444% \n",
      "[epoch:2, iter:137] Loss: 0.563 | Acc: 69.027% \n",
      "[epoch:2, iter:138] Loss: 0.562 | Acc: 69.158% \n",
      "[epoch:2, iter:139] Loss: 0.561 | Acc: 68.872% \n",
      "[epoch:2, iter:140] Loss: 0.563 | Acc: 68.500% \n",
      "[epoch:2, iter:141] Loss: 0.562 | Acc: 68.439% \n",
      "[epoch:2, iter:142] Loss: 0.559 | Acc: 68.905% \n",
      "[epoch:2, iter:143] Loss: 0.568 | Acc: 68.651% \n",
      "[epoch:2, iter:144] Loss: 0.568 | Acc: 68.545% \n",
      "[epoch:2, iter:145] Loss: 0.569 | Acc: 68.356% \n",
      "[epoch:2, iter:146] Loss: 0.567 | Acc: 68.478% \n",
      "[epoch:2, iter:147] Loss: 0.565 | Acc: 68.723% \n",
      "[epoch:2, iter:148] Loss: 0.565 | Acc: 68.750% \n",
      "[epoch:2, iter:149] Loss: 0.565 | Acc: 68.857% \n",
      "[epoch:2, iter:150] Loss: 0.566 | Acc: 68.920% \n",
      "[epoch:2, iter:151] Loss: 0.565 | Acc: 68.980% \n",
      "[epoch:2, iter:152] Loss: 0.566 | Acc: 69.000% \n",
      "[epoch:2, iter:153] Loss: 0.564 | Acc: 69.057% \n",
      "[epoch:2, iter:154] Loss: 0.563 | Acc: 68.963% \n",
      "[epoch:2, iter:155] Loss: 0.564 | Acc: 69.127% \n",
      "[epoch:2, iter:156] Loss: 0.564 | Acc: 69.143% \n",
      "[epoch:2, iter:157] Loss: 0.571 | Acc: 69.053% \n",
      "[epoch:2, iter:158] Loss: 0.571 | Acc: 69.034% \n",
      "[epoch:2, iter:159] Loss: 0.570 | Acc: 69.220% \n",
      "[epoch:2, iter:160] Loss: 0.570 | Acc: 69.100% \n",
      "[epoch:2, iter:161] Loss: 0.569 | Acc: 69.082% \n",
      "[epoch:2, iter:162] Loss: 0.567 | Acc: 69.032% \n",
      "[epoch:2, iter:163] Loss: 0.567 | Acc: 69.111% \n",
      "[epoch:2, iter:164] Loss: 0.567 | Acc: 69.000% \n",
      "[epoch:2, iter:165] Loss: 0.567 | Acc: 69.015% \n",
      "[epoch:2, iter:166] Loss: 0.566 | Acc: 69.061% \n",
      "[epoch:2, iter:167] Loss: 0.566 | Acc: 69.164% \n",
      "[epoch:2, iter:168] Loss: 0.565 | Acc: 69.235% \n",
      "[epoch:2, iter:169] Loss: 0.566 | Acc: 69.246% \n",
      "[epoch:2, iter:170] Loss: 0.567 | Acc: 69.229% \n",
      "[epoch:2, iter:171] Loss: 0.566 | Acc: 69.408% \n",
      "[epoch:2, iter:172] Loss: 0.566 | Acc: 69.444% \n",
      "[epoch:2, iter:173] Loss: 0.566 | Acc: 69.507% \n",
      "[epoch:2, iter:174] Loss: 0.566 | Acc: 69.541% \n",
      "[epoch:2, iter:175] Loss: 0.567 | Acc: 69.493% \n",
      "[epoch:2, iter:176] Loss: 0.566 | Acc: 69.526% \n",
      "[epoch:2, iter:177] Loss: 0.566 | Acc: 69.506% \n",
      "[epoch:2, iter:178] Loss: 0.566 | Acc: 69.487% \n",
      "[epoch:2, iter:179] Loss: 0.566 | Acc: 69.595% \n",
      "[epoch:2, iter:180] Loss: 0.564 | Acc: 69.750% \n",
      "[epoch:2, iter:181] Loss: 0.566 | Acc: 69.679% \n",
      "[epoch:2, iter:182] Loss: 0.566 | Acc: 69.585% \n",
      "[epoch:2, iter:183] Loss: 0.569 | Acc: 69.639% \n",
      "[epoch:2, iter:184] Loss: 0.568 | Acc: 69.738% \n",
      "[epoch:2, iter:185] Loss: 0.566 | Acc: 69.882% \n",
      "[epoch:2, iter:186] Loss: 0.568 | Acc: 69.814% \n",
      "[epoch:2, iter:187] Loss: 0.568 | Acc: 69.816% \n",
      "[epoch:2, iter:188] Loss: 0.568 | Acc: 69.864% \n",
      "[epoch:2, iter:189] Loss: 0.568 | Acc: 69.753% \n",
      "[epoch:2, iter:190] Loss: 0.568 | Acc: 69.689% \n",
      "[epoch:2, iter:191] Loss: 0.569 | Acc: 69.626% \n",
      "[epoch:2, iter:192] Loss: 0.568 | Acc: 69.696% \n",
      "[epoch:2, iter:193] Loss: 0.567 | Acc: 69.763% \n",
      "[epoch:2, iter:194] Loss: 0.567 | Acc: 69.617% \n",
      "[epoch:2, iter:195] Loss: 0.567 | Acc: 69.579% \n",
      "[epoch:2, iter:196] Loss: 0.568 | Acc: 69.604% \n",
      "[epoch:2, iter:197] Loss: 0.568 | Acc: 69.588% \n",
      "[epoch:2, iter:198] Loss: 0.567 | Acc: 69.633% \n",
      "[epoch:2, iter:199] Loss: 0.565 | Acc: 69.697% \n",
      "[epoch:2, iter:200] Loss: 0.565 | Acc: 69.740% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.200%\n",
      "Training set's accuracy (after quantization) is: 72.900%\n",
      "Test set's accuracy (before quantization) is: 69.500%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.585 | Train Acc: 69.200% | Test Loss: 0.589 | Test Acc: 69.500% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.900% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 3\n",
      "[epoch:3, iter:201] Loss: 0.496 | Acc: 66.000% \n",
      "[epoch:3, iter:202] Loss: 0.498 | Acc: 67.000% \n",
      "[epoch:3, iter:203] Loss: 0.550 | Acc: 66.667% \n",
      "[epoch:3, iter:204] Loss: 0.525 | Acc: 70.000% \n",
      "[epoch:3, iter:205] Loss: 0.527 | Acc: 69.600% \n",
      "[epoch:3, iter:206] Loss: 0.513 | Acc: 70.667% \n",
      "[epoch:3, iter:207] Loss: 0.545 | Acc: 70.286% \n",
      "[epoch:3, iter:208] Loss: 0.552 | Acc: 69.250% \n",
      "[epoch:3, iter:209] Loss: 0.550 | Acc: 68.889% \n",
      "[epoch:3, iter:210] Loss: 0.548 | Acc: 69.400% \n",
      "[epoch:3, iter:211] Loss: 0.544 | Acc: 69.455% \n",
      "[epoch:3, iter:212] Loss: 0.545 | Acc: 69.500% \n",
      "[epoch:3, iter:213] Loss: 0.549 | Acc: 69.692% \n",
      "[epoch:3, iter:214] Loss: 0.545 | Acc: 70.286% \n",
      "[epoch:3, iter:215] Loss: 0.564 | Acc: 69.600% \n",
      "[epoch:3, iter:216] Loss: 0.560 | Acc: 70.000% \n",
      "[epoch:3, iter:217] Loss: 0.563 | Acc: 69.529% \n",
      "[epoch:3, iter:218] Loss: 0.558 | Acc: 69.889% \n",
      "[epoch:3, iter:219] Loss: 0.555 | Acc: 69.895% \n",
      "[epoch:3, iter:220] Loss: 0.552 | Acc: 70.500% \n",
      "[epoch:3, iter:221] Loss: 0.560 | Acc: 70.000% \n",
      "[epoch:3, iter:222] Loss: 0.555 | Acc: 70.455% \n",
      "[epoch:3, iter:223] Loss: 0.552 | Acc: 70.609% \n",
      "[epoch:3, iter:224] Loss: 0.549 | Acc: 70.583% \n",
      "[epoch:3, iter:225] Loss: 0.549 | Acc: 70.240% \n",
      "[epoch:3, iter:226] Loss: 0.550 | Acc: 70.077% \n",
      "[epoch:3, iter:227] Loss: 0.547 | Acc: 70.444% \n",
      "[epoch:3, iter:228] Loss: 0.550 | Acc: 70.357% \n",
      "[epoch:3, iter:229] Loss: 0.556 | Acc: 70.138% \n",
      "[epoch:3, iter:230] Loss: 0.553 | Acc: 70.333% \n",
      "[epoch:3, iter:231] Loss: 0.551 | Acc: 70.516% \n",
      "[epoch:3, iter:232] Loss: 0.552 | Acc: 70.500% \n",
      "[epoch:3, iter:233] Loss: 0.551 | Acc: 70.667% \n",
      "[epoch:3, iter:234] Loss: 0.550 | Acc: 70.765% \n",
      "[epoch:3, iter:235] Loss: 0.551 | Acc: 70.800% \n",
      "[epoch:3, iter:236] Loss: 0.552 | Acc: 70.889% \n",
      "[epoch:3, iter:237] Loss: 0.552 | Acc: 70.757% \n",
      "[epoch:3, iter:238] Loss: 0.550 | Acc: 70.842% \n",
      "[epoch:3, iter:239] Loss: 0.548 | Acc: 71.026% \n",
      "[epoch:3, iter:240] Loss: 0.548 | Acc: 71.150% \n",
      "[epoch:3, iter:241] Loss: 0.549 | Acc: 71.073% \n",
      "[epoch:3, iter:242] Loss: 0.550 | Acc: 70.810% \n",
      "[epoch:3, iter:243] Loss: 0.551 | Acc: 70.930% \n",
      "[epoch:3, iter:244] Loss: 0.551 | Acc: 70.727% \n",
      "[epoch:3, iter:245] Loss: 0.549 | Acc: 70.844% \n",
      "[epoch:3, iter:246] Loss: 0.549 | Acc: 70.870% \n",
      "[epoch:3, iter:247] Loss: 0.549 | Acc: 70.936% \n",
      "[epoch:3, iter:248] Loss: 0.547 | Acc: 70.875% \n",
      "[epoch:3, iter:249] Loss: 0.548 | Acc: 70.735% \n",
      "[epoch:3, iter:250] Loss: 0.548 | Acc: 70.600% \n",
      "[epoch:3, iter:251] Loss: 0.550 | Acc: 70.510% \n",
      "[epoch:3, iter:252] Loss: 0.551 | Acc: 70.462% \n",
      "[epoch:3, iter:253] Loss: 0.553 | Acc: 70.528% \n",
      "[epoch:3, iter:254] Loss: 0.553 | Acc: 70.519% \n",
      "[epoch:3, iter:255] Loss: 0.559 | Acc: 70.255% \n",
      "[epoch:3, iter:256] Loss: 0.562 | Acc: 70.179% \n",
      "[epoch:3, iter:257] Loss: 0.563 | Acc: 69.930% \n",
      "[epoch:3, iter:258] Loss: 0.563 | Acc: 69.862% \n",
      "[epoch:3, iter:259] Loss: 0.563 | Acc: 69.932% \n",
      "[epoch:3, iter:260] Loss: 0.562 | Acc: 69.967% \n",
      "[epoch:3, iter:261] Loss: 0.563 | Acc: 69.869% \n",
      "[epoch:3, iter:262] Loss: 0.562 | Acc: 69.935% \n",
      "[epoch:3, iter:263] Loss: 0.561 | Acc: 70.032% \n",
      "[epoch:3, iter:264] Loss: 0.560 | Acc: 69.969% \n",
      "[epoch:3, iter:265] Loss: 0.560 | Acc: 69.908% \n",
      "[epoch:3, iter:266] Loss: 0.562 | Acc: 69.788% \n",
      "[epoch:3, iter:267] Loss: 0.560 | Acc: 69.881% \n",
      "[epoch:3, iter:268] Loss: 0.560 | Acc: 69.882% \n",
      "[epoch:3, iter:269] Loss: 0.560 | Acc: 69.739% \n",
      "[epoch:3, iter:270] Loss: 0.560 | Acc: 69.686% \n",
      "[epoch:3, iter:271] Loss: 0.559 | Acc: 69.803% \n",
      "[epoch:3, iter:272] Loss: 0.561 | Acc: 69.667% \n",
      "[epoch:3, iter:273] Loss: 0.562 | Acc: 69.562% \n",
      "[epoch:3, iter:274] Loss: 0.562 | Acc: 69.568% \n",
      "[epoch:3, iter:275] Loss: 0.561 | Acc: 69.627% \n",
      "[epoch:3, iter:276] Loss: 0.560 | Acc: 69.553% \n",
      "[epoch:3, iter:277] Loss: 0.560 | Acc: 69.532% \n",
      "[epoch:3, iter:278] Loss: 0.560 | Acc: 69.590% \n",
      "[epoch:3, iter:279] Loss: 0.559 | Acc: 69.620% \n",
      "[epoch:3, iter:280] Loss: 0.558 | Acc: 69.600% \n",
      "[epoch:3, iter:281] Loss: 0.559 | Acc: 69.457% \n",
      "[epoch:3, iter:282] Loss: 0.557 | Acc: 69.561% \n",
      "[epoch:3, iter:283] Loss: 0.557 | Acc: 69.542% \n",
      "[epoch:3, iter:284] Loss: 0.557 | Acc: 69.524% \n",
      "[epoch:3, iter:285] Loss: 0.558 | Acc: 69.529% \n",
      "[epoch:3, iter:286] Loss: 0.558 | Acc: 69.535% \n",
      "[epoch:3, iter:287] Loss: 0.559 | Acc: 69.517% \n",
      "[epoch:3, iter:288] Loss: 0.559 | Acc: 69.500% \n",
      "[epoch:3, iter:289] Loss: 0.559 | Acc: 69.528% \n",
      "[epoch:3, iter:290] Loss: 0.560 | Acc: 69.489% \n",
      "[epoch:3, iter:291] Loss: 0.560 | Acc: 69.473% \n",
      "[epoch:3, iter:292] Loss: 0.560 | Acc: 69.413% \n",
      "[epoch:3, iter:293] Loss: 0.560 | Acc: 69.355% \n",
      "[epoch:3, iter:294] Loss: 0.559 | Acc: 69.404% \n",
      "[epoch:3, iter:295] Loss: 0.559 | Acc: 69.389% \n",
      "[epoch:3, iter:296] Loss: 0.560 | Acc: 69.354% \n",
      "[epoch:3, iter:297] Loss: 0.559 | Acc: 69.423% \n",
      "[epoch:3, iter:298] Loss: 0.557 | Acc: 69.490% \n",
      "[epoch:3, iter:299] Loss: 0.558 | Acc: 69.515% \n",
      "[epoch:3, iter:300] Loss: 0.557 | Acc: 69.500% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.040%\n",
      "Training set's accuracy (after quantization) is: 72.980%\n",
      "Test set's accuracy (before quantization) is: 69.400%\n",
      "Test set's accuracy (after quantization) is: 73.800%\n",
      "Train Loss: 0.643 | Train Acc: 69.040% | Test Loss: 0.644 | Test Acc: 69.400% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.980% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.800% \n",
      "\n",
      "Epoch: 4\n",
      "[epoch:4, iter:301] Loss: 0.536 | Acc: 76.000% \n",
      "[epoch:4, iter:302] Loss: 1.042 | Acc: 68.000% \n",
      "[epoch:4, iter:303] Loss: 0.901 | Acc: 65.333% \n",
      "[epoch:4, iter:304] Loss: 0.819 | Acc: 65.500% \n",
      "[epoch:4, iter:305] Loss: 0.761 | Acc: 67.600% \n",
      "[epoch:4, iter:306] Loss: 0.709 | Acc: 69.667% \n",
      "[epoch:4, iter:307] Loss: 0.690 | Acc: 69.429% \n",
      "[epoch:4, iter:308] Loss: 0.671 | Acc: 69.250% \n",
      "[epoch:4, iter:309] Loss: 0.654 | Acc: 70.000% \n",
      "[epoch:4, iter:310] Loss: 0.642 | Acc: 70.800% \n",
      "[epoch:4, iter:311] Loss: 0.633 | Acc: 69.818% \n",
      "[epoch:4, iter:312] Loss: 0.629 | Acc: 69.333% \n",
      "[epoch:4, iter:313] Loss: 0.627 | Acc: 69.692% \n",
      "[epoch:4, iter:314] Loss: 0.624 | Acc: 69.429% \n",
      "[epoch:4, iter:315] Loss: 0.619 | Acc: 69.467% \n",
      "[epoch:4, iter:316] Loss: 0.617 | Acc: 69.750% \n",
      "[epoch:4, iter:317] Loss: 0.610 | Acc: 69.412% \n",
      "[epoch:4, iter:318] Loss: 0.602 | Acc: 69.889% \n",
      "[epoch:4, iter:319] Loss: 0.617 | Acc: 69.368% \n",
      "[epoch:4, iter:320] Loss: 0.621 | Acc: 69.600% \n",
      "[epoch:4, iter:321] Loss: 0.620 | Acc: 69.810% \n",
      "[epoch:4, iter:322] Loss: 0.616 | Acc: 69.727% \n",
      "[epoch:4, iter:323] Loss: 0.607 | Acc: 70.261% \n",
      "[epoch:4, iter:324] Loss: 0.612 | Acc: 69.917% \n",
      "[epoch:4, iter:325] Loss: 0.609 | Acc: 70.160% \n",
      "[epoch:4, iter:326] Loss: 0.602 | Acc: 70.692% \n",
      "[epoch:4, iter:327] Loss: 0.602 | Acc: 70.370% \n",
      "[epoch:4, iter:328] Loss: 0.599 | Acc: 70.143% \n",
      "[epoch:4, iter:329] Loss: 0.602 | Acc: 69.724% \n",
      "[epoch:4, iter:330] Loss: 0.602 | Acc: 69.800% \n",
      "[epoch:4, iter:331] Loss: 0.601 | Acc: 69.677% \n",
      "[epoch:4, iter:332] Loss: 0.600 | Acc: 69.500% \n",
      "[epoch:4, iter:333] Loss: 0.594 | Acc: 69.879% \n",
      "[epoch:4, iter:334] Loss: 0.593 | Acc: 69.882% \n",
      "[epoch:4, iter:335] Loss: 0.592 | Acc: 69.771% \n",
      "[epoch:4, iter:336] Loss: 0.594 | Acc: 69.444% \n",
      "[epoch:4, iter:337] Loss: 0.591 | Acc: 69.514% \n",
      "[epoch:4, iter:338] Loss: 0.589 | Acc: 69.684% \n",
      "[epoch:4, iter:339] Loss: 0.586 | Acc: 69.692% \n",
      "[epoch:4, iter:340] Loss: 0.585 | Acc: 69.550% \n",
      "[epoch:4, iter:341] Loss: 0.586 | Acc: 69.317% \n",
      "[epoch:4, iter:342] Loss: 0.586 | Acc: 69.286% \n",
      "[epoch:4, iter:343] Loss: 0.585 | Acc: 69.349% \n",
      "[epoch:4, iter:344] Loss: 0.585 | Acc: 69.455% \n",
      "[epoch:4, iter:345] Loss: 0.586 | Acc: 69.333% \n",
      "[epoch:4, iter:346] Loss: 0.585 | Acc: 69.348% \n",
      "[epoch:4, iter:347] Loss: 0.586 | Acc: 69.149% \n",
      "[epoch:4, iter:348] Loss: 0.588 | Acc: 69.208% \n",
      "[epoch:4, iter:349] Loss: 0.588 | Acc: 69.347% \n",
      "[epoch:4, iter:350] Loss: 0.589 | Acc: 69.120% \n",
      "[epoch:4, iter:351] Loss: 0.588 | Acc: 69.137% \n",
      "[epoch:4, iter:352] Loss: 0.587 | Acc: 69.154% \n",
      "[epoch:4, iter:353] Loss: 0.587 | Acc: 68.981% \n",
      "[epoch:4, iter:354] Loss: 0.585 | Acc: 69.148% \n",
      "[epoch:4, iter:355] Loss: 0.584 | Acc: 69.236% \n",
      "[epoch:4, iter:356] Loss: 0.581 | Acc: 69.107% \n",
      "[epoch:4, iter:357] Loss: 0.578 | Acc: 69.298% \n",
      "[epoch:4, iter:358] Loss: 0.577 | Acc: 69.207% \n",
      "[epoch:4, iter:359] Loss: 0.577 | Acc: 69.186% \n",
      "[epoch:4, iter:360] Loss: 0.577 | Acc: 69.200% \n",
      "[epoch:4, iter:361] Loss: 0.577 | Acc: 69.180% \n",
      "[epoch:4, iter:362] Loss: 0.578 | Acc: 69.097% \n",
      "[epoch:4, iter:363] Loss: 0.579 | Acc: 69.111% \n",
      "[epoch:4, iter:364] Loss: 0.579 | Acc: 69.156% \n",
      "[epoch:4, iter:365] Loss: 0.579 | Acc: 69.046% \n",
      "[epoch:4, iter:366] Loss: 0.579 | Acc: 69.030% \n",
      "[epoch:4, iter:367] Loss: 0.577 | Acc: 69.194% \n",
      "[epoch:4, iter:368] Loss: 0.575 | Acc: 69.294% \n",
      "[epoch:4, iter:369] Loss: 0.580 | Acc: 69.304% \n",
      "[epoch:4, iter:370] Loss: 0.580 | Acc: 69.286% \n",
      "[epoch:4, iter:371] Loss: 0.579 | Acc: 69.380% \n",
      "[epoch:4, iter:372] Loss: 0.579 | Acc: 69.361% \n",
      "[epoch:4, iter:373] Loss: 0.578 | Acc: 69.370% \n",
      "[epoch:4, iter:374] Loss: 0.577 | Acc: 69.297% \n",
      "[epoch:4, iter:375] Loss: 0.576 | Acc: 69.387% \n",
      "[epoch:4, iter:376] Loss: 0.576 | Acc: 69.342% \n",
      "[epoch:4, iter:377] Loss: 0.576 | Acc: 69.247% \n",
      "[epoch:4, iter:378] Loss: 0.575 | Acc: 69.333% \n",
      "[epoch:4, iter:379] Loss: 0.575 | Acc: 69.316% \n",
      "[epoch:4, iter:380] Loss: 0.574 | Acc: 69.400% \n",
      "[epoch:4, iter:381] Loss: 0.572 | Acc: 69.556% \n",
      "[epoch:4, iter:382] Loss: 0.573 | Acc: 69.537% \n",
      "[epoch:4, iter:383] Loss: 0.575 | Acc: 69.470% \n",
      "[epoch:4, iter:384] Loss: 0.575 | Acc: 69.429% \n",
      "[epoch:4, iter:385] Loss: 0.574 | Acc: 69.600% \n",
      "[epoch:4, iter:386] Loss: 0.573 | Acc: 69.581% \n",
      "[epoch:4, iter:387] Loss: 0.575 | Acc: 69.517% \n",
      "[epoch:4, iter:388] Loss: 0.575 | Acc: 69.568% \n",
      "[epoch:4, iter:389] Loss: 0.575 | Acc: 69.528% \n",
      "[epoch:4, iter:390] Loss: 0.575 | Acc: 69.422% \n",
      "[epoch:4, iter:391] Loss: 0.575 | Acc: 69.429% \n",
      "[epoch:4, iter:392] Loss: 0.575 | Acc: 69.391% \n",
      "[epoch:4, iter:393] Loss: 0.574 | Acc: 69.441% \n",
      "[epoch:4, iter:394] Loss: 0.574 | Acc: 69.468% \n",
      "[epoch:4, iter:395] Loss: 0.574 | Acc: 69.411% \n",
      "[epoch:4, iter:396] Loss: 0.573 | Acc: 69.479% \n",
      "[epoch:4, iter:397] Loss: 0.572 | Acc: 69.485% \n",
      "[epoch:4, iter:398] Loss: 0.571 | Acc: 69.551% \n",
      "[epoch:4, iter:399] Loss: 0.570 | Acc: 69.576% \n",
      "[epoch:4, iter:400] Loss: 0.570 | Acc: 69.580% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.120%\n",
      "Training set's accuracy (after quantization) is: 72.920%\n",
      "Test set's accuracy (before quantization) is: 69.500%\n",
      "Test set's accuracy (after quantization) is: 73.000%\n",
      "Train Loss: 0.639 | Train Acc: 69.120% | Test Loss: 0.641 | Test Acc: 69.500% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.920% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.000% \n",
      "\n",
      "Epoch: 5\n",
      "[epoch:5, iter:401] Loss: 0.475 | Acc: 80.000% \n",
      "[epoch:5, iter:402] Loss: 0.647 | Acc: 72.000% \n",
      "[epoch:5, iter:403] Loss: 0.638 | Acc: 70.667% \n",
      "[epoch:5, iter:404] Loss: 0.642 | Acc: 68.500% \n",
      "[epoch:5, iter:405] Loss: 0.620 | Acc: 70.800% \n",
      "[epoch:5, iter:406] Loss: 0.647 | Acc: 69.333% \n",
      "[epoch:5, iter:407] Loss: 0.631 | Acc: 70.000% \n",
      "[epoch:5, iter:408] Loss: 0.624 | Acc: 70.500% \n",
      "[epoch:5, iter:409] Loss: 0.616 | Acc: 71.111% \n",
      "[epoch:5, iter:410] Loss: 0.602 | Acc: 71.800% \n",
      "[epoch:5, iter:411] Loss: 0.611 | Acc: 71.636% \n",
      "[epoch:5, iter:412] Loss: 0.618 | Acc: 72.000% \n",
      "[epoch:5, iter:413] Loss: 0.612 | Acc: 72.308% \n",
      "[epoch:5, iter:414] Loss: 0.603 | Acc: 72.429% \n",
      "[epoch:5, iter:415] Loss: 0.592 | Acc: 72.533% \n",
      "[epoch:5, iter:416] Loss: 0.596 | Acc: 72.500% \n",
      "[epoch:5, iter:417] Loss: 0.593 | Acc: 72.588% \n",
      "[epoch:5, iter:418] Loss: 0.590 | Acc: 71.889% \n",
      "[epoch:5, iter:419] Loss: 0.582 | Acc: 71.895% \n",
      "[epoch:5, iter:420] Loss: 0.577 | Acc: 71.900% \n",
      "[epoch:5, iter:421] Loss: 0.573 | Acc: 71.810% \n",
      "[epoch:5, iter:422] Loss: 0.571 | Acc: 71.727% \n",
      "[epoch:5, iter:423] Loss: 0.573 | Acc: 71.565% \n",
      "[epoch:5, iter:424] Loss: 0.584 | Acc: 71.083% \n",
      "[epoch:5, iter:425] Loss: 0.582 | Acc: 71.200% \n",
      "[epoch:5, iter:426] Loss: 0.581 | Acc: 71.000% \n",
      "[epoch:5, iter:427] Loss: 0.580 | Acc: 71.259% \n",
      "[epoch:5, iter:428] Loss: 0.579 | Acc: 71.000% \n",
      "[epoch:5, iter:429] Loss: 0.577 | Acc: 70.621% \n",
      "[epoch:5, iter:430] Loss: 0.575 | Acc: 70.800% \n",
      "[epoch:5, iter:431] Loss: 0.575 | Acc: 70.710% \n",
      "[epoch:5, iter:432] Loss: 0.578 | Acc: 70.438% \n",
      "[epoch:5, iter:433] Loss: 0.574 | Acc: 70.364% \n",
      "[epoch:5, iter:434] Loss: 0.577 | Acc: 70.176% \n",
      "[epoch:5, iter:435] Loss: 0.575 | Acc: 70.114% \n",
      "[epoch:5, iter:436] Loss: 0.578 | Acc: 69.833% \n",
      "[epoch:5, iter:437] Loss: 0.575 | Acc: 70.324% \n",
      "[epoch:5, iter:438] Loss: 0.580 | Acc: 70.105% \n",
      "[epoch:5, iter:439] Loss: 0.577 | Acc: 70.205% \n",
      "[epoch:5, iter:440] Loss: 0.577 | Acc: 70.300% \n",
      "[epoch:5, iter:441] Loss: 0.577 | Acc: 70.341% \n",
      "[epoch:5, iter:442] Loss: 0.577 | Acc: 70.190% \n",
      "[epoch:5, iter:443] Loss: 0.574 | Acc: 70.419% \n",
      "[epoch:5, iter:444] Loss: 0.571 | Acc: 70.409% \n",
      "[epoch:5, iter:445] Loss: 0.571 | Acc: 70.489% \n",
      "[epoch:5, iter:446] Loss: 0.571 | Acc: 70.304% \n",
      "[epoch:5, iter:447] Loss: 0.570 | Acc: 70.255% \n",
      "[epoch:5, iter:448] Loss: 0.568 | Acc: 70.208% \n",
      "[epoch:5, iter:449] Loss: 0.569 | Acc: 70.122% \n",
      "[epoch:5, iter:450] Loss: 0.571 | Acc: 69.960% \n",
      "[epoch:5, iter:451] Loss: 0.572 | Acc: 69.882% \n",
      "[epoch:5, iter:452] Loss: 0.572 | Acc: 69.808% \n",
      "[epoch:5, iter:453] Loss: 0.572 | Acc: 69.698% \n",
      "[epoch:5, iter:454] Loss: 0.571 | Acc: 69.815% \n",
      "[epoch:5, iter:455] Loss: 0.572 | Acc: 69.709% \n",
      "[epoch:5, iter:456] Loss: 0.573 | Acc: 69.571% \n",
      "[epoch:5, iter:457] Loss: 0.572 | Acc: 69.614% \n",
      "[epoch:5, iter:458] Loss: 0.570 | Acc: 69.621% \n",
      "[epoch:5, iter:459] Loss: 0.569 | Acc: 69.695% \n",
      "[epoch:5, iter:460] Loss: 0.570 | Acc: 69.733% \n",
      "[epoch:5, iter:461] Loss: 0.569 | Acc: 69.672% \n",
      "[epoch:5, iter:462] Loss: 0.567 | Acc: 69.774% \n",
      "[epoch:5, iter:463] Loss: 0.567 | Acc: 69.778% \n",
      "[epoch:5, iter:464] Loss: 0.568 | Acc: 69.875% \n",
      "[epoch:5, iter:465] Loss: 0.566 | Acc: 69.908% \n",
      "[epoch:5, iter:466] Loss: 0.565 | Acc: 69.970% \n",
      "[epoch:5, iter:467] Loss: 0.566 | Acc: 69.851% \n",
      "[epoch:5, iter:468] Loss: 0.564 | Acc: 69.853% \n",
      "[epoch:5, iter:469] Loss: 0.565 | Acc: 69.826% \n",
      "[epoch:5, iter:470] Loss: 0.564 | Acc: 69.857% \n",
      "[epoch:5, iter:471] Loss: 0.565 | Acc: 69.803% \n",
      "[epoch:5, iter:472] Loss: 0.565 | Acc: 69.833% \n",
      "[epoch:5, iter:473] Loss: 0.564 | Acc: 69.726% \n",
      "[epoch:5, iter:474] Loss: 0.564 | Acc: 69.676% \n",
      "[epoch:5, iter:475] Loss: 0.564 | Acc: 69.627% \n",
      "[epoch:5, iter:476] Loss: 0.563 | Acc: 69.632% \n",
      "[epoch:5, iter:477] Loss: 0.563 | Acc: 69.688% \n",
      "[epoch:5, iter:478] Loss: 0.564 | Acc: 69.590% \n",
      "[epoch:5, iter:479] Loss: 0.563 | Acc: 69.544% \n",
      "[epoch:5, iter:480] Loss: 0.563 | Acc: 69.525% \n",
      "[epoch:5, iter:481] Loss: 0.563 | Acc: 69.679% \n",
      "[epoch:5, iter:482] Loss: 0.561 | Acc: 69.756% \n",
      "[epoch:5, iter:483] Loss: 0.559 | Acc: 69.807% \n",
      "[epoch:5, iter:484] Loss: 0.563 | Acc: 69.643% \n",
      "[epoch:5, iter:485] Loss: 0.564 | Acc: 69.412% \n",
      "[epoch:5, iter:486] Loss: 0.564 | Acc: 69.419% \n",
      "[epoch:5, iter:487] Loss: 0.564 | Acc: 69.448% \n",
      "[epoch:5, iter:488] Loss: 0.563 | Acc: 69.455% \n",
      "[epoch:5, iter:489] Loss: 0.562 | Acc: 69.506% \n",
      "[epoch:5, iter:490] Loss: 0.561 | Acc: 69.556% \n",
      "[epoch:5, iter:491] Loss: 0.561 | Acc: 69.626% \n",
      "[epoch:5, iter:492] Loss: 0.560 | Acc: 69.674% \n",
      "[epoch:5, iter:493] Loss: 0.562 | Acc: 69.570% \n",
      "[epoch:5, iter:494] Loss: 0.562 | Acc: 69.574% \n",
      "[epoch:5, iter:495] Loss: 0.562 | Acc: 69.495% \n",
      "[epoch:5, iter:496] Loss: 0.561 | Acc: 69.458% \n",
      "[epoch:5, iter:497] Loss: 0.561 | Acc: 69.402% \n",
      "[epoch:5, iter:498] Loss: 0.561 | Acc: 69.347% \n",
      "[epoch:5, iter:499] Loss: 0.562 | Acc: 69.333% \n",
      "[epoch:5, iter:500] Loss: 0.561 | Acc: 69.420% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.400%\n",
      "Training set's accuracy (after quantization) is: 72.920%\n",
      "Test set's accuracy (before quantization) is: 69.800%\n",
      "Test set's accuracy (after quantization) is: 73.300%\n",
      "Train Loss: 0.621 | Train Acc: 69.400% | Test Loss: 0.623 | Test Acc: 69.800% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.920% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.300% \n",
      "\n",
      "Epoch: 6\n",
      "[epoch:6, iter:501] Loss: 0.538 | Acc: 68.000% \n",
      "[epoch:6, iter:502] Loss: 0.591 | Acc: 62.000% \n",
      "[epoch:6, iter:503] Loss: 0.638 | Acc: 62.000% \n",
      "[epoch:6, iter:504] Loss: 0.638 | Acc: 65.500% \n",
      "[epoch:6, iter:505] Loss: 0.702 | Acc: 65.600% \n",
      "[epoch:6, iter:506] Loss: 0.675 | Acc: 67.667% \n",
      "[epoch:6, iter:507] Loss: 0.642 | Acc: 68.571% \n",
      "[epoch:6, iter:508] Loss: 0.630 | Acc: 67.750% \n",
      "[epoch:6, iter:509] Loss: 0.625 | Acc: 68.667% \n",
      "[epoch:6, iter:510] Loss: 0.641 | Acc: 68.200% \n",
      "[epoch:6, iter:511] Loss: 0.649 | Acc: 68.909% \n",
      "[epoch:6, iter:512] Loss: 0.642 | Acc: 69.333% \n",
      "[epoch:6, iter:513] Loss: 0.635 | Acc: 68.615% \n",
      "[epoch:6, iter:514] Loss: 0.632 | Acc: 68.571% \n",
      "[epoch:6, iter:515] Loss: 0.629 | Acc: 68.267% \n",
      "[epoch:6, iter:516] Loss: 0.624 | Acc: 68.500% \n",
      "[epoch:6, iter:517] Loss: 0.619 | Acc: 68.471% \n",
      "[epoch:6, iter:518] Loss: 0.620 | Acc: 68.333% \n",
      "[epoch:6, iter:519] Loss: 0.615 | Acc: 68.421% \n",
      "[epoch:6, iter:520] Loss: 0.609 | Acc: 68.800% \n",
      "[epoch:6, iter:521] Loss: 0.610 | Acc: 69.238% \n",
      "[epoch:6, iter:522] Loss: 0.610 | Acc: 69.000% \n",
      "[epoch:6, iter:523] Loss: 0.612 | Acc: 68.522% \n",
      "[epoch:6, iter:524] Loss: 0.615 | Acc: 68.583% \n",
      "[epoch:6, iter:525] Loss: 0.615 | Acc: 68.240% \n",
      "[epoch:6, iter:526] Loss: 0.613 | Acc: 68.154% \n",
      "[epoch:6, iter:527] Loss: 0.606 | Acc: 68.444% \n",
      "[epoch:6, iter:528] Loss: 0.602 | Acc: 68.857% \n",
      "[epoch:6, iter:529] Loss: 0.602 | Acc: 68.690% \n",
      "[epoch:6, iter:530] Loss: 0.605 | Acc: 68.533% \n",
      "[epoch:6, iter:531] Loss: 0.603 | Acc: 67.935% \n",
      "[epoch:6, iter:532] Loss: 0.599 | Acc: 68.000% \n",
      "[epoch:6, iter:533] Loss: 0.600 | Acc: 67.818% \n",
      "[epoch:6, iter:534] Loss: 0.597 | Acc: 68.000% \n",
      "[epoch:6, iter:535] Loss: 0.597 | Acc: 67.943% \n",
      "[epoch:6, iter:536] Loss: 0.598 | Acc: 67.944% \n",
      "[epoch:6, iter:537] Loss: 0.600 | Acc: 67.946% \n",
      "[epoch:6, iter:538] Loss: 0.597 | Acc: 67.947% \n",
      "[epoch:6, iter:539] Loss: 0.595 | Acc: 67.949% \n",
      "[epoch:6, iter:540] Loss: 0.596 | Acc: 67.600% \n",
      "[epoch:6, iter:541] Loss: 0.594 | Acc: 67.902% \n",
      "[epoch:6, iter:542] Loss: 0.590 | Acc: 67.952% \n",
      "[epoch:6, iter:543] Loss: 0.592 | Acc: 67.814% \n",
      "[epoch:6, iter:544] Loss: 0.588 | Acc: 67.909% \n",
      "[epoch:6, iter:545] Loss: 0.588 | Acc: 67.822% \n",
      "[epoch:6, iter:546] Loss: 0.587 | Acc: 67.913% \n",
      "[epoch:6, iter:547] Loss: 0.585 | Acc: 67.915% \n",
      "[epoch:6, iter:548] Loss: 0.585 | Acc: 67.875% \n",
      "[epoch:6, iter:549] Loss: 0.583 | Acc: 67.918% \n",
      "[epoch:6, iter:550] Loss: 0.582 | Acc: 68.000% \n",
      "[epoch:6, iter:551] Loss: 0.579 | Acc: 68.353% \n",
      "[epoch:6, iter:552] Loss: 0.583 | Acc: 68.231% \n",
      "[epoch:6, iter:553] Loss: 0.586 | Acc: 68.038% \n",
      "[epoch:6, iter:554] Loss: 0.584 | Acc: 68.222% \n",
      "[epoch:6, iter:555] Loss: 0.584 | Acc: 68.182% \n",
      "[epoch:6, iter:556] Loss: 0.584 | Acc: 68.179% \n",
      "[epoch:6, iter:557] Loss: 0.583 | Acc: 68.175% \n",
      "[epoch:6, iter:558] Loss: 0.579 | Acc: 68.483% \n",
      "[epoch:6, iter:559] Loss: 0.578 | Acc: 68.610% \n",
      "[epoch:6, iter:560] Loss: 0.580 | Acc: 68.667% \n",
      "[epoch:6, iter:561] Loss: 0.579 | Acc: 68.656% \n",
      "[epoch:6, iter:562] Loss: 0.576 | Acc: 68.806% \n",
      "[epoch:6, iter:563] Loss: 0.574 | Acc: 68.762% \n",
      "[epoch:6, iter:564] Loss: 0.574 | Acc: 68.750% \n",
      "[epoch:6, iter:565] Loss: 0.575 | Acc: 68.554% \n",
      "[epoch:6, iter:566] Loss: 0.573 | Acc: 68.727% \n",
      "[epoch:6, iter:567] Loss: 0.574 | Acc: 68.657% \n",
      "[epoch:6, iter:568] Loss: 0.574 | Acc: 68.618% \n",
      "[epoch:6, iter:569] Loss: 0.574 | Acc: 68.580% \n",
      "[epoch:6, iter:570] Loss: 0.573 | Acc: 68.714% \n",
      "[epoch:6, iter:571] Loss: 0.573 | Acc: 68.845% \n",
      "[epoch:6, iter:572] Loss: 0.572 | Acc: 68.944% \n",
      "[epoch:6, iter:573] Loss: 0.572 | Acc: 68.932% \n",
      "[epoch:6, iter:574] Loss: 0.572 | Acc: 68.811% \n",
      "[epoch:6, iter:575] Loss: 0.572 | Acc: 68.853% \n",
      "[epoch:6, iter:576] Loss: 0.571 | Acc: 68.921% \n",
      "[epoch:6, iter:577] Loss: 0.571 | Acc: 68.935% \n",
      "[epoch:6, iter:578] Loss: 0.570 | Acc: 68.923% \n",
      "[epoch:6, iter:579] Loss: 0.568 | Acc: 69.089% \n",
      "[epoch:6, iter:580] Loss: 0.572 | Acc: 68.950% \n",
      "[epoch:6, iter:581] Loss: 0.570 | Acc: 69.012% \n",
      "[epoch:6, iter:582] Loss: 0.570 | Acc: 69.024% \n",
      "[epoch:6, iter:583] Loss: 0.570 | Acc: 69.133% \n",
      "[epoch:6, iter:584] Loss: 0.569 | Acc: 69.095% \n",
      "[epoch:6, iter:585] Loss: 0.568 | Acc: 69.082% \n",
      "[epoch:6, iter:586] Loss: 0.569 | Acc: 69.000% \n",
      "[epoch:6, iter:587] Loss: 0.568 | Acc: 69.011% \n",
      "[epoch:6, iter:588] Loss: 0.568 | Acc: 69.159% \n",
      "[epoch:6, iter:589] Loss: 0.567 | Acc: 69.236% \n",
      "[epoch:6, iter:590] Loss: 0.567 | Acc: 69.156% \n",
      "[epoch:6, iter:591] Loss: 0.567 | Acc: 69.231% \n",
      "[epoch:6, iter:592] Loss: 0.567 | Acc: 69.239% \n",
      "[epoch:6, iter:593] Loss: 0.566 | Acc: 69.247% \n",
      "[epoch:6, iter:594] Loss: 0.566 | Acc: 69.319% \n",
      "[epoch:6, iter:595] Loss: 0.566 | Acc: 69.368% \n",
      "[epoch:6, iter:596] Loss: 0.566 | Acc: 69.292% \n",
      "[epoch:6, iter:597] Loss: 0.565 | Acc: 69.464% \n",
      "[epoch:6, iter:598] Loss: 0.563 | Acc: 69.612% \n",
      "[epoch:6, iter:599] Loss: 0.566 | Acc: 69.576% \n",
      "[epoch:6, iter:600] Loss: 0.565 | Acc: 69.620% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.140%\n",
      "Training set's accuracy (after quantization) is: 73.040%\n",
      "Test set's accuracy (before quantization) is: 69.600%\n",
      "Test set's accuracy (after quantization) is: 73.700%\n",
      "Train Loss: 0.628 | Train Acc: 69.140% | Test Loss: 0.629 | Test Acc: 69.600% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 73.040% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.700% \n",
      "\n",
      "Epoch: 7\n",
      "[epoch:7, iter:601] Loss: 0.487 | Acc: 64.000% \n",
      "[epoch:7, iter:602] Loss: 0.524 | Acc: 65.000% \n",
      "[epoch:7, iter:603] Loss: 0.528 | Acc: 64.667% \n",
      "[epoch:7, iter:604] Loss: 0.542 | Acc: 64.500% \n",
      "[epoch:7, iter:605] Loss: 0.548 | Acc: 65.200% \n",
      "[epoch:7, iter:606] Loss: 0.559 | Acc: 65.000% \n",
      "[epoch:7, iter:607] Loss: 0.558 | Acc: 64.571% \n",
      "[epoch:7, iter:608] Loss: 0.560 | Acc: 65.250% \n",
      "[epoch:7, iter:609] Loss: 0.553 | Acc: 66.222% \n",
      "[epoch:7, iter:610] Loss: 0.555 | Acc: 66.400% \n",
      "[epoch:7, iter:611] Loss: 0.559 | Acc: 66.909% \n",
      "[epoch:7, iter:612] Loss: 0.555 | Acc: 67.000% \n",
      "[epoch:7, iter:613] Loss: 0.550 | Acc: 67.231% \n",
      "[epoch:7, iter:614] Loss: 0.554 | Acc: 66.571% \n",
      "[epoch:7, iter:615] Loss: 0.554 | Acc: 66.800% \n",
      "[epoch:7, iter:616] Loss: 0.556 | Acc: 67.000% \n",
      "[epoch:7, iter:617] Loss: 0.562 | Acc: 66.353% \n",
      "[epoch:7, iter:618] Loss: 0.555 | Acc: 66.778% \n",
      "[epoch:7, iter:619] Loss: 0.559 | Acc: 66.947% \n",
      "[epoch:7, iter:620] Loss: 0.563 | Acc: 66.800% \n",
      "[epoch:7, iter:621] Loss: 0.563 | Acc: 66.857% \n",
      "[epoch:7, iter:622] Loss: 0.564 | Acc: 66.455% \n",
      "[epoch:7, iter:623] Loss: 0.561 | Acc: 66.783% \n",
      "[epoch:7, iter:624] Loss: 0.559 | Acc: 67.000% \n",
      "[epoch:7, iter:625] Loss: 0.553 | Acc: 67.280% \n",
      "[epoch:7, iter:626] Loss: 0.552 | Acc: 67.923% \n",
      "[epoch:7, iter:627] Loss: 0.554 | Acc: 68.148% \n",
      "[epoch:7, iter:628] Loss: 0.558 | Acc: 67.571% \n",
      "[epoch:7, iter:629] Loss: 0.557 | Acc: 68.138% \n",
      "[epoch:7, iter:630] Loss: 0.553 | Acc: 68.133% \n",
      "[epoch:7, iter:631] Loss: 0.556 | Acc: 67.742% \n",
      "[epoch:7, iter:632] Loss: 0.557 | Acc: 67.688% \n",
      "[epoch:7, iter:633] Loss: 0.557 | Acc: 67.636% \n",
      "[epoch:7, iter:634] Loss: 0.561 | Acc: 67.412% \n",
      "[epoch:7, iter:635] Loss: 0.561 | Acc: 67.371% \n",
      "[epoch:7, iter:636] Loss: 0.564 | Acc: 67.444% \n",
      "[epoch:7, iter:637] Loss: 0.562 | Acc: 67.730% \n",
      "[epoch:7, iter:638] Loss: 0.559 | Acc: 68.211% \n",
      "[epoch:7, iter:639] Loss: 0.560 | Acc: 68.256% \n",
      "[epoch:7, iter:640] Loss: 0.559 | Acc: 68.200% \n",
      "[epoch:7, iter:641] Loss: 0.562 | Acc: 68.146% \n",
      "[epoch:7, iter:642] Loss: 0.560 | Acc: 68.381% \n",
      "[epoch:7, iter:643] Loss: 0.560 | Acc: 68.419% \n",
      "[epoch:7, iter:644] Loss: 0.559 | Acc: 68.545% \n",
      "[epoch:7, iter:645] Loss: 0.558 | Acc: 68.756% \n",
      "[epoch:7, iter:646] Loss: 0.557 | Acc: 69.000% \n",
      "[epoch:7, iter:647] Loss: 0.558 | Acc: 68.936% \n",
      "[epoch:7, iter:648] Loss: 0.556 | Acc: 68.917% \n",
      "[epoch:7, iter:649] Loss: 0.555 | Acc: 68.980% \n",
      "[epoch:7, iter:650] Loss: 0.554 | Acc: 69.120% \n",
      "[epoch:7, iter:651] Loss: 0.552 | Acc: 69.059% \n",
      "[epoch:7, iter:652] Loss: 0.551 | Acc: 69.000% \n",
      "[epoch:7, iter:653] Loss: 0.551 | Acc: 68.906% \n",
      "[epoch:7, iter:654] Loss: 0.551 | Acc: 68.889% \n",
      "[epoch:7, iter:655] Loss: 0.549 | Acc: 68.982% \n",
      "[epoch:7, iter:656] Loss: 0.549 | Acc: 68.857% \n",
      "[epoch:7, iter:657] Loss: 0.549 | Acc: 68.982% \n",
      "[epoch:7, iter:658] Loss: 0.547 | Acc: 69.069% \n",
      "[epoch:7, iter:659] Loss: 0.550 | Acc: 68.983% \n",
      "[epoch:7, iter:660] Loss: 0.549 | Acc: 69.100% \n",
      "[epoch:7, iter:661] Loss: 0.549 | Acc: 69.115% \n",
      "[epoch:7, iter:662] Loss: 0.548 | Acc: 69.129% \n",
      "[epoch:7, iter:663] Loss: 0.549 | Acc: 69.238% \n",
      "[epoch:7, iter:664] Loss: 0.549 | Acc: 69.156% \n",
      "[epoch:7, iter:665] Loss: 0.548 | Acc: 69.169% \n",
      "[epoch:7, iter:666] Loss: 0.547 | Acc: 69.212% \n",
      "[epoch:7, iter:667] Loss: 0.546 | Acc: 69.313% \n",
      "[epoch:7, iter:668] Loss: 0.549 | Acc: 69.294% \n",
      "[epoch:7, iter:669] Loss: 0.548 | Acc: 69.159% \n",
      "[epoch:7, iter:670] Loss: 0.549 | Acc: 69.171% \n",
      "[epoch:7, iter:671] Loss: 0.549 | Acc: 69.155% \n",
      "[epoch:7, iter:672] Loss: 0.549 | Acc: 69.194% \n",
      "[epoch:7, iter:673] Loss: 0.548 | Acc: 69.260% \n",
      "[epoch:7, iter:674] Loss: 0.548 | Acc: 69.432% \n",
      "[epoch:7, iter:675] Loss: 0.546 | Acc: 69.520% \n",
      "[epoch:7, iter:676] Loss: 0.547 | Acc: 69.579% \n",
      "[epoch:7, iter:677] Loss: 0.547 | Acc: 69.532% \n",
      "[epoch:7, iter:678] Loss: 0.548 | Acc: 69.462% \n",
      "[epoch:7, iter:679] Loss: 0.548 | Acc: 69.367% \n",
      "[epoch:7, iter:680] Loss: 0.549 | Acc: 69.250% \n",
      "[epoch:7, iter:681] Loss: 0.548 | Acc: 69.358% \n",
      "[epoch:7, iter:682] Loss: 0.547 | Acc: 69.293% \n",
      "[epoch:7, iter:683] Loss: 0.547 | Acc: 69.205% \n",
      "[epoch:7, iter:684] Loss: 0.547 | Acc: 69.214% \n",
      "[epoch:7, iter:685] Loss: 0.546 | Acc: 69.294% \n",
      "[epoch:7, iter:686] Loss: 0.548 | Acc: 69.256% \n",
      "[epoch:7, iter:687] Loss: 0.548 | Acc: 69.310% \n",
      "[epoch:7, iter:688] Loss: 0.547 | Acc: 69.386% \n",
      "[epoch:7, iter:689] Loss: 0.545 | Acc: 69.528% \n",
      "[epoch:7, iter:690] Loss: 0.547 | Acc: 69.511% \n",
      "[epoch:7, iter:691] Loss: 0.546 | Acc: 69.538% \n",
      "[epoch:7, iter:692] Loss: 0.548 | Acc: 69.391% \n",
      "[epoch:7, iter:693] Loss: 0.548 | Acc: 69.398% \n",
      "[epoch:7, iter:694] Loss: 0.549 | Acc: 69.362% \n",
      "[epoch:7, iter:695] Loss: 0.549 | Acc: 69.411% \n",
      "[epoch:7, iter:696] Loss: 0.549 | Acc: 69.354% \n",
      "[epoch:7, iter:697] Loss: 0.548 | Acc: 69.485% \n",
      "[epoch:7, iter:698] Loss: 0.553 | Acc: 69.449% \n",
      "[epoch:7, iter:699] Loss: 0.553 | Acc: 69.455% \n",
      "[epoch:7, iter:700] Loss: 0.552 | Acc: 69.560% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.280%\n",
      "Training set's accuracy (after quantization) is: 72.860%\n",
      "Test set's accuracy (before quantization) is: 69.800%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.627 | Train Acc: 69.280% | Test Loss: 0.628 | Test Acc: 69.800% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.860% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 8\n",
      "[epoch:8, iter:701] Loss: 0.558 | Acc: 74.000% \n",
      "[epoch:8, iter:702] Loss: 0.559 | Acc: 70.000% \n",
      "[epoch:8, iter:703] Loss: 0.544 | Acc: 70.667% \n",
      "[epoch:8, iter:704] Loss: 0.544 | Acc: 68.500% \n",
      "[epoch:8, iter:705] Loss: 0.546 | Acc: 66.400% \n",
      "[epoch:8, iter:706] Loss: 0.562 | Acc: 65.000% \n",
      "[epoch:8, iter:707] Loss: 0.565 | Acc: 65.714% \n",
      "[epoch:8, iter:708] Loss: 0.565 | Acc: 65.750% \n",
      "[epoch:8, iter:709] Loss: 0.567 | Acc: 65.111% \n",
      "[epoch:8, iter:710] Loss: 0.552 | Acc: 66.400% \n",
      "[epoch:8, iter:711] Loss: 0.557 | Acc: 66.000% \n",
      "[epoch:8, iter:712] Loss: 0.554 | Acc: 67.000% \n",
      "[epoch:8, iter:713] Loss: 0.545 | Acc: 67.231% \n",
      "[epoch:8, iter:714] Loss: 0.544 | Acc: 66.857% \n",
      "[epoch:8, iter:715] Loss: 0.547 | Acc: 67.067% \n",
      "[epoch:8, iter:716] Loss: 0.545 | Acc: 67.000% \n",
      "[epoch:8, iter:717] Loss: 0.551 | Acc: 66.706% \n",
      "[epoch:8, iter:718] Loss: 0.550 | Acc: 67.111% \n",
      "[epoch:8, iter:719] Loss: 0.556 | Acc: 67.053% \n",
      "[epoch:8, iter:720] Loss: 0.561 | Acc: 67.000% \n",
      "[epoch:8, iter:721] Loss: 0.559 | Acc: 67.143% \n",
      "[epoch:8, iter:722] Loss: 0.557 | Acc: 67.273% \n",
      "[epoch:8, iter:723] Loss: 0.557 | Acc: 67.304% \n",
      "[epoch:8, iter:724] Loss: 0.552 | Acc: 67.750% \n",
      "[epoch:8, iter:725] Loss: 0.553 | Acc: 67.840% \n",
      "[epoch:8, iter:726] Loss: 0.554 | Acc: 67.538% \n",
      "[epoch:8, iter:727] Loss: 0.553 | Acc: 67.333% \n",
      "[epoch:8, iter:728] Loss: 0.549 | Acc: 67.500% \n",
      "[epoch:8, iter:729] Loss: 0.549 | Acc: 67.655% \n",
      "[epoch:8, iter:730] Loss: 0.551 | Acc: 67.667% \n",
      "[epoch:8, iter:731] Loss: 0.551 | Acc: 67.613% \n",
      "[epoch:8, iter:732] Loss: 0.551 | Acc: 67.812% \n",
      "[epoch:8, iter:733] Loss: 0.551 | Acc: 67.758% \n",
      "[epoch:8, iter:734] Loss: 0.553 | Acc: 67.471% \n",
      "[epoch:8, iter:735] Loss: 0.556 | Acc: 67.543% \n",
      "[epoch:8, iter:736] Loss: 0.555 | Acc: 67.556% \n",
      "[epoch:8, iter:737] Loss: 0.556 | Acc: 67.568% \n",
      "[epoch:8, iter:738] Loss: 0.554 | Acc: 67.632% \n",
      "[epoch:8, iter:739] Loss: 0.550 | Acc: 67.897% \n",
      "[epoch:8, iter:740] Loss: 0.551 | Acc: 67.900% \n",
      "[epoch:8, iter:741] Loss: 0.552 | Acc: 67.902% \n",
      "[epoch:8, iter:742] Loss: 0.549 | Acc: 68.143% \n",
      "[epoch:8, iter:743] Loss: 0.547 | Acc: 68.279% \n",
      "[epoch:8, iter:744] Loss: 0.546 | Acc: 68.409% \n",
      "[epoch:8, iter:745] Loss: 0.546 | Acc: 68.444% \n",
      "[epoch:8, iter:746] Loss: 0.546 | Acc: 68.217% \n",
      "[epoch:8, iter:747] Loss: 0.547 | Acc: 68.213% \n",
      "[epoch:8, iter:748] Loss: 0.547 | Acc: 68.208% \n",
      "[epoch:8, iter:749] Loss: 0.547 | Acc: 68.367% \n",
      "[epoch:8, iter:750] Loss: 0.546 | Acc: 68.560% \n",
      "[epoch:8, iter:751] Loss: 0.546 | Acc: 68.588% \n",
      "[epoch:8, iter:752] Loss: 0.546 | Acc: 68.538% \n",
      "[epoch:8, iter:753] Loss: 0.546 | Acc: 68.566% \n",
      "[epoch:8, iter:754] Loss: 0.546 | Acc: 68.519% \n",
      "[epoch:8, iter:755] Loss: 0.549 | Acc: 68.436% \n",
      "[epoch:8, iter:756] Loss: 0.550 | Acc: 68.464% \n",
      "[epoch:8, iter:757] Loss: 0.552 | Acc: 68.246% \n",
      "[epoch:8, iter:758] Loss: 0.557 | Acc: 68.379% \n",
      "[epoch:8, iter:759] Loss: 0.565 | Acc: 68.305% \n",
      "[epoch:8, iter:760] Loss: 0.564 | Acc: 68.433% \n",
      "[epoch:8, iter:761] Loss: 0.564 | Acc: 68.393% \n",
      "[epoch:8, iter:762] Loss: 0.562 | Acc: 68.452% \n",
      "[epoch:8, iter:763] Loss: 0.562 | Acc: 68.571% \n",
      "[epoch:8, iter:764] Loss: 0.569 | Acc: 68.469% \n",
      "[epoch:8, iter:765] Loss: 0.569 | Acc: 68.431% \n",
      "[epoch:8, iter:766] Loss: 0.567 | Acc: 68.576% \n",
      "[epoch:8, iter:767] Loss: 0.564 | Acc: 68.776% \n",
      "[epoch:8, iter:768] Loss: 0.564 | Acc: 68.794% \n",
      "[epoch:8, iter:769] Loss: 0.564 | Acc: 68.667% \n",
      "[epoch:8, iter:770] Loss: 0.564 | Acc: 68.571% \n",
      "[epoch:8, iter:771] Loss: 0.564 | Acc: 68.676% \n",
      "[epoch:8, iter:772] Loss: 0.562 | Acc: 68.833% \n",
      "[epoch:8, iter:773] Loss: 0.563 | Acc: 68.904% \n",
      "[epoch:8, iter:774] Loss: 0.563 | Acc: 68.946% \n",
      "[epoch:8, iter:775] Loss: 0.562 | Acc: 69.040% \n",
      "[epoch:8, iter:776] Loss: 0.561 | Acc: 69.105% \n",
      "[epoch:8, iter:777] Loss: 0.561 | Acc: 69.169% \n",
      "[epoch:8, iter:778] Loss: 0.561 | Acc: 69.205% \n",
      "[epoch:8, iter:779] Loss: 0.563 | Acc: 69.114% \n",
      "[epoch:8, iter:780] Loss: 0.561 | Acc: 69.200% \n",
      "[epoch:8, iter:781] Loss: 0.559 | Acc: 69.259% \n",
      "[epoch:8, iter:782] Loss: 0.559 | Acc: 69.317% \n",
      "[epoch:8, iter:783] Loss: 0.560 | Acc: 69.325% \n",
      "[epoch:8, iter:784] Loss: 0.561 | Acc: 69.333% \n",
      "[epoch:8, iter:785] Loss: 0.561 | Acc: 69.365% \n",
      "[epoch:8, iter:786] Loss: 0.560 | Acc: 69.442% \n",
      "[epoch:8, iter:787] Loss: 0.560 | Acc: 69.402% \n",
      "[epoch:8, iter:788] Loss: 0.561 | Acc: 69.250% \n",
      "[epoch:8, iter:789] Loss: 0.560 | Acc: 69.236% \n",
      "[epoch:8, iter:790] Loss: 0.561 | Acc: 69.289% \n",
      "[epoch:8, iter:791] Loss: 0.561 | Acc: 69.363% \n",
      "[epoch:8, iter:792] Loss: 0.561 | Acc: 69.304% \n",
      "[epoch:8, iter:793] Loss: 0.561 | Acc: 69.398% \n",
      "[epoch:8, iter:794] Loss: 0.561 | Acc: 69.383% \n",
      "[epoch:8, iter:795] Loss: 0.562 | Acc: 69.305% \n",
      "[epoch:8, iter:796] Loss: 0.561 | Acc: 69.375% \n",
      "[epoch:8, iter:797] Loss: 0.561 | Acc: 69.443% \n",
      "[epoch:8, iter:798] Loss: 0.562 | Acc: 69.429% \n",
      "[epoch:8, iter:799] Loss: 0.561 | Acc: 69.475% \n",
      "[epoch:8, iter:800] Loss: 0.562 | Acc: 69.520% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.600%\n",
      "Training set's accuracy (after quantization) is: 72.800%\n",
      "Test set's accuracy (before quantization) is: 69.900%\n",
      "Test set's accuracy (after quantization) is: 73.100%\n",
      "Train Loss: 0.590 | Train Acc: 69.600% | Test Loss: 0.591 | Test Acc: 69.900% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.800% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.100% \n",
      "\n",
      "Epoch: 9\n",
      "[epoch:9, iter:801] Loss: 0.623 | Acc: 58.000% \n",
      "[epoch:9, iter:802] Loss: 0.610 | Acc: 63.000% \n",
      "[epoch:9, iter:803] Loss: 0.603 | Acc: 66.000% \n",
      "[epoch:9, iter:804] Loss: 0.609 | Acc: 64.500% \n",
      "[epoch:9, iter:805] Loss: 0.600 | Acc: 65.600% \n",
      "[epoch:9, iter:806] Loss: 0.583 | Acc: 68.333% \n",
      "[epoch:9, iter:807] Loss: 0.568 | Acc: 70.000% \n",
      "[epoch:9, iter:808] Loss: 0.583 | Acc: 69.000% \n",
      "[epoch:9, iter:809] Loss: 0.577 | Acc: 67.333% \n",
      "[epoch:9, iter:810] Loss: 0.580 | Acc: 66.600% \n",
      "[epoch:9, iter:811] Loss: 0.569 | Acc: 67.455% \n",
      "[epoch:9, iter:812] Loss: 0.574 | Acc: 68.000% \n",
      "[epoch:9, iter:813] Loss: 0.573 | Acc: 68.154% \n",
      "[epoch:9, iter:814] Loss: 0.576 | Acc: 67.714% \n",
      "[epoch:9, iter:815] Loss: 0.568 | Acc: 68.667% \n",
      "[epoch:9, iter:816] Loss: 0.563 | Acc: 69.000% \n",
      "[epoch:9, iter:817] Loss: 0.563 | Acc: 68.824% \n",
      "[epoch:9, iter:818] Loss: 0.564 | Acc: 68.778% \n",
      "[epoch:9, iter:819] Loss: 0.566 | Acc: 68.316% \n",
      "[epoch:9, iter:820] Loss: 0.568 | Acc: 67.900% \n",
      "[epoch:9, iter:821] Loss: 0.562 | Acc: 68.476% \n",
      "[epoch:9, iter:822] Loss: 0.560 | Acc: 68.818% \n",
      "[epoch:9, iter:823] Loss: 0.570 | Acc: 68.348% \n",
      "[epoch:9, iter:824] Loss: 0.568 | Acc: 68.750% \n",
      "[epoch:9, iter:825] Loss: 0.569 | Acc: 68.960% \n",
      "[epoch:9, iter:826] Loss: 0.567 | Acc: 68.692% \n",
      "[epoch:9, iter:827] Loss: 0.564 | Acc: 68.519% \n",
      "[epoch:9, iter:828] Loss: 0.565 | Acc: 68.643% \n",
      "[epoch:9, iter:829] Loss: 0.570 | Acc: 68.759% \n",
      "[epoch:9, iter:830] Loss: 0.566 | Acc: 69.133% \n",
      "[epoch:9, iter:831] Loss: 0.566 | Acc: 68.645% \n",
      "[epoch:9, iter:832] Loss: 0.567 | Acc: 68.750% \n",
      "[epoch:9, iter:833] Loss: 0.565 | Acc: 68.788% \n",
      "[epoch:9, iter:834] Loss: 0.562 | Acc: 68.824% \n",
      "[epoch:9, iter:835] Loss: 0.563 | Acc: 68.514% \n",
      "[epoch:9, iter:836] Loss: 0.562 | Acc: 68.556% \n",
      "[epoch:9, iter:837] Loss: 0.560 | Acc: 68.595% \n",
      "[epoch:9, iter:838] Loss: 0.558 | Acc: 68.737% \n",
      "[epoch:9, iter:839] Loss: 0.568 | Acc: 68.718% \n",
      "[epoch:9, iter:840] Loss: 0.566 | Acc: 68.650% \n",
      "[epoch:9, iter:841] Loss: 0.566 | Acc: 68.732% \n",
      "[epoch:9, iter:842] Loss: 0.563 | Acc: 68.905% \n",
      "[epoch:9, iter:843] Loss: 0.564 | Acc: 68.791% \n",
      "[epoch:9, iter:844] Loss: 0.566 | Acc: 68.545% \n",
      "[epoch:9, iter:845] Loss: 0.566 | Acc: 68.533% \n",
      "[epoch:9, iter:846] Loss: 0.566 | Acc: 68.565% \n",
      "[epoch:9, iter:847] Loss: 0.566 | Acc: 68.426% \n",
      "[epoch:9, iter:848] Loss: 0.564 | Acc: 68.625% \n",
      "[epoch:9, iter:849] Loss: 0.566 | Acc: 68.571% \n",
      "[epoch:9, iter:850] Loss: 0.567 | Acc: 68.480% \n",
      "[epoch:9, iter:851] Loss: 0.568 | Acc: 68.510% \n",
      "[epoch:9, iter:852] Loss: 0.572 | Acc: 68.500% \n",
      "[epoch:9, iter:853] Loss: 0.575 | Acc: 68.377% \n",
      "[epoch:9, iter:854] Loss: 0.576 | Acc: 68.259% \n",
      "[epoch:9, iter:855] Loss: 0.580 | Acc: 68.327% \n",
      "[epoch:9, iter:856] Loss: 0.585 | Acc: 68.250% \n",
      "[epoch:9, iter:857] Loss: 0.583 | Acc: 68.175% \n",
      "[epoch:9, iter:858] Loss: 0.584 | Acc: 68.069% \n",
      "[epoch:9, iter:859] Loss: 0.582 | Acc: 68.373% \n",
      "[epoch:9, iter:860] Loss: 0.583 | Acc: 68.500% \n",
      "[epoch:9, iter:861] Loss: 0.582 | Acc: 68.492% \n",
      "[epoch:9, iter:862] Loss: 0.583 | Acc: 68.516% \n",
      "[epoch:9, iter:863] Loss: 0.583 | Acc: 68.508% \n",
      "[epoch:9, iter:864] Loss: 0.582 | Acc: 68.406% \n",
      "[epoch:9, iter:865] Loss: 0.581 | Acc: 68.523% \n",
      "[epoch:9, iter:866] Loss: 0.579 | Acc: 68.727% \n",
      "[epoch:9, iter:867] Loss: 0.582 | Acc: 68.716% \n",
      "[epoch:9, iter:868] Loss: 0.581 | Acc: 68.676% \n",
      "[epoch:9, iter:869] Loss: 0.585 | Acc: 68.783% \n",
      "[epoch:9, iter:870] Loss: 0.587 | Acc: 68.857% \n",
      "[epoch:9, iter:871] Loss: 0.586 | Acc: 68.958% \n",
      "[epoch:9, iter:872] Loss: 0.589 | Acc: 68.889% \n",
      "[epoch:9, iter:873] Loss: 0.588 | Acc: 68.959% \n",
      "[epoch:9, iter:874] Loss: 0.588 | Acc: 69.000% \n",
      "[epoch:9, iter:875] Loss: 0.587 | Acc: 68.987% \n",
      "[epoch:9, iter:876] Loss: 0.585 | Acc: 69.105% \n",
      "[epoch:9, iter:877] Loss: 0.586 | Acc: 69.091% \n",
      "[epoch:9, iter:878] Loss: 0.589 | Acc: 69.000% \n",
      "[epoch:9, iter:879] Loss: 0.587 | Acc: 69.063% \n",
      "[epoch:9, iter:880] Loss: 0.588 | Acc: 69.050% \n",
      "[epoch:9, iter:881] Loss: 0.587 | Acc: 69.160% \n",
      "[epoch:9, iter:882] Loss: 0.585 | Acc: 69.195% \n",
      "[epoch:9, iter:883] Loss: 0.586 | Acc: 69.012% \n",
      "[epoch:9, iter:884] Loss: 0.585 | Acc: 69.048% \n",
      "[epoch:9, iter:885] Loss: 0.585 | Acc: 69.153% \n",
      "[epoch:9, iter:886] Loss: 0.583 | Acc: 69.209% \n",
      "[epoch:9, iter:887] Loss: 0.583 | Acc: 69.241% \n",
      "[epoch:9, iter:888] Loss: 0.584 | Acc: 69.250% \n",
      "[epoch:9, iter:889] Loss: 0.583 | Acc: 69.303% \n",
      "[epoch:9, iter:890] Loss: 0.582 | Acc: 69.444% \n",
      "[epoch:9, iter:891] Loss: 0.582 | Acc: 69.560% \n",
      "[epoch:9, iter:892] Loss: 0.581 | Acc: 69.630% \n",
      "[epoch:9, iter:893] Loss: 0.581 | Acc: 69.656% \n",
      "[epoch:9, iter:894] Loss: 0.580 | Acc: 69.638% \n",
      "[epoch:9, iter:895] Loss: 0.581 | Acc: 69.579% \n",
      "[epoch:9, iter:896] Loss: 0.581 | Acc: 69.646% \n",
      "[epoch:9, iter:897] Loss: 0.580 | Acc: 69.670% \n",
      "[epoch:9, iter:898] Loss: 0.581 | Acc: 69.653% \n",
      "[epoch:9, iter:899] Loss: 0.580 | Acc: 69.636% \n",
      "[epoch:9, iter:900] Loss: 0.579 | Acc: 69.660% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.120%\n",
      "Training set's accuracy (after quantization) is: 72.980%\n",
      "Test set's accuracy (before quantization) is: 69.400%\n",
      "Test set's accuracy (after quantization) is: 73.600%\n",
      "Train Loss: 0.646 | Train Acc: 69.120% | Test Loss: 0.647 | Test Acc: 69.400% \n",
      "Quantized Train Loss: 0.540 | Quantized Train Acc: 72.980% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.600% \n",
      "\n",
      "Epoch: 10\n",
      "[epoch:10, iter:901] Loss: 0.510 | Acc: 72.000% \n",
      "[epoch:10, iter:902] Loss: 0.506 | Acc: 73.000% \n",
      "[epoch:10, iter:903] Loss: 0.522 | Acc: 72.667% \n",
      "[epoch:10, iter:904] Loss: 0.560 | Acc: 73.000% \n",
      "[epoch:10, iter:905] Loss: 0.563 | Acc: 72.000% \n",
      "[epoch:10, iter:906] Loss: 0.559 | Acc: 72.333% \n",
      "[epoch:10, iter:907] Loss: 0.573 | Acc: 71.429% \n",
      "[epoch:10, iter:908] Loss: 0.588 | Acc: 69.750% \n",
      "[epoch:10, iter:909] Loss: 0.595 | Acc: 70.000% \n",
      "[epoch:10, iter:910] Loss: 0.577 | Acc: 70.600% \n",
      "[epoch:10, iter:911] Loss: 0.583 | Acc: 69.818% \n",
      "[epoch:10, iter:912] Loss: 0.592 | Acc: 69.500% \n",
      "[epoch:10, iter:913] Loss: 0.593 | Acc: 69.385% \n",
      "[epoch:10, iter:914] Loss: 0.588 | Acc: 69.143% \n",
      "[epoch:10, iter:915] Loss: 0.577 | Acc: 69.600% \n",
      "[epoch:10, iter:916] Loss: 0.575 | Acc: 69.500% \n",
      "[epoch:10, iter:917] Loss: 0.583 | Acc: 69.176% \n",
      "[epoch:10, iter:918] Loss: 0.583 | Acc: 69.111% \n",
      "[epoch:10, iter:919] Loss: 0.589 | Acc: 69.263% \n",
      "[epoch:10, iter:920] Loss: 0.583 | Acc: 69.800% \n",
      "[epoch:10, iter:921] Loss: 0.586 | Acc: 69.714% \n",
      "[epoch:10, iter:922] Loss: 0.584 | Acc: 69.909% \n",
      "[epoch:10, iter:923] Loss: 0.583 | Acc: 70.087% \n",
      "[epoch:10, iter:924] Loss: 0.583 | Acc: 69.917% \n",
      "[epoch:10, iter:925] Loss: 0.582 | Acc: 70.000% \n",
      "[epoch:10, iter:926] Loss: 0.582 | Acc: 70.077% \n",
      "[epoch:10, iter:927] Loss: 0.580 | Acc: 69.926% \n",
      "[epoch:10, iter:928] Loss: 0.582 | Acc: 69.714% \n",
      "[epoch:10, iter:929] Loss: 0.581 | Acc: 69.586% \n",
      "[epoch:10, iter:930] Loss: 0.580 | Acc: 69.467% \n",
      "[epoch:10, iter:931] Loss: 0.581 | Acc: 69.484% \n",
      "[epoch:10, iter:932] Loss: 0.579 | Acc: 69.750% \n",
      "[epoch:10, iter:933] Loss: 0.586 | Acc: 69.455% \n",
      "[epoch:10, iter:934] Loss: 0.586 | Acc: 69.412% \n",
      "[epoch:10, iter:935] Loss: 0.586 | Acc: 69.429% \n",
      "[epoch:10, iter:936] Loss: 0.582 | Acc: 69.722% \n",
      "[epoch:10, iter:937] Loss: 0.584 | Acc: 69.405% \n",
      "[epoch:10, iter:938] Loss: 0.585 | Acc: 69.158% \n",
      "[epoch:10, iter:939] Loss: 0.585 | Acc: 69.026% \n",
      "[epoch:10, iter:940] Loss: 0.583 | Acc: 69.000% \n",
      "[epoch:10, iter:941] Loss: 0.582 | Acc: 69.122% \n",
      "[epoch:10, iter:942] Loss: 0.581 | Acc: 69.143% \n",
      "[epoch:10, iter:943] Loss: 0.580 | Acc: 69.116% \n",
      "[epoch:10, iter:944] Loss: 0.579 | Acc: 69.136% \n",
      "[epoch:10, iter:945] Loss: 0.582 | Acc: 69.111% \n",
      "[epoch:10, iter:946] Loss: 0.583 | Acc: 69.130% \n",
      "[epoch:10, iter:947] Loss: 0.583 | Acc: 69.319% \n",
      "[epoch:10, iter:948] Loss: 0.582 | Acc: 69.458% \n",
      "[epoch:10, iter:949] Loss: 0.579 | Acc: 69.633% \n",
      "[epoch:10, iter:950] Loss: 0.576 | Acc: 69.960% \n",
      "[epoch:10, iter:951] Loss: 0.576 | Acc: 69.843% \n",
      "[epoch:10, iter:952] Loss: 0.575 | Acc: 69.885% \n",
      "[epoch:10, iter:953] Loss: 0.574 | Acc: 70.038% \n",
      "[epoch:10, iter:954] Loss: 0.579 | Acc: 70.037% \n",
      "[epoch:10, iter:955] Loss: 0.579 | Acc: 69.782% \n",
      "[epoch:10, iter:956] Loss: 0.579 | Acc: 69.750% \n",
      "[epoch:10, iter:957] Loss: 0.579 | Acc: 69.614% \n",
      "[epoch:10, iter:958] Loss: 0.579 | Acc: 69.552% \n",
      "[epoch:10, iter:959] Loss: 0.579 | Acc: 69.559% \n",
      "[epoch:10, iter:960] Loss: 0.579 | Acc: 69.633% \n",
      "[epoch:10, iter:961] Loss: 0.577 | Acc: 69.738% \n",
      "[epoch:10, iter:962] Loss: 0.577 | Acc: 69.806% \n",
      "[epoch:10, iter:963] Loss: 0.577 | Acc: 69.905% \n",
      "[epoch:10, iter:964] Loss: 0.575 | Acc: 69.938% \n",
      "[epoch:10, iter:965] Loss: 0.574 | Acc: 69.938% \n",
      "[epoch:10, iter:966] Loss: 0.573 | Acc: 70.000% \n",
      "[epoch:10, iter:967] Loss: 0.574 | Acc: 69.910% \n",
      "[epoch:10, iter:968] Loss: 0.576 | Acc: 69.706% \n",
      "[epoch:10, iter:969] Loss: 0.576 | Acc: 69.681% \n",
      "[epoch:10, iter:970] Loss: 0.576 | Acc: 69.686% \n",
      "[epoch:10, iter:971] Loss: 0.574 | Acc: 69.803% \n",
      "[epoch:10, iter:972] Loss: 0.574 | Acc: 69.722% \n",
      "[epoch:10, iter:973] Loss: 0.573 | Acc: 69.753% \n",
      "[epoch:10, iter:974] Loss: 0.572 | Acc: 69.811% \n",
      "[epoch:10, iter:975] Loss: 0.571 | Acc: 69.867% \n",
      "[epoch:10, iter:976] Loss: 0.571 | Acc: 69.816% \n",
      "[epoch:10, iter:977] Loss: 0.570 | Acc: 69.896% \n",
      "[epoch:10, iter:978] Loss: 0.570 | Acc: 69.872% \n",
      "[epoch:10, iter:979] Loss: 0.570 | Acc: 69.899% \n",
      "[epoch:10, iter:980] Loss: 0.568 | Acc: 69.875% \n",
      "[epoch:10, iter:981] Loss: 0.567 | Acc: 69.877% \n",
      "[epoch:10, iter:982] Loss: 0.570 | Acc: 69.780% \n",
      "[epoch:10, iter:983] Loss: 0.572 | Acc: 69.807% \n",
      "[epoch:10, iter:984] Loss: 0.571 | Acc: 69.714% \n",
      "[epoch:10, iter:985] Loss: 0.570 | Acc: 69.765% \n",
      "[epoch:10, iter:986] Loss: 0.570 | Acc: 69.698% \n",
      "[epoch:10, iter:987] Loss: 0.570 | Acc: 69.678% \n",
      "[epoch:10, iter:988] Loss: 0.570 | Acc: 69.614% \n",
      "[epoch:10, iter:989] Loss: 0.569 | Acc: 69.685% \n",
      "[epoch:10, iter:990] Loss: 0.570 | Acc: 69.578% \n",
      "[epoch:10, iter:991] Loss: 0.569 | Acc: 69.473% \n",
      "[epoch:10, iter:992] Loss: 0.569 | Acc: 69.543% \n",
      "[epoch:10, iter:993] Loss: 0.569 | Acc: 69.527% \n",
      "[epoch:10, iter:994] Loss: 0.568 | Acc: 69.574% \n",
      "[epoch:10, iter:995] Loss: 0.568 | Acc: 69.558% \n",
      "[epoch:10, iter:996] Loss: 0.567 | Acc: 69.562% \n",
      "[epoch:10, iter:997] Loss: 0.567 | Acc: 69.608% \n",
      "[epoch:10, iter:998] Loss: 0.565 | Acc: 69.755% \n",
      "[epoch:10, iter:999] Loss: 0.566 | Acc: 69.798% \n",
      "[epoch:10, iter:1000] Loss: 0.565 | Acc: 69.920% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.080%\n",
      "Training set's accuracy (after quantization) is: 69.720%\n",
      "Test set's accuracy (before quantization) is: 69.400%\n",
      "Test set's accuracy (after quantization) is: 70.400%\n",
      "Train Loss: 0.693 | Train Acc: 69.080% | Test Loss: 0.694 | Test Acc: 69.400% \n",
      "Quantized Train Loss: 0.664 | Quantized Train Acc: 69.720% | Quantized Test Loss: 0.671 | Quantized Test Acc: 70.400% \n",
      "\n",
      "Epoch: 11\n",
      "[epoch:11, iter:1001] Loss: 0.601 | Acc: 76.000% \n",
      "[epoch:11, iter:1002] Loss: 0.527 | Acc: 77.000% \n",
      "[epoch:11, iter:1003] Loss: 0.511 | Acc: 77.333% \n",
      "[epoch:11, iter:1004] Loss: 0.529 | Acc: 75.500% \n",
      "[epoch:11, iter:1005] Loss: 0.525 | Acc: 76.000% \n",
      "[epoch:11, iter:1006] Loss: 0.532 | Acc: 74.333% \n",
      "[epoch:11, iter:1007] Loss: 0.545 | Acc: 72.000% \n",
      "[epoch:11, iter:1008] Loss: 0.552 | Acc: 70.500% \n",
      "[epoch:11, iter:1009] Loss: 0.556 | Acc: 69.778% \n",
      "[epoch:11, iter:1010] Loss: 0.560 | Acc: 70.000% \n",
      "[epoch:11, iter:1011] Loss: 0.553 | Acc: 70.727% \n",
      "[epoch:11, iter:1012] Loss: 0.545 | Acc: 71.333% \n",
      "[epoch:11, iter:1013] Loss: 0.545 | Acc: 71.692% \n",
      "[epoch:11, iter:1014] Loss: 0.535 | Acc: 71.714% \n",
      "[epoch:11, iter:1015] Loss: 0.536 | Acc: 71.733% \n",
      "[epoch:11, iter:1016] Loss: 0.537 | Acc: 71.375% \n",
      "[epoch:11, iter:1017] Loss: 0.535 | Acc: 72.118% \n",
      "[epoch:11, iter:1018] Loss: 0.545 | Acc: 71.778% \n",
      "[epoch:11, iter:1019] Loss: 0.545 | Acc: 71.158% \n",
      "[epoch:11, iter:1020] Loss: 0.548 | Acc: 71.100% \n",
      "[epoch:11, iter:1021] Loss: 0.547 | Acc: 71.238% \n",
      "[epoch:11, iter:1022] Loss: 0.546 | Acc: 71.182% \n",
      "[epoch:11, iter:1023] Loss: 0.549 | Acc: 70.609% \n",
      "[epoch:11, iter:1024] Loss: 0.547 | Acc: 70.583% \n",
      "[epoch:11, iter:1025] Loss: 0.548 | Acc: 70.560% \n",
      "[epoch:11, iter:1026] Loss: 0.550 | Acc: 70.462% \n",
      "[epoch:11, iter:1027] Loss: 0.548 | Acc: 70.370% \n",
      "[epoch:11, iter:1028] Loss: 0.544 | Acc: 70.643% \n",
      "[epoch:11, iter:1029] Loss: 0.544 | Acc: 70.759% \n",
      "[epoch:11, iter:1030] Loss: 0.549 | Acc: 70.667% \n",
      "[epoch:11, iter:1031] Loss: 0.550 | Acc: 70.516% \n",
      "[epoch:11, iter:1032] Loss: 0.549 | Acc: 70.500% \n",
      "[epoch:11, iter:1033] Loss: 0.548 | Acc: 70.545% \n",
      "[epoch:11, iter:1034] Loss: 0.546 | Acc: 70.471% \n",
      "[epoch:11, iter:1035] Loss: 0.546 | Acc: 70.457% \n",
      "[epoch:11, iter:1036] Loss: 0.547 | Acc: 70.278% \n",
      "[epoch:11, iter:1037] Loss: 0.546 | Acc: 70.541% \n",
      "[epoch:11, iter:1038] Loss: 0.548 | Acc: 70.579% \n",
      "[epoch:11, iter:1039] Loss: 0.546 | Acc: 70.513% \n",
      "[epoch:11, iter:1040] Loss: 0.545 | Acc: 70.600% \n",
      "[epoch:11, iter:1041] Loss: 0.545 | Acc: 70.537% \n",
      "[epoch:11, iter:1042] Loss: 0.549 | Acc: 70.381% \n",
      "[epoch:11, iter:1043] Loss: 0.547 | Acc: 70.512% \n",
      "[epoch:11, iter:1044] Loss: 0.546 | Acc: 70.409% \n",
      "[epoch:11, iter:1045] Loss: 0.545 | Acc: 70.400% \n",
      "[epoch:11, iter:1046] Loss: 0.545 | Acc: 70.435% \n",
      "[epoch:11, iter:1047] Loss: 0.547 | Acc: 70.298% \n",
      "[epoch:11, iter:1048] Loss: 0.545 | Acc: 70.542% \n",
      "[epoch:11, iter:1049] Loss: 0.544 | Acc: 70.490% \n",
      "[epoch:11, iter:1050] Loss: 0.545 | Acc: 70.440% \n",
      "[epoch:11, iter:1051] Loss: 0.546 | Acc: 70.431% \n",
      "[epoch:11, iter:1052] Loss: 0.544 | Acc: 70.500% \n",
      "[epoch:11, iter:1053] Loss: 0.545 | Acc: 70.340% \n",
      "[epoch:11, iter:1054] Loss: 0.546 | Acc: 70.111% \n",
      "[epoch:11, iter:1055] Loss: 0.547 | Acc: 69.964% \n",
      "[epoch:11, iter:1056] Loss: 0.550 | Acc: 70.071% \n",
      "[epoch:11, iter:1057] Loss: 0.561 | Acc: 69.930% \n",
      "[epoch:11, iter:1058] Loss: 0.565 | Acc: 70.138% \n",
      "[epoch:11, iter:1059] Loss: 0.569 | Acc: 70.203% \n",
      "[epoch:11, iter:1060] Loss: 0.567 | Acc: 70.433% \n",
      "[epoch:11, iter:1061] Loss: 0.566 | Acc: 70.492% \n",
      "[epoch:11, iter:1062] Loss: 0.573 | Acc: 70.452% \n",
      "[epoch:11, iter:1063] Loss: 0.573 | Acc: 70.381% \n",
      "[epoch:11, iter:1064] Loss: 0.574 | Acc: 70.312% \n",
      "[epoch:11, iter:1065] Loss: 0.572 | Acc: 70.400% \n",
      "[epoch:11, iter:1066] Loss: 0.572 | Acc: 70.242% \n",
      "[epoch:11, iter:1067] Loss: 0.571 | Acc: 70.239% \n",
      "[epoch:11, iter:1068] Loss: 0.573 | Acc: 70.029% \n",
      "[epoch:11, iter:1069] Loss: 0.573 | Acc: 69.942% \n",
      "[epoch:11, iter:1070] Loss: 0.572 | Acc: 69.971% \n",
      "[epoch:11, iter:1071] Loss: 0.571 | Acc: 70.056% \n",
      "[epoch:11, iter:1072] Loss: 0.571 | Acc: 70.083% \n",
      "[epoch:11, iter:1073] Loss: 0.570 | Acc: 70.137% \n",
      "[epoch:11, iter:1074] Loss: 0.569 | Acc: 70.243% \n",
      "[epoch:11, iter:1075] Loss: 0.568 | Acc: 70.293% \n",
      "[epoch:11, iter:1076] Loss: 0.567 | Acc: 70.342% \n",
      "[epoch:11, iter:1077] Loss: 0.566 | Acc: 70.416% \n",
      "[epoch:11, iter:1078] Loss: 0.571 | Acc: 70.282% \n",
      "[epoch:11, iter:1079] Loss: 0.569 | Acc: 70.380% \n",
      "[epoch:11, iter:1080] Loss: 0.569 | Acc: 70.300% \n",
      "[epoch:11, iter:1081] Loss: 0.569 | Acc: 70.272% \n",
      "[epoch:11, iter:1082] Loss: 0.568 | Acc: 70.390% \n",
      "[epoch:11, iter:1083] Loss: 0.568 | Acc: 70.410% \n",
      "[epoch:11, iter:1084] Loss: 0.569 | Acc: 70.238% \n",
      "[epoch:11, iter:1085] Loss: 0.569 | Acc: 70.141% \n",
      "[epoch:11, iter:1086] Loss: 0.569 | Acc: 70.116% \n",
      "[epoch:11, iter:1087] Loss: 0.568 | Acc: 70.138% \n",
      "[epoch:11, iter:1088] Loss: 0.568 | Acc: 70.114% \n",
      "[epoch:11, iter:1089] Loss: 0.568 | Acc: 70.067% \n",
      "[epoch:11, iter:1090] Loss: 0.568 | Acc: 70.111% \n",
      "[epoch:11, iter:1091] Loss: 0.568 | Acc: 70.110% \n",
      "[epoch:11, iter:1092] Loss: 0.568 | Acc: 70.087% \n",
      "[epoch:11, iter:1093] Loss: 0.567 | Acc: 70.043% \n",
      "[epoch:11, iter:1094] Loss: 0.566 | Acc: 70.106% \n",
      "[epoch:11, iter:1095] Loss: 0.567 | Acc: 70.021% \n",
      "[epoch:11, iter:1096] Loss: 0.567 | Acc: 69.979% \n",
      "[epoch:11, iter:1097] Loss: 0.567 | Acc: 69.897% \n",
      "[epoch:11, iter:1098] Loss: 0.567 | Acc: 69.857% \n",
      "[epoch:11, iter:1099] Loss: 0.567 | Acc: 69.899% \n",
      "[epoch:11, iter:1100] Loss: 0.568 | Acc: 69.800% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 70.680%\n",
      "Training set's accuracy (after quantization) is: 72.440%\n",
      "Test set's accuracy (before quantization) is: 71.800%\n",
      "Test set's accuracy (after quantization) is: 72.200%\n",
      "Train Loss: 0.561 | Train Acc: 70.680% | Test Loss: 0.562 | Test Acc: 71.800% \n",
      "Quantized Train Loss: 0.545 | Quantized Train Acc: 72.440% | Quantized Test Loss: 0.548 | Quantized Test Acc: 72.200% \n",
      "\n",
      "Epoch: 12\n",
      "[epoch:12, iter:1101] Loss: 0.581 | Acc: 62.000% \n",
      "[epoch:12, iter:1102] Loss: 0.560 | Acc: 71.000% \n",
      "[epoch:12, iter:1103] Loss: 0.555 | Acc: 69.333% \n",
      "[epoch:12, iter:1104] Loss: 0.535 | Acc: 71.500% \n",
      "[epoch:12, iter:1105] Loss: 0.526 | Acc: 71.200% \n",
      "[epoch:12, iter:1106] Loss: 0.527 | Acc: 70.667% \n",
      "[epoch:12, iter:1107] Loss: 0.523 | Acc: 70.857% \n",
      "[epoch:12, iter:1108] Loss: 0.524 | Acc: 71.000% \n",
      "[epoch:12, iter:1109] Loss: 0.557 | Acc: 71.111% \n",
      "[epoch:12, iter:1110] Loss: 0.556 | Acc: 71.400% \n",
      "[epoch:12, iter:1111] Loss: 0.562 | Acc: 70.000% \n",
      "[epoch:12, iter:1112] Loss: 0.561 | Acc: 69.667% \n",
      "[epoch:12, iter:1113] Loss: 0.559 | Acc: 70.154% \n",
      "[epoch:12, iter:1114] Loss: 0.561 | Acc: 69.143% \n",
      "[epoch:12, iter:1115] Loss: 0.570 | Acc: 68.800% \n",
      "[epoch:12, iter:1116] Loss: 0.568 | Acc: 68.500% \n",
      "[epoch:12, iter:1117] Loss: 0.573 | Acc: 68.353% \n",
      "[epoch:12, iter:1118] Loss: 0.573 | Acc: 68.333% \n",
      "[epoch:12, iter:1119] Loss: 0.567 | Acc: 68.421% \n",
      "[epoch:12, iter:1120] Loss: 0.564 | Acc: 68.900% \n",
      "[epoch:12, iter:1121] Loss: 0.566 | Acc: 68.762% \n",
      "[epoch:12, iter:1122] Loss: 0.569 | Acc: 68.818% \n",
      "[epoch:12, iter:1123] Loss: 0.565 | Acc: 69.130% \n",
      "[epoch:12, iter:1124] Loss: 0.560 | Acc: 69.500% \n",
      "[epoch:12, iter:1125] Loss: 0.557 | Acc: 69.680% \n",
      "[epoch:12, iter:1126] Loss: 0.561 | Acc: 69.308% \n",
      "[epoch:12, iter:1127] Loss: 0.561 | Acc: 69.556% \n",
      "[epoch:12, iter:1128] Loss: 0.574 | Acc: 69.500% \n",
      "[epoch:12, iter:1129] Loss: 0.575 | Acc: 69.655% \n",
      "[epoch:12, iter:1130] Loss: 0.578 | Acc: 69.000% \n",
      "[epoch:12, iter:1131] Loss: 0.576 | Acc: 69.097% \n",
      "[epoch:12, iter:1132] Loss: 0.575 | Acc: 69.062% \n",
      "[epoch:12, iter:1133] Loss: 0.578 | Acc: 68.788% \n",
      "[epoch:12, iter:1134] Loss: 0.578 | Acc: 68.706% \n",
      "[epoch:12, iter:1135] Loss: 0.576 | Acc: 68.914% \n",
      "[epoch:12, iter:1136] Loss: 0.575 | Acc: 69.000% \n",
      "[epoch:12, iter:1137] Loss: 0.578 | Acc: 68.973% \n",
      "[epoch:12, iter:1138] Loss: 0.576 | Acc: 69.000% \n",
      "[epoch:12, iter:1139] Loss: 0.575 | Acc: 69.077% \n",
      "[epoch:12, iter:1140] Loss: 0.572 | Acc: 69.050% \n",
      "[epoch:12, iter:1141] Loss: 0.572 | Acc: 69.073% \n",
      "[epoch:12, iter:1142] Loss: 0.570 | Acc: 69.333% \n",
      "[epoch:12, iter:1143] Loss: 0.574 | Acc: 69.256% \n",
      "[epoch:12, iter:1144] Loss: 0.573 | Acc: 69.409% \n",
      "[epoch:12, iter:1145] Loss: 0.573 | Acc: 69.422% \n",
      "[epoch:12, iter:1146] Loss: 0.571 | Acc: 69.435% \n",
      "[epoch:12, iter:1147] Loss: 0.571 | Acc: 69.277% \n",
      "[epoch:12, iter:1148] Loss: 0.568 | Acc: 69.542% \n",
      "[epoch:12, iter:1149] Loss: 0.567 | Acc: 69.510% \n",
      "[epoch:12, iter:1150] Loss: 0.567 | Acc: 69.440% \n",
      "[epoch:12, iter:1151] Loss: 0.567 | Acc: 69.294% \n",
      "[epoch:12, iter:1152] Loss: 0.567 | Acc: 69.154% \n",
      "[epoch:12, iter:1153] Loss: 0.566 | Acc: 68.943% \n",
      "[epoch:12, iter:1154] Loss: 0.565 | Acc: 69.074% \n",
      "[epoch:12, iter:1155] Loss: 0.565 | Acc: 69.018% \n",
      "[epoch:12, iter:1156] Loss: 0.566 | Acc: 68.857% \n",
      "[epoch:12, iter:1157] Loss: 0.565 | Acc: 68.947% \n",
      "[epoch:12, iter:1158] Loss: 0.564 | Acc: 68.966% \n",
      "[epoch:12, iter:1159] Loss: 0.563 | Acc: 69.119% \n",
      "[epoch:12, iter:1160] Loss: 0.562 | Acc: 69.233% \n",
      "[epoch:12, iter:1161] Loss: 0.563 | Acc: 69.311% \n",
      "[epoch:12, iter:1162] Loss: 0.561 | Acc: 69.387% \n",
      "[epoch:12, iter:1163] Loss: 0.560 | Acc: 69.365% \n",
      "[epoch:12, iter:1164] Loss: 0.560 | Acc: 69.281% \n",
      "[epoch:12, iter:1165] Loss: 0.561 | Acc: 69.323% \n",
      "[epoch:12, iter:1166] Loss: 0.562 | Acc: 69.152% \n",
      "[epoch:12, iter:1167] Loss: 0.562 | Acc: 69.104% \n",
      "[epoch:12, iter:1168] Loss: 0.565 | Acc: 68.882% \n",
      "[epoch:12, iter:1169] Loss: 0.567 | Acc: 68.928% \n",
      "[epoch:12, iter:1170] Loss: 0.568 | Acc: 69.086% \n",
      "[epoch:12, iter:1171] Loss: 0.566 | Acc: 69.268% \n",
      "[epoch:12, iter:1172] Loss: 0.565 | Acc: 69.278% \n",
      "[epoch:12, iter:1173] Loss: 0.564 | Acc: 69.260% \n",
      "[epoch:12, iter:1174] Loss: 0.564 | Acc: 69.189% \n",
      "[epoch:12, iter:1175] Loss: 0.564 | Acc: 69.173% \n",
      "[epoch:12, iter:1176] Loss: 0.563 | Acc: 69.316% \n",
      "[epoch:12, iter:1177] Loss: 0.563 | Acc: 69.455% \n",
      "[epoch:12, iter:1178] Loss: 0.563 | Acc: 69.436% \n",
      "[epoch:12, iter:1179] Loss: 0.562 | Acc: 69.544% \n",
      "[epoch:12, iter:1180] Loss: 0.565 | Acc: 69.525% \n",
      "[epoch:12, iter:1181] Loss: 0.565 | Acc: 69.457% \n",
      "[epoch:12, iter:1182] Loss: 0.565 | Acc: 69.561% \n",
      "[epoch:12, iter:1183] Loss: 0.564 | Acc: 69.590% \n",
      "[epoch:12, iter:1184] Loss: 0.564 | Acc: 69.476% \n",
      "[epoch:12, iter:1185] Loss: 0.565 | Acc: 69.341% \n",
      "[epoch:12, iter:1186] Loss: 0.565 | Acc: 69.395% \n",
      "[epoch:12, iter:1187] Loss: 0.564 | Acc: 69.310% \n",
      "[epoch:12, iter:1188] Loss: 0.564 | Acc: 69.295% \n",
      "[epoch:12, iter:1189] Loss: 0.564 | Acc: 69.393% \n",
      "[epoch:12, iter:1190] Loss: 0.564 | Acc: 69.311% \n",
      "[epoch:12, iter:1191] Loss: 0.564 | Acc: 69.319% \n",
      "[epoch:12, iter:1192] Loss: 0.563 | Acc: 69.348% \n",
      "[epoch:12, iter:1193] Loss: 0.562 | Acc: 69.419% \n",
      "[epoch:12, iter:1194] Loss: 0.561 | Acc: 69.447% \n",
      "[epoch:12, iter:1195] Loss: 0.560 | Acc: 69.495% \n",
      "[epoch:12, iter:1196] Loss: 0.561 | Acc: 69.479% \n",
      "[epoch:12, iter:1197] Loss: 0.562 | Acc: 69.381% \n",
      "[epoch:12, iter:1198] Loss: 0.562 | Acc: 69.388% \n",
      "[epoch:12, iter:1199] Loss: 0.563 | Acc: 69.495% \n",
      "[epoch:12, iter:1200] Loss: 0.564 | Acc: 69.460% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.320%\n",
      "Training set's accuracy (after quantization) is: 72.900%\n",
      "Test set's accuracy (before quantization) is: 69.800%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.614 | Train Acc: 69.320% | Test Loss: 0.616 | Test Acc: 69.800% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.900% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 13\n",
      "[epoch:13, iter:1201] Loss: 0.552 | Acc: 68.000% \n",
      "[epoch:13, iter:1202] Loss: 0.601 | Acc: 65.000% \n",
      "[epoch:13, iter:1203] Loss: 0.563 | Acc: 68.667% \n",
      "[epoch:13, iter:1204] Loss: 0.541 | Acc: 70.000% \n",
      "[epoch:13, iter:1205] Loss: 0.528 | Acc: 70.400% \n",
      "[epoch:13, iter:1206] Loss: 0.544 | Acc: 69.000% \n",
      "[epoch:13, iter:1207] Loss: 0.536 | Acc: 68.857% \n",
      "[epoch:13, iter:1208] Loss: 0.535 | Acc: 68.750% \n",
      "[epoch:13, iter:1209] Loss: 0.545 | Acc: 67.778% \n",
      "[epoch:13, iter:1210] Loss: 0.553 | Acc: 67.400% \n",
      "[epoch:13, iter:1211] Loss: 0.561 | Acc: 66.545% \n",
      "[epoch:13, iter:1212] Loss: 0.572 | Acc: 67.333% \n",
      "[epoch:13, iter:1213] Loss: 0.576 | Acc: 66.923% \n",
      "[epoch:13, iter:1214] Loss: 0.568 | Acc: 67.857% \n",
      "[epoch:13, iter:1215] Loss: 0.605 | Acc: 67.467% \n",
      "[epoch:13, iter:1216] Loss: 0.610 | Acc: 67.125% \n",
      "[epoch:13, iter:1217] Loss: 0.606 | Acc: 67.765% \n",
      "[epoch:13, iter:1218] Loss: 0.601 | Acc: 68.111% \n",
      "[epoch:13, iter:1219] Loss: 0.601 | Acc: 67.579% \n",
      "[epoch:13, iter:1220] Loss: 0.597 | Acc: 67.400% \n",
      "[epoch:13, iter:1221] Loss: 0.598 | Acc: 67.238% \n",
      "[epoch:13, iter:1222] Loss: 0.593 | Acc: 67.091% \n",
      "[epoch:13, iter:1223] Loss: 0.589 | Acc: 67.217% \n",
      "[epoch:13, iter:1224] Loss: 0.589 | Acc: 67.417% \n",
      "[epoch:13, iter:1225] Loss: 0.587 | Acc: 67.520% \n",
      "[epoch:13, iter:1226] Loss: 0.585 | Acc: 67.308% \n",
      "[epoch:13, iter:1227] Loss: 0.584 | Acc: 67.333% \n",
      "[epoch:13, iter:1228] Loss: 0.585 | Acc: 67.357% \n",
      "[epoch:13, iter:1229] Loss: 0.585 | Acc: 67.310% \n",
      "[epoch:13, iter:1230] Loss: 0.582 | Acc: 67.733% \n",
      "[epoch:13, iter:1231] Loss: 0.580 | Acc: 67.935% \n",
      "[epoch:13, iter:1232] Loss: 0.583 | Acc: 67.688% \n",
      "[epoch:13, iter:1233] Loss: 0.581 | Acc: 67.879% \n",
      "[epoch:13, iter:1234] Loss: 0.578 | Acc: 68.059% \n",
      "[epoch:13, iter:1235] Loss: 0.572 | Acc: 68.343% \n",
      "[epoch:13, iter:1236] Loss: 0.570 | Acc: 68.444% \n",
      "[epoch:13, iter:1237] Loss: 0.571 | Acc: 68.270% \n",
      "[epoch:13, iter:1238] Loss: 0.569 | Acc: 68.421% \n",
      "[epoch:13, iter:1239] Loss: 0.570 | Acc: 68.308% \n",
      "[epoch:13, iter:1240] Loss: 0.569 | Acc: 68.500% \n",
      "[epoch:13, iter:1241] Loss: 0.570 | Acc: 68.585% \n",
      "[epoch:13, iter:1242] Loss: 0.569 | Acc: 68.667% \n",
      "[epoch:13, iter:1243] Loss: 0.570 | Acc: 68.326% \n",
      "[epoch:13, iter:1244] Loss: 0.569 | Acc: 68.318% \n",
      "[epoch:13, iter:1245] Loss: 0.568 | Acc: 68.400% \n",
      "[epoch:13, iter:1246] Loss: 0.568 | Acc: 68.478% \n",
      "[epoch:13, iter:1247] Loss: 0.566 | Acc: 68.723% \n",
      "[epoch:13, iter:1248] Loss: 0.565 | Acc: 68.833% \n",
      "[epoch:13, iter:1249] Loss: 0.565 | Acc: 68.816% \n",
      "[epoch:13, iter:1250] Loss: 0.564 | Acc: 68.800% \n",
      "[epoch:13, iter:1251] Loss: 0.562 | Acc: 68.824% \n",
      "[epoch:13, iter:1252] Loss: 0.564 | Acc: 68.654% \n",
      "[epoch:13, iter:1253] Loss: 0.565 | Acc: 68.566% \n",
      "[epoch:13, iter:1254] Loss: 0.565 | Acc: 68.519% \n",
      "[epoch:13, iter:1255] Loss: 0.565 | Acc: 68.655% \n",
      "[epoch:13, iter:1256] Loss: 0.562 | Acc: 68.857% \n",
      "[epoch:13, iter:1257] Loss: 0.563 | Acc: 68.982% \n",
      "[epoch:13, iter:1258] Loss: 0.562 | Acc: 69.069% \n",
      "[epoch:13, iter:1259] Loss: 0.561 | Acc: 69.153% \n",
      "[epoch:13, iter:1260] Loss: 0.562 | Acc: 69.233% \n",
      "[epoch:13, iter:1261] Loss: 0.562 | Acc: 69.377% \n",
      "[epoch:13, iter:1262] Loss: 0.561 | Acc: 69.419% \n",
      "[epoch:13, iter:1263] Loss: 0.558 | Acc: 69.492% \n",
      "[epoch:13, iter:1264] Loss: 0.561 | Acc: 69.312% \n",
      "[epoch:13, iter:1265] Loss: 0.560 | Acc: 69.231% \n",
      "[epoch:13, iter:1266] Loss: 0.559 | Acc: 69.394% \n",
      "[epoch:13, iter:1267] Loss: 0.566 | Acc: 69.343% \n",
      "[epoch:13, iter:1268] Loss: 0.566 | Acc: 69.441% \n",
      "[epoch:13, iter:1269] Loss: 0.566 | Acc: 69.304% \n",
      "[epoch:13, iter:1270] Loss: 0.565 | Acc: 69.514% \n",
      "[epoch:13, iter:1271] Loss: 0.567 | Acc: 69.437% \n",
      "[epoch:13, iter:1272] Loss: 0.567 | Acc: 69.500% \n",
      "[epoch:13, iter:1273] Loss: 0.566 | Acc: 69.479% \n",
      "[epoch:13, iter:1274] Loss: 0.569 | Acc: 69.351% \n",
      "[epoch:13, iter:1275] Loss: 0.571 | Acc: 69.387% \n",
      "[epoch:13, iter:1276] Loss: 0.570 | Acc: 69.447% \n",
      "[epoch:13, iter:1277] Loss: 0.572 | Acc: 69.299% \n",
      "[epoch:13, iter:1278] Loss: 0.571 | Acc: 69.436% \n",
      "[epoch:13, iter:1279] Loss: 0.570 | Acc: 69.468% \n",
      "[epoch:13, iter:1280] Loss: 0.571 | Acc: 69.475% \n",
      "[epoch:13, iter:1281] Loss: 0.572 | Acc: 69.481% \n",
      "[epoch:13, iter:1282] Loss: 0.571 | Acc: 69.537% \n",
      "[epoch:13, iter:1283] Loss: 0.570 | Acc: 69.566% \n",
      "[epoch:13, iter:1284] Loss: 0.569 | Acc: 69.571% \n",
      "[epoch:13, iter:1285] Loss: 0.573 | Acc: 69.553% \n",
      "[epoch:13, iter:1286] Loss: 0.575 | Acc: 69.442% \n",
      "[epoch:13, iter:1287] Loss: 0.575 | Acc: 69.264% \n",
      "[epoch:13, iter:1288] Loss: 0.576 | Acc: 69.068% \n",
      "[epoch:13, iter:1289] Loss: 0.575 | Acc: 69.169% \n",
      "[epoch:13, iter:1290] Loss: 0.574 | Acc: 69.178% \n",
      "[epoch:13, iter:1291] Loss: 0.573 | Acc: 69.231% \n",
      "[epoch:13, iter:1292] Loss: 0.572 | Acc: 69.326% \n",
      "[epoch:13, iter:1293] Loss: 0.572 | Acc: 69.376% \n",
      "[epoch:13, iter:1294] Loss: 0.572 | Acc: 69.362% \n",
      "[epoch:13, iter:1295] Loss: 0.571 | Acc: 69.537% \n",
      "[epoch:13, iter:1296] Loss: 0.575 | Acc: 69.521% \n",
      "[epoch:13, iter:1297] Loss: 0.577 | Acc: 69.505% \n",
      "[epoch:13, iter:1298] Loss: 0.576 | Acc: 69.490% \n",
      "[epoch:13, iter:1299] Loss: 0.576 | Acc: 69.414% \n",
      "[epoch:13, iter:1300] Loss: 0.575 | Acc: 69.400% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.740%\n",
      "Training set's accuracy (after quantization) is: 72.780%\n",
      "Test set's accuracy (before quantization) is: 70.100%\n",
      "Test set's accuracy (after quantization) is: 73.100%\n",
      "Train Loss: 0.588 | Train Acc: 69.740% | Test Loss: 0.589 | Test Acc: 70.100% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.780% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.100% \n",
      "\n",
      "Epoch: 14\n",
      "[epoch:14, iter:1301] Loss: 0.508 | Acc: 78.000% \n",
      "[epoch:14, iter:1302] Loss: 0.565 | Acc: 71.000% \n",
      "[epoch:14, iter:1303] Loss: 0.520 | Acc: 69.333% \n",
      "[epoch:14, iter:1304] Loss: 0.541 | Acc: 68.000% \n",
      "[epoch:14, iter:1305] Loss: 0.547 | Acc: 68.400% \n",
      "[epoch:14, iter:1306] Loss: 0.537 | Acc: 69.000% \n",
      "[epoch:14, iter:1307] Loss: 0.527 | Acc: 69.143% \n",
      "[epoch:14, iter:1308] Loss: 0.532 | Acc: 68.250% \n",
      "[epoch:14, iter:1309] Loss: 0.543 | Acc: 67.778% \n",
      "[epoch:14, iter:1310] Loss: 0.545 | Acc: 67.200% \n",
      "[epoch:14, iter:1311] Loss: 0.551 | Acc: 67.636% \n",
      "[epoch:14, iter:1312] Loss: 0.541 | Acc: 68.000% \n",
      "[epoch:14, iter:1313] Loss: 0.542 | Acc: 67.846% \n",
      "[epoch:14, iter:1314] Loss: 0.549 | Acc: 67.286% \n",
      "[epoch:14, iter:1315] Loss: 0.549 | Acc: 67.200% \n",
      "[epoch:14, iter:1316] Loss: 0.546 | Acc: 67.375% \n",
      "[epoch:14, iter:1317] Loss: 0.549 | Acc: 67.176% \n",
      "[epoch:14, iter:1318] Loss: 0.548 | Acc: 67.333% \n",
      "[epoch:14, iter:1319] Loss: 0.546 | Acc: 67.684% \n",
      "[epoch:14, iter:1320] Loss: 0.550 | Acc: 67.800% \n",
      "[epoch:14, iter:1321] Loss: 0.554 | Acc: 67.905% \n",
      "[epoch:14, iter:1322] Loss: 0.557 | Acc: 68.000% \n",
      "[epoch:14, iter:1323] Loss: 0.553 | Acc: 68.435% \n",
      "[epoch:14, iter:1324] Loss: 0.552 | Acc: 68.500% \n",
      "[epoch:14, iter:1325] Loss: 0.568 | Acc: 67.760% \n",
      "[epoch:14, iter:1326] Loss: 0.564 | Acc: 68.231% \n",
      "[epoch:14, iter:1327] Loss: 0.561 | Acc: 68.296% \n",
      "[epoch:14, iter:1328] Loss: 0.561 | Acc: 68.571% \n",
      "[epoch:14, iter:1329] Loss: 0.563 | Acc: 68.483% \n",
      "[epoch:14, iter:1330] Loss: 0.564 | Acc: 68.333% \n",
      "[epoch:14, iter:1331] Loss: 0.560 | Acc: 68.645% \n",
      "[epoch:14, iter:1332] Loss: 0.562 | Acc: 68.750% \n",
      "[epoch:14, iter:1333] Loss: 0.561 | Acc: 68.788% \n",
      "[epoch:14, iter:1334] Loss: 0.560 | Acc: 68.941% \n",
      "[epoch:14, iter:1335] Loss: 0.557 | Acc: 68.971% \n",
      "[epoch:14, iter:1336] Loss: 0.556 | Acc: 69.056% \n",
      "[epoch:14, iter:1337] Loss: 0.557 | Acc: 69.189% \n",
      "[epoch:14, iter:1338] Loss: 0.559 | Acc: 69.211% \n",
      "[epoch:14, iter:1339] Loss: 0.557 | Acc: 69.231% \n",
      "[epoch:14, iter:1340] Loss: 0.558 | Acc: 69.050% \n",
      "[epoch:14, iter:1341] Loss: 0.555 | Acc: 69.268% \n",
      "[epoch:14, iter:1342] Loss: 0.556 | Acc: 69.190% \n",
      "[epoch:14, iter:1343] Loss: 0.554 | Acc: 69.395% \n",
      "[epoch:14, iter:1344] Loss: 0.553 | Acc: 69.409% \n",
      "[epoch:14, iter:1345] Loss: 0.552 | Acc: 69.200% \n",
      "[epoch:14, iter:1346] Loss: 0.551 | Acc: 69.435% \n",
      "[epoch:14, iter:1347] Loss: 0.550 | Acc: 69.574% \n",
      "[epoch:14, iter:1348] Loss: 0.557 | Acc: 69.583% \n",
      "[epoch:14, iter:1349] Loss: 0.558 | Acc: 69.633% \n",
      "[epoch:14, iter:1350] Loss: 0.557 | Acc: 69.760% \n",
      "[epoch:14, iter:1351] Loss: 0.559 | Acc: 69.765% \n",
      "[epoch:14, iter:1352] Loss: 0.572 | Acc: 69.538% \n",
      "[epoch:14, iter:1353] Loss: 0.572 | Acc: 69.396% \n",
      "[epoch:14, iter:1354] Loss: 0.572 | Acc: 69.333% \n",
      "[epoch:14, iter:1355] Loss: 0.572 | Acc: 69.455% \n",
      "[epoch:14, iter:1356] Loss: 0.571 | Acc: 69.643% \n",
      "[epoch:14, iter:1357] Loss: 0.571 | Acc: 69.719% \n",
      "[epoch:14, iter:1358] Loss: 0.572 | Acc: 69.621% \n",
      "[epoch:14, iter:1359] Loss: 0.571 | Acc: 69.627% \n",
      "[epoch:14, iter:1360] Loss: 0.571 | Acc: 69.600% \n",
      "[epoch:14, iter:1361] Loss: 0.574 | Acc: 69.475% \n",
      "[epoch:14, iter:1362] Loss: 0.573 | Acc: 69.645% \n",
      "[epoch:14, iter:1363] Loss: 0.574 | Acc: 69.556% \n",
      "[epoch:14, iter:1364] Loss: 0.575 | Acc: 69.469% \n",
      "[epoch:14, iter:1365] Loss: 0.574 | Acc: 69.446% \n",
      "[epoch:14, iter:1366] Loss: 0.575 | Acc: 69.303% \n",
      "[epoch:14, iter:1367] Loss: 0.573 | Acc: 69.433% \n",
      "[epoch:14, iter:1368] Loss: 0.571 | Acc: 69.529% \n",
      "[epoch:14, iter:1369] Loss: 0.572 | Acc: 69.536% \n",
      "[epoch:14, iter:1370] Loss: 0.571 | Acc: 69.543% \n",
      "[epoch:14, iter:1371] Loss: 0.571 | Acc: 69.549% \n",
      "[epoch:14, iter:1372] Loss: 0.570 | Acc: 69.500% \n",
      "[epoch:14, iter:1373] Loss: 0.571 | Acc: 69.370% \n",
      "[epoch:14, iter:1374] Loss: 0.570 | Acc: 69.459% \n",
      "[epoch:14, iter:1375] Loss: 0.570 | Acc: 69.440% \n",
      "[epoch:14, iter:1376] Loss: 0.570 | Acc: 69.316% \n",
      "[epoch:14, iter:1377] Loss: 0.567 | Acc: 69.429% \n",
      "[epoch:14, iter:1378] Loss: 0.566 | Acc: 69.538% \n",
      "[epoch:14, iter:1379] Loss: 0.566 | Acc: 69.468% \n",
      "[epoch:14, iter:1380] Loss: 0.565 | Acc: 69.500% \n",
      "[epoch:14, iter:1381] Loss: 0.566 | Acc: 69.457% \n",
      "[epoch:14, iter:1382] Loss: 0.567 | Acc: 69.463% \n",
      "[epoch:14, iter:1383] Loss: 0.564 | Acc: 69.614% \n",
      "[epoch:14, iter:1384] Loss: 0.565 | Acc: 69.643% \n",
      "[epoch:14, iter:1385] Loss: 0.566 | Acc: 69.624% \n",
      "[epoch:14, iter:1386] Loss: 0.566 | Acc: 69.558% \n",
      "[epoch:14, iter:1387] Loss: 0.565 | Acc: 69.678% \n",
      "[epoch:14, iter:1388] Loss: 0.565 | Acc: 69.727% \n",
      "[epoch:14, iter:1389] Loss: 0.565 | Acc: 69.798% \n",
      "[epoch:14, iter:1390] Loss: 0.564 | Acc: 69.778% \n",
      "[epoch:14, iter:1391] Loss: 0.563 | Acc: 69.736% \n",
      "[epoch:14, iter:1392] Loss: 0.562 | Acc: 69.848% \n",
      "[epoch:14, iter:1393] Loss: 0.563 | Acc: 69.656% \n",
      "[epoch:14, iter:1394] Loss: 0.562 | Acc: 69.745% \n",
      "[epoch:14, iter:1395] Loss: 0.563 | Acc: 69.705% \n",
      "[epoch:14, iter:1396] Loss: 0.563 | Acc: 69.667% \n",
      "[epoch:14, iter:1397] Loss: 0.563 | Acc: 69.588% \n",
      "[epoch:14, iter:1398] Loss: 0.562 | Acc: 69.653% \n",
      "[epoch:14, iter:1399] Loss: 0.561 | Acc: 69.778% \n",
      "[epoch:14, iter:1400] Loss: 0.562 | Acc: 69.780% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.060%\n",
      "Training set's accuracy (after quantization) is: 72.900%\n",
      "Test set's accuracy (before quantization) is: 69.400%\n",
      "Test set's accuracy (after quantization) is: 73.300%\n",
      "Train Loss: 0.674 | Train Acc: 69.060% | Test Loss: 0.675 | Test Acc: 69.400% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.900% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.300% \n",
      "\n",
      "Epoch: 15\n",
      "[epoch:15, iter:1401] Loss: 0.579 | Acc: 64.000% \n",
      "[epoch:15, iter:1402] Loss: 0.601 | Acc: 65.000% \n",
      "[epoch:15, iter:1403] Loss: 0.589 | Acc: 67.333% \n",
      "[epoch:15, iter:1404] Loss: 0.562 | Acc: 70.500% \n",
      "[epoch:15, iter:1405] Loss: 0.567 | Acc: 70.000% \n",
      "[epoch:15, iter:1406] Loss: 0.571 | Acc: 70.667% \n",
      "[epoch:15, iter:1407] Loss: 0.588 | Acc: 69.429% \n",
      "[epoch:15, iter:1408] Loss: 0.582 | Acc: 69.250% \n",
      "[epoch:15, iter:1409] Loss: 0.575 | Acc: 70.444% \n",
      "[epoch:15, iter:1410] Loss: 0.573 | Acc: 70.200% \n",
      "[epoch:15, iter:1411] Loss: 0.575 | Acc: 69.455% \n",
      "[epoch:15, iter:1412] Loss: 0.583 | Acc: 69.833% \n",
      "[epoch:15, iter:1413] Loss: 0.622 | Acc: 69.385% \n",
      "[epoch:15, iter:1414] Loss: 0.615 | Acc: 69.714% \n",
      "[epoch:15, iter:1415] Loss: 0.613 | Acc: 69.333% \n",
      "[epoch:15, iter:1416] Loss: 0.602 | Acc: 69.750% \n",
      "[epoch:15, iter:1417] Loss: 0.597 | Acc: 69.882% \n",
      "[epoch:15, iter:1418] Loss: 0.604 | Acc: 69.556% \n",
      "[epoch:15, iter:1419] Loss: 0.599 | Acc: 70.000% \n",
      "[epoch:15, iter:1420] Loss: 0.598 | Acc: 70.000% \n",
      "[epoch:15, iter:1421] Loss: 0.600 | Acc: 70.095% \n",
      "[epoch:15, iter:1422] Loss: 0.595 | Acc: 70.091% \n",
      "[epoch:15, iter:1423] Loss: 0.589 | Acc: 70.435% \n",
      "[epoch:15, iter:1424] Loss: 0.583 | Acc: 71.083% \n",
      "[epoch:15, iter:1425] Loss: 0.580 | Acc: 71.280% \n",
      "[epoch:15, iter:1426] Loss: 0.579 | Acc: 71.077% \n",
      "[epoch:15, iter:1427] Loss: 0.580 | Acc: 71.111% \n",
      "[epoch:15, iter:1428] Loss: 0.580 | Acc: 71.214% \n",
      "[epoch:15, iter:1429] Loss: 0.581 | Acc: 70.483% \n",
      "[epoch:15, iter:1430] Loss: 0.583 | Acc: 70.267% \n",
      "[epoch:15, iter:1431] Loss: 0.583 | Acc: 70.258% \n",
      "[epoch:15, iter:1432] Loss: 0.579 | Acc: 70.250% \n",
      "[epoch:15, iter:1433] Loss: 0.578 | Acc: 70.121% \n",
      "[epoch:15, iter:1434] Loss: 0.578 | Acc: 70.059% \n",
      "[epoch:15, iter:1435] Loss: 0.577 | Acc: 70.057% \n",
      "[epoch:15, iter:1436] Loss: 0.573 | Acc: 70.111% \n",
      "[epoch:15, iter:1437] Loss: 0.573 | Acc: 69.784% \n",
      "[epoch:15, iter:1438] Loss: 0.575 | Acc: 69.474% \n",
      "[epoch:15, iter:1439] Loss: 0.576 | Acc: 69.590% \n",
      "[epoch:15, iter:1440] Loss: 0.574 | Acc: 69.600% \n",
      "[epoch:15, iter:1441] Loss: 0.573 | Acc: 69.659% \n",
      "[epoch:15, iter:1442] Loss: 0.579 | Acc: 69.429% \n",
      "[epoch:15, iter:1443] Loss: 0.576 | Acc: 69.674% \n",
      "[epoch:15, iter:1444] Loss: 0.577 | Acc: 69.455% \n",
      "[epoch:15, iter:1445] Loss: 0.577 | Acc: 69.244% \n",
      "[epoch:15, iter:1446] Loss: 0.575 | Acc: 69.435% \n",
      "[epoch:15, iter:1447] Loss: 0.573 | Acc: 69.702% \n",
      "[epoch:15, iter:1448] Loss: 0.579 | Acc: 69.583% \n",
      "[epoch:15, iter:1449] Loss: 0.575 | Acc: 69.837% \n",
      "[epoch:15, iter:1450] Loss: 0.576 | Acc: 69.640% \n",
      "[epoch:15, iter:1451] Loss: 0.576 | Acc: 69.686% \n",
      "[epoch:15, iter:1452] Loss: 0.578 | Acc: 69.500% \n",
      "[epoch:15, iter:1453] Loss: 0.577 | Acc: 69.472% \n",
      "[epoch:15, iter:1454] Loss: 0.576 | Acc: 69.630% \n",
      "[epoch:15, iter:1455] Loss: 0.578 | Acc: 69.382% \n",
      "[epoch:15, iter:1456] Loss: 0.574 | Acc: 69.536% \n",
      "[epoch:15, iter:1457] Loss: 0.573 | Acc: 69.474% \n",
      "[epoch:15, iter:1458] Loss: 0.571 | Acc: 69.483% \n",
      "[epoch:15, iter:1459] Loss: 0.571 | Acc: 69.593% \n",
      "[epoch:15, iter:1460] Loss: 0.572 | Acc: 69.400% \n",
      "[epoch:15, iter:1461] Loss: 0.570 | Acc: 69.410% \n",
      "[epoch:15, iter:1462] Loss: 0.570 | Acc: 69.516% \n",
      "[epoch:15, iter:1463] Loss: 0.570 | Acc: 69.429% \n",
      "[epoch:15, iter:1464] Loss: 0.569 | Acc: 69.375% \n",
      "[epoch:15, iter:1465] Loss: 0.570 | Acc: 69.323% \n",
      "[epoch:15, iter:1466] Loss: 0.571 | Acc: 69.273% \n",
      "[epoch:15, iter:1467] Loss: 0.570 | Acc: 69.313% \n",
      "[epoch:15, iter:1468] Loss: 0.568 | Acc: 69.294% \n",
      "[epoch:15, iter:1469] Loss: 0.569 | Acc: 69.130% \n",
      "[epoch:15, iter:1470] Loss: 0.568 | Acc: 69.200% \n",
      "[epoch:15, iter:1471] Loss: 0.569 | Acc: 69.127% \n",
      "[epoch:15, iter:1472] Loss: 0.568 | Acc: 69.194% \n",
      "[epoch:15, iter:1473] Loss: 0.566 | Acc: 69.233% \n",
      "[epoch:15, iter:1474] Loss: 0.565 | Acc: 69.351% \n",
      "[epoch:15, iter:1475] Loss: 0.566 | Acc: 69.307% \n",
      "[epoch:15, iter:1476] Loss: 0.566 | Acc: 69.289% \n",
      "[epoch:15, iter:1477] Loss: 0.566 | Acc: 69.299% \n",
      "[epoch:15, iter:1478] Loss: 0.565 | Acc: 69.513% \n",
      "[epoch:15, iter:1479] Loss: 0.574 | Acc: 69.342% \n",
      "[epoch:15, iter:1480] Loss: 0.575 | Acc: 69.325% \n",
      "[epoch:15, iter:1481] Loss: 0.575 | Acc: 69.309% \n",
      "[epoch:15, iter:1482] Loss: 0.576 | Acc: 69.195% \n",
      "[epoch:15, iter:1483] Loss: 0.578 | Acc: 69.205% \n",
      "[epoch:15, iter:1484] Loss: 0.583 | Acc: 69.190% \n",
      "[epoch:15, iter:1485] Loss: 0.583 | Acc: 69.153% \n",
      "[epoch:15, iter:1486] Loss: 0.582 | Acc: 69.186% \n",
      "[epoch:15, iter:1487] Loss: 0.582 | Acc: 69.287% \n",
      "[epoch:15, iter:1488] Loss: 0.581 | Acc: 69.341% \n",
      "[epoch:15, iter:1489] Loss: 0.580 | Acc: 69.438% \n",
      "[epoch:15, iter:1490] Loss: 0.581 | Acc: 69.444% \n",
      "[epoch:15, iter:1491] Loss: 0.581 | Acc: 69.363% \n",
      "[epoch:15, iter:1492] Loss: 0.581 | Acc: 69.261% \n",
      "[epoch:15, iter:1493] Loss: 0.580 | Acc: 69.290% \n",
      "[epoch:15, iter:1494] Loss: 0.579 | Acc: 69.426% \n",
      "[epoch:15, iter:1495] Loss: 0.579 | Acc: 69.474% \n",
      "[epoch:15, iter:1496] Loss: 0.579 | Acc: 69.479% \n",
      "[epoch:15, iter:1497] Loss: 0.579 | Acc: 69.546% \n",
      "[epoch:15, iter:1498] Loss: 0.579 | Acc: 69.571% \n",
      "[epoch:15, iter:1499] Loss: 0.579 | Acc: 69.495% \n",
      "[epoch:15, iter:1500] Loss: 0.577 | Acc: 69.480% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.460%\n",
      "Training set's accuracy (after quantization) is: 72.840%\n",
      "Test set's accuracy (before quantization) is: 69.700%\n",
      "Test set's accuracy (after quantization) is: 73.100%\n",
      "Train Loss: 0.602 | Train Acc: 69.460% | Test Loss: 0.603 | Test Acc: 69.700% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.840% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.100% \n",
      "\n",
      "Epoch: 16\n",
      "[epoch:16, iter:1501] Loss: 0.549 | Acc: 64.000% \n",
      "[epoch:16, iter:1502] Loss: 0.554 | Acc: 65.000% \n",
      "[epoch:16, iter:1503] Loss: 0.555 | Acc: 68.000% \n",
      "[epoch:16, iter:1504] Loss: 0.558 | Acc: 67.500% \n",
      "[epoch:16, iter:1505] Loss: 0.541 | Acc: 68.800% \n",
      "[epoch:16, iter:1506] Loss: 0.531 | Acc: 70.000% \n",
      "[epoch:16, iter:1507] Loss: 0.549 | Acc: 68.571% \n",
      "[epoch:16, iter:1508] Loss: 0.537 | Acc: 70.000% \n",
      "[epoch:16, iter:1509] Loss: 0.560 | Acc: 70.444% \n",
      "[epoch:16, iter:1510] Loss: 0.550 | Acc: 70.600% \n",
      "[epoch:16, iter:1511] Loss: 0.548 | Acc: 70.364% \n",
      "[epoch:16, iter:1512] Loss: 0.538 | Acc: 70.833% \n",
      "[epoch:16, iter:1513] Loss: 0.550 | Acc: 69.846% \n",
      "[epoch:16, iter:1514] Loss: 0.547 | Acc: 70.143% \n",
      "[epoch:16, iter:1515] Loss: 0.553 | Acc: 70.133% \n",
      "[epoch:16, iter:1516] Loss: 0.553 | Acc: 70.375% \n",
      "[epoch:16, iter:1517] Loss: 0.550 | Acc: 69.882% \n",
      "[epoch:16, iter:1518] Loss: 0.543 | Acc: 70.222% \n",
      "[epoch:16, iter:1519] Loss: 0.543 | Acc: 70.211% \n",
      "[epoch:16, iter:1520] Loss: 0.541 | Acc: 70.100% \n",
      "[epoch:16, iter:1521] Loss: 0.541 | Acc: 69.810% \n",
      "[epoch:16, iter:1522] Loss: 0.538 | Acc: 70.091% \n",
      "[epoch:16, iter:1523] Loss: 0.538 | Acc: 70.000% \n",
      "[epoch:16, iter:1524] Loss: 0.541 | Acc: 69.750% \n",
      "[epoch:16, iter:1525] Loss: 0.542 | Acc: 69.920% \n",
      "[epoch:16, iter:1526] Loss: 0.539 | Acc: 70.231% \n",
      "[epoch:16, iter:1527] Loss: 0.539 | Acc: 70.148% \n",
      "[epoch:16, iter:1528] Loss: 0.542 | Acc: 70.000% \n",
      "[epoch:16, iter:1529] Loss: 0.541 | Acc: 70.000% \n",
      "[epoch:16, iter:1530] Loss: 0.544 | Acc: 70.067% \n",
      "[epoch:16, iter:1531] Loss: 0.545 | Acc: 70.387% \n",
      "[epoch:16, iter:1532] Loss: 0.544 | Acc: 70.562% \n",
      "[epoch:16, iter:1533] Loss: 0.542 | Acc: 70.788% \n",
      "[epoch:16, iter:1534] Loss: 0.541 | Acc: 71.118% \n",
      "[epoch:16, iter:1535] Loss: 0.541 | Acc: 71.143% \n",
      "[epoch:16, iter:1536] Loss: 0.543 | Acc: 70.889% \n",
      "[epoch:16, iter:1537] Loss: 0.542 | Acc: 70.973% \n",
      "[epoch:16, iter:1538] Loss: 0.537 | Acc: 71.211% \n",
      "[epoch:16, iter:1539] Loss: 0.539 | Acc: 70.974% \n",
      "[epoch:16, iter:1540] Loss: 0.540 | Acc: 71.000% \n",
      "[epoch:16, iter:1541] Loss: 0.541 | Acc: 70.780% \n",
      "[epoch:16, iter:1542] Loss: 0.542 | Acc: 70.571% \n",
      "[epoch:16, iter:1543] Loss: 0.538 | Acc: 70.837% \n",
      "[epoch:16, iter:1544] Loss: 0.541 | Acc: 70.591% \n",
      "[epoch:16, iter:1545] Loss: 0.542 | Acc: 70.444% \n",
      "[epoch:16, iter:1546] Loss: 0.545 | Acc: 70.130% \n",
      "[epoch:16, iter:1547] Loss: 0.546 | Acc: 70.298% \n",
      "[epoch:16, iter:1548] Loss: 0.547 | Acc: 70.083% \n",
      "[epoch:16, iter:1549] Loss: 0.548 | Acc: 69.959% \n",
      "[epoch:16, iter:1550] Loss: 0.547 | Acc: 70.000% \n",
      "[epoch:16, iter:1551] Loss: 0.548 | Acc: 69.922% \n",
      "[epoch:16, iter:1552] Loss: 0.546 | Acc: 70.038% \n",
      "[epoch:16, iter:1553] Loss: 0.548 | Acc: 69.925% \n",
      "[epoch:16, iter:1554] Loss: 0.548 | Acc: 69.889% \n",
      "[epoch:16, iter:1555] Loss: 0.548 | Acc: 69.818% \n",
      "[epoch:16, iter:1556] Loss: 0.548 | Acc: 69.821% \n",
      "[epoch:16, iter:1557] Loss: 0.548 | Acc: 69.684% \n",
      "[epoch:16, iter:1558] Loss: 0.547 | Acc: 69.828% \n",
      "[epoch:16, iter:1559] Loss: 0.548 | Acc: 69.763% \n",
      "[epoch:16, iter:1560] Loss: 0.548 | Acc: 69.833% \n",
      "[epoch:16, iter:1561] Loss: 0.547 | Acc: 69.967% \n",
      "[epoch:16, iter:1562] Loss: 0.547 | Acc: 70.032% \n",
      "[epoch:16, iter:1563] Loss: 0.548 | Acc: 69.937% \n",
      "[epoch:16, iter:1564] Loss: 0.548 | Acc: 70.000% \n",
      "[epoch:16, iter:1565] Loss: 0.549 | Acc: 70.062% \n",
      "[epoch:16, iter:1566] Loss: 0.549 | Acc: 70.000% \n",
      "[epoch:16, iter:1567] Loss: 0.549 | Acc: 69.970% \n",
      "[epoch:16, iter:1568] Loss: 0.550 | Acc: 69.853% \n",
      "[epoch:16, iter:1569] Loss: 0.550 | Acc: 69.826% \n",
      "[epoch:16, iter:1570] Loss: 0.548 | Acc: 69.971% \n",
      "[epoch:16, iter:1571] Loss: 0.548 | Acc: 69.972% \n",
      "[epoch:16, iter:1572] Loss: 0.547 | Acc: 70.056% \n",
      "[epoch:16, iter:1573] Loss: 0.549 | Acc: 69.918% \n",
      "[epoch:16, iter:1574] Loss: 0.548 | Acc: 69.892% \n",
      "[epoch:16, iter:1575] Loss: 0.548 | Acc: 69.813% \n",
      "[epoch:16, iter:1576] Loss: 0.549 | Acc: 69.789% \n",
      "[epoch:16, iter:1577] Loss: 0.550 | Acc: 69.818% \n",
      "[epoch:16, iter:1578] Loss: 0.550 | Acc: 69.718% \n",
      "[epoch:16, iter:1579] Loss: 0.549 | Acc: 69.772% \n",
      "[epoch:16, iter:1580] Loss: 0.550 | Acc: 69.675% \n",
      "[epoch:16, iter:1581] Loss: 0.550 | Acc: 69.679% \n",
      "[epoch:16, iter:1582] Loss: 0.550 | Acc: 69.512% \n",
      "[epoch:16, iter:1583] Loss: 0.551 | Acc: 69.325% \n",
      "[epoch:16, iter:1584] Loss: 0.551 | Acc: 69.381% \n",
      "[epoch:16, iter:1585] Loss: 0.554 | Acc: 69.294% \n",
      "[epoch:16, iter:1586] Loss: 0.557 | Acc: 69.256% \n",
      "[epoch:16, iter:1587] Loss: 0.558 | Acc: 69.126% \n",
      "[epoch:16, iter:1588] Loss: 0.557 | Acc: 69.182% \n",
      "[epoch:16, iter:1589] Loss: 0.557 | Acc: 69.213% \n",
      "[epoch:16, iter:1590] Loss: 0.558 | Acc: 69.089% \n",
      "[epoch:16, iter:1591] Loss: 0.557 | Acc: 69.033% \n",
      "[epoch:16, iter:1592] Loss: 0.557 | Acc: 69.087% \n",
      "[epoch:16, iter:1593] Loss: 0.556 | Acc: 69.161% \n",
      "[epoch:16, iter:1594] Loss: 0.556 | Acc: 69.213% \n",
      "[epoch:16, iter:1595] Loss: 0.557 | Acc: 69.095% \n",
      "[epoch:16, iter:1596] Loss: 0.556 | Acc: 69.167% \n",
      "[epoch:16, iter:1597] Loss: 0.555 | Acc: 69.175% \n",
      "[epoch:16, iter:1598] Loss: 0.554 | Acc: 69.224% \n",
      "[epoch:16, iter:1599] Loss: 0.554 | Acc: 69.172% \n",
      "[epoch:16, iter:1600] Loss: 0.553 | Acc: 69.240% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.120%\n",
      "Training set's accuracy (after quantization) is: 73.020%\n",
      "Test set's accuracy (before quantization) is: 69.400%\n",
      "Test set's accuracy (after quantization) is: 73.800%\n",
      "Train Loss: 0.656 | Train Acc: 69.120% | Test Loss: 0.658 | Test Acc: 69.400% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 73.020% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.800% \n",
      "\n",
      "Epoch: 17\n",
      "[epoch:17, iter:1601] Loss: 0.463 | Acc: 80.000% \n",
      "[epoch:17, iter:1602] Loss: 0.934 | Acc: 66.000% \n",
      "[epoch:17, iter:1603] Loss: 0.876 | Acc: 68.667% \n",
      "[epoch:17, iter:1604] Loss: 0.804 | Acc: 68.000% \n",
      "[epoch:17, iter:1605] Loss: 0.749 | Acc: 68.400% \n",
      "[epoch:17, iter:1606] Loss: 0.706 | Acc: 69.333% \n",
      "[epoch:17, iter:1607] Loss: 0.681 | Acc: 68.000% \n",
      "[epoch:17, iter:1608] Loss: 0.659 | Acc: 67.500% \n",
      "[epoch:17, iter:1609] Loss: 0.638 | Acc: 67.778% \n",
      "[epoch:17, iter:1610] Loss: 0.630 | Acc: 67.800% \n",
      "[epoch:17, iter:1611] Loss: 0.617 | Acc: 67.818% \n",
      "[epoch:17, iter:1612] Loss: 0.607 | Acc: 68.000% \n",
      "[epoch:17, iter:1613] Loss: 0.603 | Acc: 68.154% \n",
      "[epoch:17, iter:1614] Loss: 0.597 | Acc: 68.000% \n",
      "[epoch:17, iter:1615] Loss: 0.594 | Acc: 68.133% \n",
      "[epoch:17, iter:1616] Loss: 0.589 | Acc: 68.750% \n",
      "[epoch:17, iter:1617] Loss: 0.585 | Acc: 69.294% \n",
      "[epoch:17, iter:1618] Loss: 0.579 | Acc: 69.333% \n",
      "[epoch:17, iter:1619] Loss: 0.579 | Acc: 68.947% \n",
      "[epoch:17, iter:1620] Loss: 0.583 | Acc: 68.800% \n",
      "[epoch:17, iter:1621] Loss: 0.583 | Acc: 68.381% \n",
      "[epoch:17, iter:1622] Loss: 0.583 | Acc: 68.000% \n",
      "[epoch:17, iter:1623] Loss: 0.584 | Acc: 67.826% \n",
      "[epoch:17, iter:1624] Loss: 0.580 | Acc: 67.917% \n",
      "[epoch:17, iter:1625] Loss: 0.580 | Acc: 68.000% \n",
      "[epoch:17, iter:1626] Loss: 0.577 | Acc: 67.923% \n",
      "[epoch:17, iter:1627] Loss: 0.581 | Acc: 67.704% \n",
      "[epoch:17, iter:1628] Loss: 0.579 | Acc: 67.857% \n",
      "[epoch:17, iter:1629] Loss: 0.580 | Acc: 67.724% \n",
      "[epoch:17, iter:1630] Loss: 0.581 | Acc: 67.467% \n",
      "[epoch:17, iter:1631] Loss: 0.582 | Acc: 67.484% \n",
      "[epoch:17, iter:1632] Loss: 0.579 | Acc: 67.688% \n",
      "[epoch:17, iter:1633] Loss: 0.576 | Acc: 67.758% \n",
      "[epoch:17, iter:1634] Loss: 0.576 | Acc: 67.647% \n",
      "[epoch:17, iter:1635] Loss: 0.576 | Acc: 67.714% \n",
      "[epoch:17, iter:1636] Loss: 0.576 | Acc: 67.667% \n",
      "[epoch:17, iter:1637] Loss: 0.575 | Acc: 67.784% \n",
      "[epoch:17, iter:1638] Loss: 0.575 | Acc: 67.895% \n",
      "[epoch:17, iter:1639] Loss: 0.572 | Acc: 68.154% \n",
      "[epoch:17, iter:1640] Loss: 0.572 | Acc: 68.200% \n",
      "[epoch:17, iter:1641] Loss: 0.571 | Acc: 68.390% \n",
      "[epoch:17, iter:1642] Loss: 0.573 | Acc: 68.524% \n",
      "[epoch:17, iter:1643] Loss: 0.574 | Acc: 68.605% \n",
      "[epoch:17, iter:1644] Loss: 0.572 | Acc: 68.727% \n",
      "[epoch:17, iter:1645] Loss: 0.570 | Acc: 68.889% \n",
      "[epoch:17, iter:1646] Loss: 0.572 | Acc: 68.870% \n",
      "[epoch:17, iter:1647] Loss: 0.571 | Acc: 68.809% \n",
      "[epoch:17, iter:1648] Loss: 0.570 | Acc: 68.750% \n",
      "[epoch:17, iter:1649] Loss: 0.572 | Acc: 68.857% \n",
      "[epoch:17, iter:1650] Loss: 0.571 | Acc: 68.720% \n",
      "[epoch:17, iter:1651] Loss: 0.571 | Acc: 68.784% \n",
      "[epoch:17, iter:1652] Loss: 0.569 | Acc: 68.846% \n",
      "[epoch:17, iter:1653] Loss: 0.567 | Acc: 68.981% \n",
      "[epoch:17, iter:1654] Loss: 0.566 | Acc: 69.148% \n",
      "[epoch:17, iter:1655] Loss: 0.566 | Acc: 69.236% \n",
      "[epoch:17, iter:1656] Loss: 0.566 | Acc: 69.250% \n",
      "[epoch:17, iter:1657] Loss: 0.565 | Acc: 69.228% \n",
      "[epoch:17, iter:1658] Loss: 0.562 | Acc: 69.586% \n",
      "[epoch:17, iter:1659] Loss: 0.561 | Acc: 69.695% \n",
      "[epoch:17, iter:1660] Loss: 0.559 | Acc: 69.867% \n",
      "[epoch:17, iter:1661] Loss: 0.559 | Acc: 69.869% \n",
      "[epoch:17, iter:1662] Loss: 0.558 | Acc: 69.968% \n",
      "[epoch:17, iter:1663] Loss: 0.561 | Acc: 69.810% \n",
      "[epoch:17, iter:1664] Loss: 0.561 | Acc: 69.750% \n",
      "[epoch:17, iter:1665] Loss: 0.561 | Acc: 69.631% \n",
      "[epoch:17, iter:1666] Loss: 0.561 | Acc: 69.576% \n",
      "[epoch:17, iter:1667] Loss: 0.561 | Acc: 69.701% \n",
      "[epoch:17, iter:1668] Loss: 0.561 | Acc: 69.706% \n",
      "[epoch:17, iter:1669] Loss: 0.563 | Acc: 69.652% \n",
      "[epoch:17, iter:1670] Loss: 0.563 | Acc: 69.629% \n",
      "[epoch:17, iter:1671] Loss: 0.562 | Acc: 69.690% \n",
      "[epoch:17, iter:1672] Loss: 0.561 | Acc: 69.833% \n",
      "[epoch:17, iter:1673] Loss: 0.571 | Acc: 69.616% \n",
      "[epoch:17, iter:1674] Loss: 0.575 | Acc: 69.730% \n",
      "[epoch:17, iter:1675] Loss: 0.577 | Acc: 69.867% \n",
      "[epoch:17, iter:1676] Loss: 0.576 | Acc: 69.895% \n",
      "[epoch:17, iter:1677] Loss: 0.575 | Acc: 69.922% \n",
      "[epoch:17, iter:1678] Loss: 0.575 | Acc: 69.949% \n",
      "[epoch:17, iter:1679] Loss: 0.576 | Acc: 69.975% \n",
      "[epoch:17, iter:1680] Loss: 0.575 | Acc: 69.825% \n",
      "[epoch:17, iter:1681] Loss: 0.574 | Acc: 69.901% \n",
      "[epoch:17, iter:1682] Loss: 0.573 | Acc: 69.976% \n",
      "[epoch:17, iter:1683] Loss: 0.573 | Acc: 69.928% \n",
      "[epoch:17, iter:1684] Loss: 0.572 | Acc: 69.929% \n",
      "[epoch:17, iter:1685] Loss: 0.570 | Acc: 69.859% \n",
      "[epoch:17, iter:1686] Loss: 0.570 | Acc: 69.860% \n",
      "[epoch:17, iter:1687] Loss: 0.570 | Acc: 69.770% \n",
      "[epoch:17, iter:1688] Loss: 0.569 | Acc: 69.955% \n",
      "[epoch:17, iter:1689] Loss: 0.569 | Acc: 69.865% \n",
      "[epoch:17, iter:1690] Loss: 0.569 | Acc: 69.778% \n",
      "[epoch:17, iter:1691] Loss: 0.570 | Acc: 69.714% \n",
      "[epoch:17, iter:1692] Loss: 0.570 | Acc: 69.674% \n",
      "[epoch:17, iter:1693] Loss: 0.571 | Acc: 69.634% \n",
      "[epoch:17, iter:1694] Loss: 0.571 | Acc: 69.532% \n",
      "[epoch:17, iter:1695] Loss: 0.570 | Acc: 69.642% \n",
      "[epoch:17, iter:1696] Loss: 0.570 | Acc: 69.708% \n",
      "[epoch:17, iter:1697] Loss: 0.570 | Acc: 69.608% \n",
      "[epoch:17, iter:1698] Loss: 0.569 | Acc: 69.592% \n",
      "[epoch:17, iter:1699] Loss: 0.570 | Acc: 69.475% \n",
      "[epoch:17, iter:1700] Loss: 0.570 | Acc: 69.480% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.880%\n",
      "Training set's accuracy (after quantization) is: 72.760%\n",
      "Test set's accuracy (before quantization) is: 70.400%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.581 | Train Acc: 69.880% | Test Loss: 0.582 | Test Acc: 70.400% \n",
      "Quantized Train Loss: 0.542 | Quantized Train Acc: 72.760% | Quantized Test Loss: 0.545 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 18\n",
      "[epoch:18, iter:1701] Loss: 0.437 | Acc: 82.000% \n",
      "[epoch:18, iter:1702] Loss: 0.486 | Acc: 79.000% \n",
      "[epoch:18, iter:1703] Loss: 0.593 | Acc: 73.333% \n",
      "[epoch:18, iter:1704] Loss: 0.619 | Acc: 68.500% \n",
      "[epoch:18, iter:1705] Loss: 0.640 | Acc: 70.000% \n",
      "[epoch:18, iter:1706] Loss: 0.620 | Acc: 70.667% \n",
      "[epoch:18, iter:1707] Loss: 0.603 | Acc: 70.857% \n",
      "[epoch:18, iter:1708] Loss: 0.607 | Acc: 69.500% \n",
      "[epoch:18, iter:1709] Loss: 0.604 | Acc: 69.556% \n",
      "[epoch:18, iter:1710] Loss: 0.601 | Acc: 70.000% \n",
      "[epoch:18, iter:1711] Loss: 0.595 | Acc: 70.545% \n",
      "[epoch:18, iter:1712] Loss: 0.595 | Acc: 69.333% \n",
      "[epoch:18, iter:1713] Loss: 0.586 | Acc: 69.385% \n",
      "[epoch:18, iter:1714] Loss: 0.586 | Acc: 69.143% \n",
      "[epoch:18, iter:1715] Loss: 0.581 | Acc: 69.333% \n",
      "[epoch:18, iter:1716] Loss: 0.579 | Acc: 69.500% \n",
      "[epoch:18, iter:1717] Loss: 0.583 | Acc: 69.765% \n",
      "[epoch:18, iter:1718] Loss: 0.606 | Acc: 69.444% \n",
      "[epoch:18, iter:1719] Loss: 0.610 | Acc: 69.789% \n",
      "[epoch:18, iter:1720] Loss: 0.609 | Acc: 69.800% \n",
      "[epoch:18, iter:1721] Loss: 0.607 | Acc: 69.905% \n",
      "[epoch:18, iter:1722] Loss: 0.607 | Acc: 69.455% \n",
      "[epoch:18, iter:1723] Loss: 0.602 | Acc: 69.304% \n",
      "[epoch:18, iter:1724] Loss: 0.602 | Acc: 68.833% \n",
      "[epoch:18, iter:1725] Loss: 0.602 | Acc: 68.960% \n",
      "[epoch:18, iter:1726] Loss: 0.597 | Acc: 69.308% \n",
      "[epoch:18, iter:1727] Loss: 0.596 | Acc: 69.333% \n",
      "[epoch:18, iter:1728] Loss: 0.598 | Acc: 69.214% \n",
      "[epoch:18, iter:1729] Loss: 0.596 | Acc: 69.448% \n",
      "[epoch:18, iter:1730] Loss: 0.597 | Acc: 69.200% \n",
      "[epoch:18, iter:1731] Loss: 0.591 | Acc: 69.419% \n",
      "[epoch:18, iter:1732] Loss: 0.588 | Acc: 69.625% \n",
      "[epoch:18, iter:1733] Loss: 0.589 | Acc: 69.515% \n",
      "[epoch:18, iter:1734] Loss: 0.587 | Acc: 69.588% \n",
      "[epoch:18, iter:1735] Loss: 0.584 | Acc: 69.543% \n",
      "[epoch:18, iter:1736] Loss: 0.581 | Acc: 69.667% \n",
      "[epoch:18, iter:1737] Loss: 0.583 | Acc: 69.405% \n",
      "[epoch:18, iter:1738] Loss: 0.581 | Acc: 69.579% \n",
      "[epoch:18, iter:1739] Loss: 0.577 | Acc: 69.744% \n",
      "[epoch:18, iter:1740] Loss: 0.578 | Acc: 69.750% \n",
      "[epoch:18, iter:1741] Loss: 0.579 | Acc: 69.512% \n",
      "[epoch:18, iter:1742] Loss: 0.576 | Acc: 69.762% \n",
      "[epoch:18, iter:1743] Loss: 0.572 | Acc: 69.907% \n",
      "[epoch:18, iter:1744] Loss: 0.576 | Acc: 69.818% \n",
      "[epoch:18, iter:1745] Loss: 0.577 | Acc: 69.511% \n",
      "[epoch:18, iter:1746] Loss: 0.576 | Acc: 69.696% \n",
      "[epoch:18, iter:1747] Loss: 0.574 | Acc: 69.915% \n",
      "[epoch:18, iter:1748] Loss: 0.573 | Acc: 69.875% \n",
      "[epoch:18, iter:1749] Loss: 0.571 | Acc: 69.959% \n",
      "[epoch:18, iter:1750] Loss: 0.571 | Acc: 69.840% \n",
      "[epoch:18, iter:1751] Loss: 0.570 | Acc: 69.922% \n",
      "[epoch:18, iter:1752] Loss: 0.571 | Acc: 70.077% \n",
      "[epoch:18, iter:1753] Loss: 0.572 | Acc: 70.075% \n",
      "[epoch:18, iter:1754] Loss: 0.571 | Acc: 70.000% \n",
      "[epoch:18, iter:1755] Loss: 0.568 | Acc: 70.218% \n",
      "[epoch:18, iter:1756] Loss: 0.569 | Acc: 70.143% \n",
      "[epoch:18, iter:1757] Loss: 0.569 | Acc: 70.035% \n",
      "[epoch:18, iter:1758] Loss: 0.569 | Acc: 70.034% \n",
      "[epoch:18, iter:1759] Loss: 0.568 | Acc: 70.000% \n",
      "[epoch:18, iter:1760] Loss: 0.568 | Acc: 70.000% \n",
      "[epoch:18, iter:1761] Loss: 0.567 | Acc: 69.967% \n",
      "[epoch:18, iter:1762] Loss: 0.567 | Acc: 70.000% \n",
      "[epoch:18, iter:1763] Loss: 0.566 | Acc: 69.968% \n",
      "[epoch:18, iter:1764] Loss: 0.566 | Acc: 69.969% \n",
      "[epoch:18, iter:1765] Loss: 0.566 | Acc: 70.031% \n",
      "[epoch:18, iter:1766] Loss: 0.568 | Acc: 70.000% \n",
      "[epoch:18, iter:1767] Loss: 0.569 | Acc: 69.821% \n",
      "[epoch:18, iter:1768] Loss: 0.570 | Acc: 69.765% \n",
      "[epoch:18, iter:1769] Loss: 0.569 | Acc: 69.913% \n",
      "[epoch:18, iter:1770] Loss: 0.568 | Acc: 69.829% \n",
      "[epoch:18, iter:1771] Loss: 0.567 | Acc: 69.944% \n",
      "[epoch:18, iter:1772] Loss: 0.566 | Acc: 69.889% \n",
      "[epoch:18, iter:1773] Loss: 0.567 | Acc: 69.808% \n",
      "[epoch:18, iter:1774] Loss: 0.566 | Acc: 69.784% \n",
      "[epoch:18, iter:1775] Loss: 0.565 | Acc: 69.840% \n",
      "[epoch:18, iter:1776] Loss: 0.565 | Acc: 69.789% \n",
      "[epoch:18, iter:1777] Loss: 0.565 | Acc: 69.766% \n",
      "[epoch:18, iter:1778] Loss: 0.565 | Acc: 69.795% \n",
      "[epoch:18, iter:1779] Loss: 0.566 | Acc: 69.848% \n",
      "[epoch:18, iter:1780] Loss: 0.566 | Acc: 69.850% \n",
      "[epoch:18, iter:1781] Loss: 0.567 | Acc: 69.827% \n",
      "[epoch:18, iter:1782] Loss: 0.567 | Acc: 69.683% \n",
      "[epoch:18, iter:1783] Loss: 0.567 | Acc: 69.590% \n",
      "[epoch:18, iter:1784] Loss: 0.568 | Acc: 69.595% \n",
      "[epoch:18, iter:1785] Loss: 0.568 | Acc: 69.647% \n",
      "[epoch:18, iter:1786] Loss: 0.567 | Acc: 69.767% \n",
      "[epoch:18, iter:1787] Loss: 0.567 | Acc: 69.770% \n",
      "[epoch:18, iter:1788] Loss: 0.568 | Acc: 69.682% \n",
      "[epoch:18, iter:1789] Loss: 0.567 | Acc: 69.730% \n",
      "[epoch:18, iter:1790] Loss: 0.567 | Acc: 69.667% \n",
      "[epoch:18, iter:1791] Loss: 0.566 | Acc: 69.648% \n",
      "[epoch:18, iter:1792] Loss: 0.566 | Acc: 69.543% \n",
      "[epoch:18, iter:1793] Loss: 0.566 | Acc: 69.527% \n",
      "[epoch:18, iter:1794] Loss: 0.566 | Acc: 69.596% \n",
      "[epoch:18, iter:1795] Loss: 0.567 | Acc: 69.495% \n",
      "[epoch:18, iter:1796] Loss: 0.566 | Acc: 69.542% \n",
      "[epoch:18, iter:1797] Loss: 0.565 | Acc: 69.526% \n",
      "[epoch:18, iter:1798] Loss: 0.565 | Acc: 69.408% \n",
      "[epoch:18, iter:1799] Loss: 0.564 | Acc: 69.576% \n",
      "[epoch:18, iter:1800] Loss: 0.564 | Acc: 69.420% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.320%\n",
      "Training set's accuracy (after quantization) is: 72.900%\n",
      "Test set's accuracy (before quantization) is: 69.800%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.616 | Train Acc: 69.320% | Test Loss: 0.618 | Test Acc: 69.800% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.900% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 19\n",
      "[epoch:19, iter:1801] Loss: 0.490 | Acc: 64.000% \n",
      "[epoch:19, iter:1802] Loss: 0.512 | Acc: 68.000% \n",
      "[epoch:19, iter:1803] Loss: 0.566 | Acc: 63.333% \n",
      "[epoch:19, iter:1804] Loss: 0.560 | Acc: 64.500% \n",
      "[epoch:19, iter:1805] Loss: 0.568 | Acc: 64.400% \n",
      "[epoch:19, iter:1806] Loss: 0.573 | Acc: 63.333% \n",
      "[epoch:19, iter:1807] Loss: 0.549 | Acc: 66.571% \n",
      "[epoch:19, iter:1808] Loss: 0.543 | Acc: 66.750% \n",
      "[epoch:19, iter:1809] Loss: 0.539 | Acc: 67.111% \n",
      "[epoch:19, iter:1810] Loss: 0.544 | Acc: 66.600% \n",
      "[epoch:19, iter:1811] Loss: 0.548 | Acc: 67.273% \n",
      "[epoch:19, iter:1812] Loss: 0.543 | Acc: 68.000% \n",
      "[epoch:19, iter:1813] Loss: 0.543 | Acc: 67.385% \n",
      "[epoch:19, iter:1814] Loss: 0.536 | Acc: 68.429% \n",
      "[epoch:19, iter:1815] Loss: 0.530 | Acc: 68.800% \n",
      "[epoch:19, iter:1816] Loss: 0.521 | Acc: 70.000% \n",
      "[epoch:19, iter:1817] Loss: 0.527 | Acc: 70.118% \n",
      "[epoch:19, iter:1818] Loss: 0.526 | Acc: 70.667% \n",
      "[epoch:19, iter:1819] Loss: 0.533 | Acc: 70.316% \n",
      "[epoch:19, iter:1820] Loss: 0.531 | Acc: 70.600% \n",
      "[epoch:19, iter:1821] Loss: 0.531 | Acc: 70.286% \n",
      "[epoch:19, iter:1822] Loss: 0.532 | Acc: 70.364% \n",
      "[epoch:19, iter:1823] Loss: 0.528 | Acc: 70.870% \n",
      "[epoch:19, iter:1824] Loss: 0.534 | Acc: 70.833% \n",
      "[epoch:19, iter:1825] Loss: 0.537 | Acc: 70.640% \n",
      "[epoch:19, iter:1826] Loss: 0.535 | Acc: 70.769% \n",
      "[epoch:19, iter:1827] Loss: 0.533 | Acc: 71.037% \n",
      "[epoch:19, iter:1828] Loss: 0.531 | Acc: 71.071% \n",
      "[epoch:19, iter:1829] Loss: 0.533 | Acc: 71.172% \n",
      "[epoch:19, iter:1830] Loss: 0.535 | Acc: 71.267% \n",
      "[epoch:19, iter:1831] Loss: 0.538 | Acc: 71.097% \n",
      "[epoch:19, iter:1832] Loss: 0.539 | Acc: 70.750% \n",
      "[epoch:19, iter:1833] Loss: 0.536 | Acc: 70.727% \n",
      "[epoch:19, iter:1834] Loss: 0.539 | Acc: 70.588% \n",
      "[epoch:19, iter:1835] Loss: 0.538 | Acc: 70.571% \n",
      "[epoch:19, iter:1836] Loss: 0.538 | Acc: 70.611% \n",
      "[epoch:19, iter:1837] Loss: 0.538 | Acc: 70.486% \n",
      "[epoch:19, iter:1838] Loss: 0.535 | Acc: 70.632% \n",
      "[epoch:19, iter:1839] Loss: 0.535 | Acc: 70.718% \n",
      "[epoch:19, iter:1840] Loss: 0.538 | Acc: 70.400% \n",
      "[epoch:19, iter:1841] Loss: 0.538 | Acc: 70.390% \n",
      "[epoch:19, iter:1842] Loss: 0.536 | Acc: 70.714% \n",
      "[epoch:19, iter:1843] Loss: 0.538 | Acc: 70.512% \n",
      "[epoch:19, iter:1844] Loss: 0.538 | Acc: 70.364% \n",
      "[epoch:19, iter:1845] Loss: 0.538 | Acc: 70.444% \n",
      "[epoch:19, iter:1846] Loss: 0.537 | Acc: 70.478% \n",
      "[epoch:19, iter:1847] Loss: 0.538 | Acc: 70.298% \n",
      "[epoch:19, iter:1848] Loss: 0.539 | Acc: 70.333% \n",
      "[epoch:19, iter:1849] Loss: 0.540 | Acc: 70.245% \n",
      "[epoch:19, iter:1850] Loss: 0.541 | Acc: 70.240% \n",
      "[epoch:19, iter:1851] Loss: 0.538 | Acc: 70.235% \n",
      "[epoch:19, iter:1852] Loss: 0.537 | Acc: 70.346% \n",
      "[epoch:19, iter:1853] Loss: 0.536 | Acc: 70.302% \n",
      "[epoch:19, iter:1854] Loss: 0.535 | Acc: 70.333% \n",
      "[epoch:19, iter:1855] Loss: 0.535 | Acc: 70.255% \n",
      "[epoch:19, iter:1856] Loss: 0.537 | Acc: 70.179% \n",
      "[epoch:19, iter:1857] Loss: 0.538 | Acc: 70.140% \n",
      "[epoch:19, iter:1858] Loss: 0.539 | Acc: 70.000% \n",
      "[epoch:19, iter:1859] Loss: 0.539 | Acc: 70.034% \n",
      "[epoch:19, iter:1860] Loss: 0.537 | Acc: 70.233% \n",
      "[epoch:19, iter:1861] Loss: 0.539 | Acc: 70.131% \n",
      "[epoch:19, iter:1862] Loss: 0.541 | Acc: 69.935% \n",
      "[epoch:19, iter:1863] Loss: 0.542 | Acc: 69.968% \n",
      "[epoch:19, iter:1864] Loss: 0.541 | Acc: 70.031% \n",
      "[epoch:19, iter:1865] Loss: 0.541 | Acc: 70.062% \n",
      "[epoch:19, iter:1866] Loss: 0.539 | Acc: 70.333% \n",
      "[epoch:19, iter:1867] Loss: 0.546 | Acc: 70.269% \n",
      "[epoch:19, iter:1868] Loss: 0.547 | Acc: 70.176% \n",
      "[epoch:19, iter:1869] Loss: 0.546 | Acc: 70.261% \n",
      "[epoch:19, iter:1870] Loss: 0.546 | Acc: 70.257% \n",
      "[epoch:19, iter:1871] Loss: 0.544 | Acc: 70.282% \n",
      "[epoch:19, iter:1872] Loss: 0.545 | Acc: 70.333% \n",
      "[epoch:19, iter:1873] Loss: 0.546 | Acc: 70.164% \n",
      "[epoch:19, iter:1874] Loss: 0.545 | Acc: 70.243% \n",
      "[epoch:19, iter:1875] Loss: 0.546 | Acc: 70.187% \n",
      "[epoch:19, iter:1876] Loss: 0.548 | Acc: 70.184% \n",
      "[epoch:19, iter:1877] Loss: 0.547 | Acc: 70.130% \n",
      "[epoch:19, iter:1878] Loss: 0.548 | Acc: 70.154% \n",
      "[epoch:19, iter:1879] Loss: 0.547 | Acc: 70.228% \n",
      "[epoch:19, iter:1880] Loss: 0.548 | Acc: 70.175% \n",
      "[epoch:19, iter:1881] Loss: 0.548 | Acc: 70.074% \n",
      "[epoch:19, iter:1882] Loss: 0.548 | Acc: 70.000% \n",
      "[epoch:19, iter:1883] Loss: 0.548 | Acc: 69.952% \n",
      "[epoch:19, iter:1884] Loss: 0.547 | Acc: 69.929% \n",
      "[epoch:19, iter:1885] Loss: 0.547 | Acc: 69.906% \n",
      "[epoch:19, iter:1886] Loss: 0.548 | Acc: 69.907% \n",
      "[epoch:19, iter:1887] Loss: 0.548 | Acc: 69.885% \n",
      "[epoch:19, iter:1888] Loss: 0.550 | Acc: 69.818% \n",
      "[epoch:19, iter:1889] Loss: 0.549 | Acc: 69.865% \n",
      "[epoch:19, iter:1890] Loss: 0.549 | Acc: 69.800% \n",
      "[epoch:19, iter:1891] Loss: 0.550 | Acc: 69.714% \n",
      "[epoch:19, iter:1892] Loss: 0.550 | Acc: 69.739% \n",
      "[epoch:19, iter:1893] Loss: 0.549 | Acc: 69.763% \n",
      "[epoch:19, iter:1894] Loss: 0.549 | Acc: 69.809% \n",
      "[epoch:19, iter:1895] Loss: 0.549 | Acc: 69.853% \n",
      "[epoch:19, iter:1896] Loss: 0.549 | Acc: 69.812% \n",
      "[epoch:19, iter:1897] Loss: 0.551 | Acc: 69.753% \n",
      "[epoch:19, iter:1898] Loss: 0.552 | Acc: 69.612% \n",
      "[epoch:19, iter:1899] Loss: 0.554 | Acc: 69.657% \n",
      "[epoch:19, iter:1900] Loss: 0.553 | Acc: 69.720% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.120%\n",
      "Training set's accuracy (after quantization) is: 73.000%\n",
      "Test set's accuracy (before quantization) is: 69.500%\n",
      "Test set's accuracy (after quantization) is: 73.100%\n",
      "Train Loss: 0.625 | Train Acc: 69.120% | Test Loss: 0.627 | Test Acc: 69.500% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 73.000% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.100% \n",
      "\n",
      "Epoch: 20\n",
      "[epoch:20, iter:1901] Loss: 0.563 | Acc: 78.000% \n",
      "[epoch:20, iter:1902] Loss: 0.561 | Acc: 76.000% \n",
      "[epoch:20, iter:1903] Loss: 0.535 | Acc: 74.667% \n",
      "[epoch:20, iter:1904] Loss: 0.508 | Acc: 77.500% \n",
      "[epoch:20, iter:1905] Loss: 0.638 | Acc: 74.000% \n",
      "[epoch:20, iter:1906] Loss: 0.641 | Acc: 70.333% \n",
      "[epoch:20, iter:1907] Loss: 0.634 | Acc: 70.000% \n",
      "[epoch:20, iter:1908] Loss: 0.629 | Acc: 69.750% \n",
      "[epoch:20, iter:1909] Loss: 0.620 | Acc: 70.222% \n",
      "[epoch:20, iter:1910] Loss: 0.613 | Acc: 70.000% \n",
      "[epoch:20, iter:1911] Loss: 0.612 | Acc: 69.636% \n",
      "[epoch:20, iter:1912] Loss: 0.600 | Acc: 69.833% \n",
      "[epoch:20, iter:1913] Loss: 0.594 | Acc: 69.385% \n",
      "[epoch:20, iter:1914] Loss: 0.594 | Acc: 68.714% \n",
      "[epoch:20, iter:1915] Loss: 0.589 | Acc: 68.933% \n",
      "[epoch:20, iter:1916] Loss: 0.582 | Acc: 69.000% \n",
      "[epoch:20, iter:1917] Loss: 0.581 | Acc: 69.294% \n",
      "[epoch:20, iter:1918] Loss: 0.581 | Acc: 69.444% \n",
      "[epoch:20, iter:1919] Loss: 0.583 | Acc: 69.053% \n",
      "[epoch:20, iter:1920] Loss: 0.576 | Acc: 69.300% \n",
      "[epoch:20, iter:1921] Loss: 0.572 | Acc: 69.333% \n",
      "[epoch:20, iter:1922] Loss: 0.568 | Acc: 69.545% \n",
      "[epoch:20, iter:1923] Loss: 0.562 | Acc: 69.739% \n",
      "[epoch:20, iter:1924] Loss: 0.560 | Acc: 69.917% \n",
      "[epoch:20, iter:1925] Loss: 0.556 | Acc: 70.320% \n",
      "[epoch:20, iter:1926] Loss: 0.552 | Acc: 70.462% \n",
      "[epoch:20, iter:1927] Loss: 0.560 | Acc: 69.926% \n",
      "[epoch:20, iter:1928] Loss: 0.561 | Acc: 70.000% \n",
      "[epoch:20, iter:1929] Loss: 0.562 | Acc: 69.931% \n",
      "[epoch:20, iter:1930] Loss: 0.562 | Acc: 69.667% \n",
      "[epoch:20, iter:1931] Loss: 0.562 | Acc: 69.871% \n",
      "[epoch:20, iter:1932] Loss: 0.559 | Acc: 70.062% \n",
      "[epoch:20, iter:1933] Loss: 0.559 | Acc: 69.697% \n",
      "[epoch:20, iter:1934] Loss: 0.561 | Acc: 69.647% \n",
      "[epoch:20, iter:1935] Loss: 0.564 | Acc: 69.543% \n",
      "[epoch:20, iter:1936] Loss: 0.563 | Acc: 69.722% \n",
      "[epoch:20, iter:1937] Loss: 0.561 | Acc: 69.892% \n",
      "[epoch:20, iter:1938] Loss: 0.559 | Acc: 69.947% \n",
      "[epoch:20, iter:1939] Loss: 0.557 | Acc: 69.949% \n",
      "[epoch:20, iter:1940] Loss: 0.560 | Acc: 69.950% \n",
      "[epoch:20, iter:1941] Loss: 0.559 | Acc: 69.951% \n",
      "[epoch:20, iter:1942] Loss: 0.556 | Acc: 70.000% \n",
      "[epoch:20, iter:1943] Loss: 0.553 | Acc: 70.279% \n",
      "[epoch:20, iter:1944] Loss: 0.556 | Acc: 70.227% \n",
      "[epoch:20, iter:1945] Loss: 0.554 | Acc: 70.222% \n",
      "[epoch:20, iter:1946] Loss: 0.554 | Acc: 70.043% \n",
      "[epoch:20, iter:1947] Loss: 0.556 | Acc: 70.128% \n",
      "[epoch:20, iter:1948] Loss: 0.559 | Acc: 69.750% \n",
      "[epoch:20, iter:1949] Loss: 0.558 | Acc: 69.796% \n",
      "[epoch:20, iter:1950] Loss: 0.563 | Acc: 69.680% \n",
      "[epoch:20, iter:1951] Loss: 0.564 | Acc: 69.686% \n",
      "[epoch:20, iter:1952] Loss: 0.563 | Acc: 69.615% \n",
      "[epoch:20, iter:1953] Loss: 0.562 | Acc: 69.736% \n",
      "[epoch:20, iter:1954] Loss: 0.561 | Acc: 69.815% \n",
      "[epoch:20, iter:1955] Loss: 0.574 | Acc: 69.600% \n",
      "[epoch:20, iter:1956] Loss: 0.576 | Acc: 69.500% \n",
      "[epoch:20, iter:1957] Loss: 0.574 | Acc: 69.474% \n",
      "[epoch:20, iter:1958] Loss: 0.574 | Acc: 69.483% \n",
      "[epoch:20, iter:1959] Loss: 0.574 | Acc: 69.559% \n",
      "[epoch:20, iter:1960] Loss: 0.573 | Acc: 69.633% \n",
      "[epoch:20, iter:1961] Loss: 0.575 | Acc: 69.508% \n",
      "[epoch:20, iter:1962] Loss: 0.574 | Acc: 69.516% \n",
      "[epoch:20, iter:1963] Loss: 0.573 | Acc: 69.619% \n",
      "[epoch:20, iter:1964] Loss: 0.572 | Acc: 69.688% \n",
      "[epoch:20, iter:1965] Loss: 0.573 | Acc: 69.631% \n",
      "[epoch:20, iter:1966] Loss: 0.571 | Acc: 69.758% \n",
      "[epoch:20, iter:1967] Loss: 0.572 | Acc: 69.701% \n",
      "[epoch:20, iter:1968] Loss: 0.570 | Acc: 69.824% \n",
      "[epoch:20, iter:1969] Loss: 0.570 | Acc: 69.681% \n",
      "[epoch:20, iter:1970] Loss: 0.569 | Acc: 69.657% \n",
      "[epoch:20, iter:1971] Loss: 0.568 | Acc: 69.662% \n",
      "[epoch:20, iter:1972] Loss: 0.569 | Acc: 69.667% \n",
      "[epoch:20, iter:1973] Loss: 0.568 | Acc: 69.589% \n",
      "[epoch:20, iter:1974] Loss: 0.567 | Acc: 69.541% \n",
      "[epoch:20, iter:1975] Loss: 0.567 | Acc: 69.547% \n",
      "[epoch:20, iter:1976] Loss: 0.567 | Acc: 69.500% \n",
      "[epoch:20, iter:1977] Loss: 0.569 | Acc: 69.610% \n",
      "[epoch:20, iter:1978] Loss: 0.570 | Acc: 69.564% \n",
      "[epoch:20, iter:1979] Loss: 0.571 | Acc: 69.544% \n",
      "[epoch:20, iter:1980] Loss: 0.572 | Acc: 69.450% \n",
      "[epoch:20, iter:1981] Loss: 0.572 | Acc: 69.481% \n",
      "[epoch:20, iter:1982] Loss: 0.572 | Acc: 69.390% \n",
      "[epoch:20, iter:1983] Loss: 0.573 | Acc: 69.277% \n",
      "[epoch:20, iter:1984] Loss: 0.573 | Acc: 69.238% \n",
      "[epoch:20, iter:1985] Loss: 0.574 | Acc: 69.176% \n",
      "[epoch:20, iter:1986] Loss: 0.573 | Acc: 69.070% \n",
      "[epoch:20, iter:1987] Loss: 0.573 | Acc: 69.011% \n",
      "[epoch:20, iter:1988] Loss: 0.573 | Acc: 68.977% \n",
      "[epoch:20, iter:1989] Loss: 0.571 | Acc: 69.056% \n",
      "[epoch:20, iter:1990] Loss: 0.570 | Acc: 69.089% \n",
      "[epoch:20, iter:1991] Loss: 0.568 | Acc: 69.187% \n",
      "[epoch:20, iter:1992] Loss: 0.568 | Acc: 69.152% \n",
      "[epoch:20, iter:1993] Loss: 0.568 | Acc: 69.097% \n",
      "[epoch:20, iter:1994] Loss: 0.571 | Acc: 69.000% \n",
      "[epoch:20, iter:1995] Loss: 0.570 | Acc: 69.074% \n",
      "[epoch:20, iter:1996] Loss: 0.569 | Acc: 69.083% \n",
      "[epoch:20, iter:1997] Loss: 0.569 | Acc: 68.969% \n",
      "[epoch:20, iter:1998] Loss: 0.568 | Acc: 69.082% \n",
      "[epoch:20, iter:1999] Loss: 0.567 | Acc: 69.212% \n",
      "[epoch:20, iter:2000] Loss: 0.567 | Acc: 69.200% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.040%\n",
      "Training set's accuracy (after quantization) is: 72.900%\n",
      "Test set's accuracy (before quantization) is: 69.400%\n",
      "Test set's accuracy (after quantization) is: 73.500%\n",
      "Train Loss: 0.690 | Train Acc: 69.040% | Test Loss: 0.692 | Test Acc: 69.400% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.900% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.500% \n",
      "\n",
      "Epoch: 21\n",
      "[epoch:21, iter:2001] Loss: 0.617 | Acc: 68.000% \n",
      "[epoch:21, iter:2002] Loss: 0.551 | Acc: 69.000% \n",
      "[epoch:21, iter:2003] Loss: 0.576 | Acc: 70.667% \n",
      "[epoch:21, iter:2004] Loss: 0.564 | Acc: 71.500% \n",
      "[epoch:21, iter:2005] Loss: 0.567 | Acc: 70.400% \n",
      "[epoch:21, iter:2006] Loss: 0.575 | Acc: 70.333% \n",
      "[epoch:21, iter:2007] Loss: 0.570 | Acc: 68.857% \n",
      "[epoch:21, iter:2008] Loss: 0.569 | Acc: 69.750% \n",
      "[epoch:21, iter:2009] Loss: 0.641 | Acc: 68.000% \n",
      "[epoch:21, iter:2010] Loss: 0.629 | Acc: 67.200% \n",
      "[epoch:21, iter:2011] Loss: 0.622 | Acc: 66.909% \n",
      "[epoch:21, iter:2012] Loss: 0.616 | Acc: 67.500% \n",
      "[epoch:21, iter:2013] Loss: 0.615 | Acc: 66.923% \n",
      "[epoch:21, iter:2014] Loss: 0.621 | Acc: 66.429% \n",
      "[epoch:21, iter:2015] Loss: 0.620 | Acc: 66.267% \n",
      "[epoch:21, iter:2016] Loss: 0.615 | Acc: 66.125% \n",
      "[epoch:21, iter:2017] Loss: 0.610 | Acc: 66.235% \n",
      "[epoch:21, iter:2018] Loss: 0.614 | Acc: 65.889% \n",
      "[epoch:21, iter:2019] Loss: 0.611 | Acc: 66.211% \n",
      "[epoch:21, iter:2020] Loss: 0.602 | Acc: 66.700% \n",
      "[epoch:21, iter:2021] Loss: 0.600 | Acc: 67.048% \n",
      "[epoch:21, iter:2022] Loss: 0.592 | Acc: 67.364% \n",
      "[epoch:21, iter:2023] Loss: 0.585 | Acc: 67.826% \n",
      "[epoch:21, iter:2024] Loss: 0.590 | Acc: 68.083% \n",
      "[epoch:21, iter:2025] Loss: 0.588 | Acc: 67.760% \n",
      "[epoch:21, iter:2026] Loss: 0.588 | Acc: 67.615% \n",
      "[epoch:21, iter:2027] Loss: 0.586 | Acc: 67.704% \n",
      "[epoch:21, iter:2028] Loss: 0.581 | Acc: 68.214% \n",
      "[epoch:21, iter:2029] Loss: 0.597 | Acc: 68.138% \n",
      "[epoch:21, iter:2030] Loss: 0.593 | Acc: 68.600% \n",
      "[epoch:21, iter:2031] Loss: 0.593 | Acc: 68.258% \n",
      "[epoch:21, iter:2032] Loss: 0.588 | Acc: 68.812% \n",
      "[epoch:21, iter:2033] Loss: 0.587 | Acc: 68.909% \n",
      "[epoch:21, iter:2034] Loss: 0.584 | Acc: 68.824% \n",
      "[epoch:21, iter:2035] Loss: 0.582 | Acc: 69.029% \n",
      "[epoch:21, iter:2036] Loss: 0.581 | Acc: 69.111% \n",
      "[epoch:21, iter:2037] Loss: 0.586 | Acc: 68.865% \n",
      "[epoch:21, iter:2038] Loss: 0.582 | Acc: 68.789% \n",
      "[epoch:21, iter:2039] Loss: 0.581 | Acc: 68.667% \n",
      "[epoch:21, iter:2040] Loss: 0.582 | Acc: 68.650% \n",
      "[epoch:21, iter:2041] Loss: 0.581 | Acc: 68.829% \n",
      "[epoch:21, iter:2042] Loss: 0.581 | Acc: 68.571% \n",
      "[epoch:21, iter:2043] Loss: 0.580 | Acc: 68.558% \n",
      "[epoch:21, iter:2044] Loss: 0.578 | Acc: 68.682% \n",
      "[epoch:21, iter:2045] Loss: 0.577 | Acc: 68.800% \n",
      "[epoch:21, iter:2046] Loss: 0.576 | Acc: 68.739% \n",
      "[epoch:21, iter:2047] Loss: 0.575 | Acc: 68.723% \n",
      "[epoch:21, iter:2048] Loss: 0.573 | Acc: 68.958% \n",
      "[epoch:21, iter:2049] Loss: 0.575 | Acc: 68.980% \n",
      "[epoch:21, iter:2050] Loss: 0.576 | Acc: 68.880% \n",
      "[epoch:21, iter:2051] Loss: 0.577 | Acc: 68.863% \n",
      "[epoch:21, iter:2052] Loss: 0.578 | Acc: 68.923% \n",
      "[epoch:21, iter:2053] Loss: 0.576 | Acc: 69.170% \n",
      "[epoch:21, iter:2054] Loss: 0.575 | Acc: 69.333% \n",
      "[epoch:21, iter:2055] Loss: 0.586 | Acc: 69.200% \n",
      "[epoch:21, iter:2056] Loss: 0.586 | Acc: 69.143% \n",
      "[epoch:21, iter:2057] Loss: 0.585 | Acc: 69.158% \n",
      "[epoch:21, iter:2058] Loss: 0.584 | Acc: 69.172% \n",
      "[epoch:21, iter:2059] Loss: 0.584 | Acc: 68.983% \n",
      "[epoch:21, iter:2060] Loss: 0.583 | Acc: 69.067% \n",
      "[epoch:21, iter:2061] Loss: 0.582 | Acc: 69.115% \n",
      "[epoch:21, iter:2062] Loss: 0.582 | Acc: 69.097% \n",
      "[epoch:21, iter:2063] Loss: 0.581 | Acc: 69.111% \n",
      "[epoch:21, iter:2064] Loss: 0.583 | Acc: 69.094% \n",
      "[epoch:21, iter:2065] Loss: 0.584 | Acc: 69.138% \n",
      "[epoch:21, iter:2066] Loss: 0.584 | Acc: 69.182% \n",
      "[epoch:21, iter:2067] Loss: 0.583 | Acc: 69.224% \n",
      "[epoch:21, iter:2068] Loss: 0.582 | Acc: 69.088% \n",
      "[epoch:21, iter:2069] Loss: 0.583 | Acc: 69.072% \n",
      "[epoch:21, iter:2070] Loss: 0.582 | Acc: 69.200% \n",
      "[epoch:21, iter:2071] Loss: 0.582 | Acc: 69.183% \n",
      "[epoch:21, iter:2072] Loss: 0.581 | Acc: 69.194% \n",
      "[epoch:21, iter:2073] Loss: 0.581 | Acc: 69.233% \n",
      "[epoch:21, iter:2074] Loss: 0.581 | Acc: 69.216% \n",
      "[epoch:21, iter:2075] Loss: 0.580 | Acc: 69.360% \n",
      "[epoch:21, iter:2076] Loss: 0.580 | Acc: 69.368% \n",
      "[epoch:21, iter:2077] Loss: 0.579 | Acc: 69.299% \n",
      "[epoch:21, iter:2078] Loss: 0.579 | Acc: 69.231% \n",
      "[epoch:21, iter:2079] Loss: 0.579 | Acc: 69.114% \n",
      "[epoch:21, iter:2080] Loss: 0.579 | Acc: 69.050% \n",
      "[epoch:21, iter:2081] Loss: 0.578 | Acc: 69.111% \n",
      "[epoch:21, iter:2082] Loss: 0.577 | Acc: 69.146% \n",
      "[epoch:21, iter:2083] Loss: 0.577 | Acc: 69.229% \n",
      "[epoch:21, iter:2084] Loss: 0.576 | Acc: 69.286% \n",
      "[epoch:21, iter:2085] Loss: 0.575 | Acc: 69.412% \n",
      "[epoch:21, iter:2086] Loss: 0.575 | Acc: 69.395% \n",
      "[epoch:21, iter:2087] Loss: 0.575 | Acc: 69.356% \n",
      "[epoch:21, iter:2088] Loss: 0.573 | Acc: 69.409% \n",
      "[epoch:21, iter:2089] Loss: 0.572 | Acc: 69.416% \n",
      "[epoch:21, iter:2090] Loss: 0.574 | Acc: 69.378% \n",
      "[epoch:21, iter:2091] Loss: 0.573 | Acc: 69.407% \n",
      "[epoch:21, iter:2092] Loss: 0.573 | Acc: 69.370% \n",
      "[epoch:21, iter:2093] Loss: 0.572 | Acc: 69.333% \n",
      "[epoch:21, iter:2094] Loss: 0.571 | Acc: 69.447% \n",
      "[epoch:21, iter:2095] Loss: 0.571 | Acc: 69.558% \n",
      "[epoch:21, iter:2096] Loss: 0.573 | Acc: 69.542% \n",
      "[epoch:21, iter:2097] Loss: 0.572 | Acc: 69.423% \n",
      "[epoch:21, iter:2098] Loss: 0.571 | Acc: 69.551% \n",
      "[epoch:21, iter:2099] Loss: 0.572 | Acc: 69.495% \n",
      "[epoch:21, iter:2100] Loss: 0.572 | Acc: 69.480% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.720%\n",
      "Training set's accuracy (after quantization) is: 72.660%\n",
      "Test set's accuracy (before quantization) is: 69.900%\n",
      "Test set's accuracy (after quantization) is: 73.300%\n",
      "Train Loss: 0.590 | Train Acc: 69.720% | Test Loss: 0.592 | Test Acc: 69.900% \n",
      "Quantized Train Loss: 0.542 | Quantized Train Acc: 72.660% | Quantized Test Loss: 0.544 | Quantized Test Acc: 73.300% \n",
      "\n",
      "Epoch: 22\n",
      "[epoch:22, iter:2101] Loss: 0.529 | Acc: 70.000% \n",
      "[epoch:22, iter:2102] Loss: 0.533 | Acc: 71.000% \n",
      "[epoch:22, iter:2103] Loss: 0.554 | Acc: 68.000% \n",
      "[epoch:22, iter:2104] Loss: 0.578 | Acc: 68.000% \n",
      "[epoch:22, iter:2105] Loss: 0.566 | Acc: 69.600% \n",
      "[epoch:22, iter:2106] Loss: 0.558 | Acc: 70.667% \n",
      "[epoch:22, iter:2107] Loss: 0.564 | Acc: 71.429% \n",
      "[epoch:22, iter:2108] Loss: 0.574 | Acc: 69.750% \n",
      "[epoch:22, iter:2109] Loss: 0.576 | Acc: 68.889% \n",
      "[epoch:22, iter:2110] Loss: 0.577 | Acc: 69.000% \n",
      "[epoch:22, iter:2111] Loss: 0.576 | Acc: 68.545% \n",
      "[epoch:22, iter:2112] Loss: 0.585 | Acc: 68.667% \n",
      "[epoch:22, iter:2113] Loss: 0.589 | Acc: 68.923% \n",
      "[epoch:22, iter:2114] Loss: 0.586 | Acc: 68.714% \n",
      "[epoch:22, iter:2115] Loss: 0.586 | Acc: 68.133% \n",
      "[epoch:22, iter:2116] Loss: 0.587 | Acc: 67.375% \n",
      "[epoch:22, iter:2117] Loss: 0.582 | Acc: 68.235% \n",
      "[epoch:22, iter:2118] Loss: 0.582 | Acc: 68.111% \n",
      "[epoch:22, iter:2119] Loss: 0.586 | Acc: 67.579% \n",
      "[epoch:22, iter:2120] Loss: 0.590 | Acc: 67.300% \n",
      "[epoch:22, iter:2121] Loss: 0.587 | Acc: 67.429% \n",
      "[epoch:22, iter:2122] Loss: 0.582 | Acc: 67.727% \n",
      "[epoch:22, iter:2123] Loss: 0.580 | Acc: 67.739% \n",
      "[epoch:22, iter:2124] Loss: 0.576 | Acc: 67.583% \n",
      "[epoch:22, iter:2125] Loss: 0.572 | Acc: 67.840% \n",
      "[epoch:22, iter:2126] Loss: 0.571 | Acc: 67.538% \n",
      "[epoch:22, iter:2127] Loss: 0.568 | Acc: 67.926% \n",
      "[epoch:22, iter:2128] Loss: 0.568 | Acc: 67.929% \n",
      "[epoch:22, iter:2129] Loss: 0.573 | Acc: 67.931% \n",
      "[epoch:22, iter:2130] Loss: 0.569 | Acc: 68.133% \n",
      "[epoch:22, iter:2131] Loss: 0.566 | Acc: 67.935% \n",
      "[epoch:22, iter:2132] Loss: 0.566 | Acc: 68.000% \n",
      "[epoch:22, iter:2133] Loss: 0.563 | Acc: 68.242% \n",
      "[epoch:22, iter:2134] Loss: 0.563 | Acc: 68.235% \n",
      "[epoch:22, iter:2135] Loss: 0.564 | Acc: 68.000% \n",
      "[epoch:22, iter:2136] Loss: 0.562 | Acc: 68.222% \n",
      "[epoch:22, iter:2137] Loss: 0.562 | Acc: 68.270% \n",
      "[epoch:22, iter:2138] Loss: 0.563 | Acc: 68.211% \n",
      "[epoch:22, iter:2139] Loss: 0.564 | Acc: 68.154% \n",
      "[epoch:22, iter:2140] Loss: 0.561 | Acc: 68.400% \n",
      "[epoch:22, iter:2141] Loss: 0.562 | Acc: 68.439% \n",
      "[epoch:22, iter:2142] Loss: 0.563 | Acc: 68.429% \n",
      "[epoch:22, iter:2143] Loss: 0.563 | Acc: 68.558% \n",
      "[epoch:22, iter:2144] Loss: 0.561 | Acc: 68.682% \n",
      "[epoch:22, iter:2145] Loss: 0.559 | Acc: 68.889% \n",
      "[epoch:22, iter:2146] Loss: 0.564 | Acc: 68.783% \n",
      "[epoch:22, iter:2147] Loss: 0.565 | Acc: 68.553% \n",
      "[epoch:22, iter:2148] Loss: 0.567 | Acc: 68.375% \n",
      "[epoch:22, iter:2149] Loss: 0.563 | Acc: 68.531% \n",
      "[epoch:22, iter:2150] Loss: 0.564 | Acc: 68.600% \n",
      "[epoch:22, iter:2151] Loss: 0.562 | Acc: 68.706% \n",
      "[epoch:22, iter:2152] Loss: 0.562 | Acc: 68.769% \n",
      "[epoch:22, iter:2153] Loss: 0.563 | Acc: 68.604% \n",
      "[epoch:22, iter:2154] Loss: 0.562 | Acc: 68.630% \n",
      "[epoch:22, iter:2155] Loss: 0.563 | Acc: 68.582% \n",
      "[epoch:22, iter:2156] Loss: 0.561 | Acc: 68.786% \n",
      "[epoch:22, iter:2157] Loss: 0.561 | Acc: 68.947% \n",
      "[epoch:22, iter:2158] Loss: 0.567 | Acc: 69.000% \n",
      "[epoch:22, iter:2159] Loss: 0.566 | Acc: 68.983% \n",
      "[epoch:22, iter:2160] Loss: 0.564 | Acc: 69.167% \n",
      "[epoch:22, iter:2161] Loss: 0.564 | Acc: 69.049% \n",
      "[epoch:22, iter:2162] Loss: 0.566 | Acc: 68.903% \n",
      "[epoch:22, iter:2163] Loss: 0.565 | Acc: 69.079% \n",
      "[epoch:22, iter:2164] Loss: 0.566 | Acc: 69.156% \n",
      "[epoch:22, iter:2165] Loss: 0.565 | Acc: 69.262% \n",
      "[epoch:22, iter:2166] Loss: 0.565 | Acc: 69.242% \n",
      "[epoch:22, iter:2167] Loss: 0.567 | Acc: 69.463% \n",
      "[epoch:22, iter:2168] Loss: 0.576 | Acc: 69.441% \n",
      "[epoch:22, iter:2169] Loss: 0.576 | Acc: 69.507% \n",
      "[epoch:22, iter:2170] Loss: 0.574 | Acc: 69.457% \n",
      "[epoch:22, iter:2171] Loss: 0.575 | Acc: 69.352% \n",
      "[epoch:22, iter:2172] Loss: 0.575 | Acc: 69.306% \n",
      "[epoch:22, iter:2173] Loss: 0.574 | Acc: 69.397% \n",
      "[epoch:22, iter:2174] Loss: 0.573 | Acc: 69.486% \n",
      "[epoch:22, iter:2175] Loss: 0.572 | Acc: 69.547% \n",
      "[epoch:22, iter:2176] Loss: 0.574 | Acc: 69.553% \n",
      "[epoch:22, iter:2177] Loss: 0.576 | Acc: 69.455% \n",
      "[epoch:22, iter:2178] Loss: 0.576 | Acc: 69.513% \n",
      "[epoch:22, iter:2179] Loss: 0.574 | Acc: 69.570% \n",
      "[epoch:22, iter:2180] Loss: 0.574 | Acc: 69.675% \n",
      "[epoch:22, iter:2181] Loss: 0.573 | Acc: 69.802% \n",
      "[epoch:22, iter:2182] Loss: 0.572 | Acc: 69.829% \n",
      "[epoch:22, iter:2183] Loss: 0.571 | Acc: 69.904% \n",
      "[epoch:22, iter:2184] Loss: 0.570 | Acc: 69.905% \n",
      "[epoch:22, iter:2185] Loss: 0.569 | Acc: 70.024% \n",
      "[epoch:22, iter:2186] Loss: 0.568 | Acc: 70.070% \n",
      "[epoch:22, iter:2187] Loss: 0.568 | Acc: 70.023% \n",
      "[epoch:22, iter:2188] Loss: 0.567 | Acc: 70.023% \n",
      "[epoch:22, iter:2189] Loss: 0.568 | Acc: 69.910% \n",
      "[epoch:22, iter:2190] Loss: 0.570 | Acc: 69.800% \n",
      "[epoch:22, iter:2191] Loss: 0.570 | Acc: 69.890% \n",
      "[epoch:22, iter:2192] Loss: 0.569 | Acc: 70.000% \n",
      "[epoch:22, iter:2193] Loss: 0.569 | Acc: 69.935% \n",
      "[epoch:22, iter:2194] Loss: 0.568 | Acc: 70.000% \n",
      "[epoch:22, iter:2195] Loss: 0.568 | Acc: 69.937% \n",
      "[epoch:22, iter:2196] Loss: 0.567 | Acc: 69.917% \n",
      "[epoch:22, iter:2197] Loss: 0.567 | Acc: 69.814% \n",
      "[epoch:22, iter:2198] Loss: 0.567 | Acc: 69.776% \n",
      "[epoch:22, iter:2199] Loss: 0.566 | Acc: 69.798% \n",
      "[epoch:22, iter:2200] Loss: 0.566 | Acc: 69.840% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.580%\n",
      "Training set's accuracy (after quantization) is: 72.900%\n",
      "Test set's accuracy (before quantization) is: 69.700%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.596 | Train Acc: 69.580% | Test Loss: 0.599 | Test Acc: 69.700% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.900% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 23\n",
      "[epoch:23, iter:2201] Loss: 0.641 | Acc: 66.000% \n",
      "[epoch:23, iter:2202] Loss: 0.597 | Acc: 70.000% \n",
      "[epoch:23, iter:2203] Loss: 0.615 | Acc: 65.333% \n",
      "[epoch:23, iter:2204] Loss: 0.590 | Acc: 69.000% \n",
      "[epoch:23, iter:2205] Loss: 0.588 | Acc: 69.600% \n",
      "[epoch:23, iter:2206] Loss: 0.578 | Acc: 70.000% \n",
      "[epoch:23, iter:2207] Loss: 0.576 | Acc: 70.000% \n",
      "[epoch:23, iter:2208] Loss: 0.584 | Acc: 70.500% \n",
      "[epoch:23, iter:2209] Loss: 0.576 | Acc: 70.667% \n",
      "[epoch:23, iter:2210] Loss: 0.577 | Acc: 70.200% \n",
      "[epoch:23, iter:2211] Loss: 0.574 | Acc: 70.364% \n",
      "[epoch:23, iter:2212] Loss: 0.567 | Acc: 70.500% \n",
      "[epoch:23, iter:2213] Loss: 0.563 | Acc: 71.077% \n",
      "[epoch:23, iter:2214] Loss: 0.557 | Acc: 71.286% \n",
      "[epoch:23, iter:2215] Loss: 0.551 | Acc: 71.733% \n",
      "[epoch:23, iter:2216] Loss: 0.557 | Acc: 71.750% \n",
      "[epoch:23, iter:2217] Loss: 0.551 | Acc: 71.176% \n",
      "[epoch:23, iter:2218] Loss: 0.554 | Acc: 71.333% \n",
      "[epoch:23, iter:2219] Loss: 0.551 | Acc: 71.053% \n",
      "[epoch:23, iter:2220] Loss: 0.548 | Acc: 71.100% \n",
      "[epoch:23, iter:2221] Loss: 0.549 | Acc: 70.952% \n",
      "[epoch:23, iter:2222] Loss: 0.548 | Acc: 70.909% \n",
      "[epoch:23, iter:2223] Loss: 0.546 | Acc: 70.870% \n",
      "[epoch:23, iter:2224] Loss: 0.547 | Acc: 70.500% \n",
      "[epoch:23, iter:2225] Loss: 0.552 | Acc: 70.160% \n",
      "[epoch:23, iter:2226] Loss: 0.557 | Acc: 69.769% \n",
      "[epoch:23, iter:2227] Loss: 0.564 | Acc: 69.852% \n",
      "[epoch:23, iter:2228] Loss: 0.574 | Acc: 69.571% \n",
      "[epoch:23, iter:2229] Loss: 0.579 | Acc: 69.103% \n",
      "[epoch:23, iter:2230] Loss: 0.582 | Acc: 68.933% \n",
      "[epoch:23, iter:2231] Loss: 0.586 | Acc: 68.774% \n",
      "[epoch:23, iter:2232] Loss: 0.589 | Acc: 68.375% \n",
      "[epoch:23, iter:2233] Loss: 0.588 | Acc: 68.485% \n",
      "[epoch:23, iter:2234] Loss: 0.593 | Acc: 68.353% \n",
      "[epoch:23, iter:2235] Loss: 0.600 | Acc: 68.171% \n",
      "[epoch:23, iter:2236] Loss: 0.603 | Acc: 68.333% \n",
      "[epoch:23, iter:2237] Loss: 0.603 | Acc: 68.270% \n",
      "[epoch:23, iter:2238] Loss: 0.601 | Acc: 68.316% \n",
      "[epoch:23, iter:2239] Loss: 0.601 | Acc: 68.308% \n",
      "[epoch:23, iter:2240] Loss: 0.599 | Acc: 68.600% \n",
      "[epoch:23, iter:2241] Loss: 0.603 | Acc: 68.585% \n",
      "[epoch:23, iter:2242] Loss: 0.599 | Acc: 69.000% \n",
      "[epoch:23, iter:2243] Loss: 0.598 | Acc: 68.884% \n",
      "[epoch:23, iter:2244] Loss: 0.596 | Acc: 69.000% \n",
      "[epoch:23, iter:2245] Loss: 0.594 | Acc: 69.200% \n",
      "[epoch:23, iter:2246] Loss: 0.591 | Acc: 69.391% \n",
      "[epoch:23, iter:2247] Loss: 0.592 | Acc: 69.234% \n",
      "[epoch:23, iter:2248] Loss: 0.589 | Acc: 69.292% \n",
      "[epoch:23, iter:2249] Loss: 0.586 | Acc: 69.469% \n",
      "[epoch:23, iter:2250] Loss: 0.588 | Acc: 69.480% \n",
      "[epoch:23, iter:2251] Loss: 0.586 | Acc: 69.647% \n",
      "[epoch:23, iter:2252] Loss: 0.584 | Acc: 69.615% \n",
      "[epoch:23, iter:2253] Loss: 0.581 | Acc: 69.774% \n",
      "[epoch:23, iter:2254] Loss: 0.580 | Acc: 69.889% \n",
      "[epoch:23, iter:2255] Loss: 0.580 | Acc: 69.782% \n",
      "[epoch:23, iter:2256] Loss: 0.580 | Acc: 69.750% \n",
      "[epoch:23, iter:2257] Loss: 0.580 | Acc: 69.684% \n",
      "[epoch:23, iter:2258] Loss: 0.579 | Acc: 69.724% \n",
      "[epoch:23, iter:2259] Loss: 0.579 | Acc: 69.593% \n",
      "[epoch:23, iter:2260] Loss: 0.577 | Acc: 69.700% \n",
      "[epoch:23, iter:2261] Loss: 0.575 | Acc: 69.672% \n",
      "[epoch:23, iter:2262] Loss: 0.574 | Acc: 69.613% \n",
      "[epoch:23, iter:2263] Loss: 0.574 | Acc: 69.556% \n",
      "[epoch:23, iter:2264] Loss: 0.575 | Acc: 69.500% \n",
      "[epoch:23, iter:2265] Loss: 0.574 | Acc: 69.446% \n",
      "[epoch:23, iter:2266] Loss: 0.575 | Acc: 69.333% \n",
      "[epoch:23, iter:2267] Loss: 0.573 | Acc: 69.493% \n",
      "[epoch:23, iter:2268] Loss: 0.572 | Acc: 69.441% \n",
      "[epoch:23, iter:2269] Loss: 0.575 | Acc: 69.275% \n",
      "[epoch:23, iter:2270] Loss: 0.575 | Acc: 69.343% \n",
      "[epoch:23, iter:2271] Loss: 0.574 | Acc: 69.465% \n",
      "[epoch:23, iter:2272] Loss: 0.573 | Acc: 69.528% \n",
      "[epoch:23, iter:2273] Loss: 0.573 | Acc: 69.452% \n",
      "[epoch:23, iter:2274] Loss: 0.572 | Acc: 69.595% \n",
      "[epoch:23, iter:2275] Loss: 0.571 | Acc: 69.573% \n",
      "[epoch:23, iter:2276] Loss: 0.571 | Acc: 69.395% \n",
      "[epoch:23, iter:2277] Loss: 0.570 | Acc: 69.506% \n",
      "[epoch:23, iter:2278] Loss: 0.569 | Acc: 69.513% \n",
      "[epoch:23, iter:2279] Loss: 0.568 | Acc: 69.570% \n",
      "[epoch:23, iter:2280] Loss: 0.568 | Acc: 69.575% \n",
      "[epoch:23, iter:2281] Loss: 0.568 | Acc: 69.605% \n",
      "[epoch:23, iter:2282] Loss: 0.569 | Acc: 69.512% \n",
      "[epoch:23, iter:2283] Loss: 0.568 | Acc: 69.590% \n",
      "[epoch:23, iter:2284] Loss: 0.567 | Acc: 69.714% \n",
      "[epoch:23, iter:2285] Loss: 0.567 | Acc: 69.741% \n",
      "[epoch:23, iter:2286] Loss: 0.566 | Acc: 69.674% \n",
      "[epoch:23, iter:2287] Loss: 0.566 | Acc: 69.632% \n",
      "[epoch:23, iter:2288] Loss: 0.567 | Acc: 69.614% \n",
      "[epoch:23, iter:2289] Loss: 0.565 | Acc: 69.730% \n",
      "[epoch:23, iter:2290] Loss: 0.566 | Acc: 69.667% \n",
      "[epoch:23, iter:2291] Loss: 0.565 | Acc: 69.648% \n",
      "[epoch:23, iter:2292] Loss: 0.565 | Acc: 69.652% \n",
      "[epoch:23, iter:2293] Loss: 0.564 | Acc: 69.656% \n",
      "[epoch:23, iter:2294] Loss: 0.565 | Acc: 69.596% \n",
      "[epoch:23, iter:2295] Loss: 0.564 | Acc: 69.579% \n",
      "[epoch:23, iter:2296] Loss: 0.562 | Acc: 69.667% \n",
      "[epoch:23, iter:2297] Loss: 0.562 | Acc: 69.691% \n",
      "[epoch:23, iter:2298] Loss: 0.562 | Acc: 69.653% \n",
      "[epoch:23, iter:2299] Loss: 0.561 | Acc: 69.596% \n",
      "[epoch:23, iter:2300] Loss: 0.561 | Acc: 69.540% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.320%\n",
      "Training set's accuracy (after quantization) is: 73.000%\n",
      "Test set's accuracy (before quantization) is: 69.800%\n",
      "Test set's accuracy (after quantization) is: 73.100%\n",
      "Train Loss: 0.619 | Train Acc: 69.320% | Test Loss: 0.621 | Test Acc: 69.800% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 73.000% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.100% \n",
      "\n",
      "Epoch: 24\n",
      "[epoch:24, iter:2301] Loss: 0.552 | Acc: 64.000% \n",
      "[epoch:24, iter:2302] Loss: 0.544 | Acc: 67.000% \n",
      "[epoch:24, iter:2303] Loss: 0.638 | Acc: 64.667% \n",
      "[epoch:24, iter:2304] Loss: 0.611 | Acc: 66.500% \n",
      "[epoch:24, iter:2305] Loss: 0.598 | Acc: 68.400% \n",
      "[epoch:24, iter:2306] Loss: 0.590 | Acc: 68.667% \n",
      "[epoch:24, iter:2307] Loss: 0.586 | Acc: 68.000% \n",
      "[epoch:24, iter:2308] Loss: 0.592 | Acc: 68.250% \n",
      "[epoch:24, iter:2309] Loss: 0.595 | Acc: 67.111% \n",
      "[epoch:24, iter:2310] Loss: 0.596 | Acc: 67.200% \n",
      "[epoch:24, iter:2311] Loss: 0.587 | Acc: 67.818% \n",
      "[epoch:24, iter:2312] Loss: 0.586 | Acc: 67.667% \n",
      "[epoch:24, iter:2313] Loss: 0.581 | Acc: 68.000% \n",
      "[epoch:24, iter:2314] Loss: 0.581 | Acc: 68.429% \n",
      "[epoch:24, iter:2315] Loss: 0.576 | Acc: 68.267% \n",
      "[epoch:24, iter:2316] Loss: 0.576 | Acc: 68.000% \n",
      "[epoch:24, iter:2317] Loss: 0.577 | Acc: 67.882% \n",
      "[epoch:24, iter:2318] Loss: 0.575 | Acc: 67.778% \n",
      "[epoch:24, iter:2319] Loss: 0.577 | Acc: 67.895% \n",
      "[epoch:24, iter:2320] Loss: 0.576 | Acc: 68.200% \n",
      "[epoch:24, iter:2321] Loss: 0.578 | Acc: 68.190% \n",
      "[epoch:24, iter:2322] Loss: 0.572 | Acc: 68.636% \n",
      "[epoch:24, iter:2323] Loss: 0.570 | Acc: 68.957% \n",
      "[epoch:24, iter:2324] Loss: 0.569 | Acc: 68.417% \n",
      "[epoch:24, iter:2325] Loss: 0.568 | Acc: 68.240% \n",
      "[epoch:24, iter:2326] Loss: 0.568 | Acc: 68.231% \n",
      "[epoch:24, iter:2327] Loss: 0.569 | Acc: 67.852% \n",
      "[epoch:24, iter:2328] Loss: 0.570 | Acc: 67.857% \n",
      "[epoch:24, iter:2329] Loss: 0.565 | Acc: 68.345% \n",
      "[epoch:24, iter:2330] Loss: 0.566 | Acc: 68.333% \n",
      "[epoch:24, iter:2331] Loss: 0.564 | Acc: 68.581% \n",
      "[epoch:24, iter:2332] Loss: 0.562 | Acc: 68.938% \n",
      "[epoch:24, iter:2333] Loss: 0.561 | Acc: 68.788% \n",
      "[epoch:24, iter:2334] Loss: 0.560 | Acc: 68.824% \n",
      "[epoch:24, iter:2335] Loss: 0.557 | Acc: 68.857% \n",
      "[epoch:24, iter:2336] Loss: 0.556 | Acc: 68.722% \n",
      "[epoch:24, iter:2337] Loss: 0.555 | Acc: 68.811% \n",
      "[epoch:24, iter:2338] Loss: 0.557 | Acc: 68.684% \n",
      "[epoch:24, iter:2339] Loss: 0.556 | Acc: 68.769% \n",
      "[epoch:24, iter:2340] Loss: 0.554 | Acc: 69.000% \n",
      "[epoch:24, iter:2341] Loss: 0.553 | Acc: 68.927% \n",
      "[epoch:24, iter:2342] Loss: 0.556 | Acc: 68.857% \n",
      "[epoch:24, iter:2343] Loss: 0.554 | Acc: 68.930% \n",
      "[epoch:24, iter:2344] Loss: 0.552 | Acc: 69.091% \n",
      "[epoch:24, iter:2345] Loss: 0.551 | Acc: 69.156% \n",
      "[epoch:24, iter:2346] Loss: 0.554 | Acc: 69.043% \n",
      "[epoch:24, iter:2347] Loss: 0.554 | Acc: 68.851% \n",
      "[epoch:24, iter:2348] Loss: 0.554 | Acc: 68.833% \n",
      "[epoch:24, iter:2349] Loss: 0.553 | Acc: 68.980% \n",
      "[epoch:24, iter:2350] Loss: 0.551 | Acc: 69.120% \n",
      "[epoch:24, iter:2351] Loss: 0.549 | Acc: 69.373% \n",
      "[epoch:24, iter:2352] Loss: 0.548 | Acc: 69.308% \n",
      "[epoch:24, iter:2353] Loss: 0.545 | Acc: 69.434% \n",
      "[epoch:24, iter:2354] Loss: 0.549 | Acc: 69.296% \n",
      "[epoch:24, iter:2355] Loss: 0.548 | Acc: 69.273% \n",
      "[epoch:24, iter:2356] Loss: 0.548 | Acc: 69.250% \n",
      "[epoch:24, iter:2357] Loss: 0.546 | Acc: 69.333% \n",
      "[epoch:24, iter:2358] Loss: 0.546 | Acc: 69.345% \n",
      "[epoch:24, iter:2359] Loss: 0.546 | Acc: 69.390% \n",
      "[epoch:24, iter:2360] Loss: 0.545 | Acc: 69.400% \n",
      "[epoch:24, iter:2361] Loss: 0.546 | Acc: 69.311% \n",
      "[epoch:24, iter:2362] Loss: 0.545 | Acc: 69.355% \n",
      "[epoch:24, iter:2363] Loss: 0.545 | Acc: 69.460% \n",
      "[epoch:24, iter:2364] Loss: 0.545 | Acc: 69.438% \n",
      "[epoch:24, iter:2365] Loss: 0.544 | Acc: 69.569% \n",
      "[epoch:24, iter:2366] Loss: 0.546 | Acc: 69.636% \n",
      "[epoch:24, iter:2367] Loss: 0.546 | Acc: 69.552% \n",
      "[epoch:24, iter:2368] Loss: 0.546 | Acc: 69.529% \n",
      "[epoch:24, iter:2369] Loss: 0.545 | Acc: 69.507% \n",
      "[epoch:24, iter:2370] Loss: 0.546 | Acc: 69.429% \n",
      "[epoch:24, iter:2371] Loss: 0.547 | Acc: 69.408% \n",
      "[epoch:24, iter:2372] Loss: 0.546 | Acc: 69.333% \n",
      "[epoch:24, iter:2373] Loss: 0.546 | Acc: 69.397% \n",
      "[epoch:24, iter:2374] Loss: 0.545 | Acc: 69.432% \n",
      "[epoch:24, iter:2375] Loss: 0.547 | Acc: 69.413% \n",
      "[epoch:24, iter:2376] Loss: 0.546 | Acc: 69.395% \n",
      "[epoch:24, iter:2377] Loss: 0.546 | Acc: 69.195% \n",
      "[epoch:24, iter:2378] Loss: 0.545 | Acc: 69.256% \n",
      "[epoch:24, iter:2379] Loss: 0.547 | Acc: 69.063% \n",
      "[epoch:24, iter:2380] Loss: 0.545 | Acc: 69.200% \n",
      "[epoch:24, iter:2381] Loss: 0.545 | Acc: 69.185% \n",
      "[epoch:24, iter:2382] Loss: 0.544 | Acc: 69.220% \n",
      "[epoch:24, iter:2383] Loss: 0.544 | Acc: 69.277% \n",
      "[epoch:24, iter:2384] Loss: 0.545 | Acc: 69.286% \n",
      "[epoch:24, iter:2385] Loss: 0.544 | Acc: 69.388% \n",
      "[epoch:24, iter:2386] Loss: 0.543 | Acc: 69.535% \n",
      "[epoch:24, iter:2387] Loss: 0.544 | Acc: 69.586% \n",
      "[epoch:24, iter:2388] Loss: 0.545 | Acc: 69.568% \n",
      "[epoch:24, iter:2389] Loss: 0.543 | Acc: 69.640% \n",
      "[epoch:24, iter:2390] Loss: 0.544 | Acc: 69.578% \n",
      "[epoch:24, iter:2391] Loss: 0.544 | Acc: 69.516% \n",
      "[epoch:24, iter:2392] Loss: 0.544 | Acc: 69.565% \n",
      "[epoch:24, iter:2393] Loss: 0.546 | Acc: 69.527% \n",
      "[epoch:24, iter:2394] Loss: 0.547 | Acc: 69.426% \n",
      "[epoch:24, iter:2395] Loss: 0.547 | Acc: 69.495% \n",
      "[epoch:24, iter:2396] Loss: 0.547 | Acc: 69.521% \n",
      "[epoch:24, iter:2397] Loss: 0.547 | Acc: 69.567% \n",
      "[epoch:24, iter:2398] Loss: 0.547 | Acc: 69.490% \n",
      "[epoch:24, iter:2399] Loss: 0.548 | Acc: 69.434% \n",
      "[epoch:24, iter:2400] Loss: 0.549 | Acc: 69.400% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 70.000%\n",
      "Training set's accuracy (after quantization) is: 72.760%\n",
      "Test set's accuracy (before quantization) is: 70.500%\n",
      "Test set's accuracy (after quantization) is: 73.300%\n",
      "Train Loss: 0.577 | Train Acc: 70.000% | Test Loss: 0.579 | Test Acc: 70.500% \n",
      "Quantized Train Loss: 0.542 | Quantized Train Acc: 72.760% | Quantized Test Loss: 0.544 | Quantized Test Acc: 73.300% \n",
      "\n",
      "Epoch: 25\n",
      "[epoch:25, iter:2401] Loss: 0.570 | Acc: 64.000% \n",
      "[epoch:25, iter:2402] Loss: 0.533 | Acc: 71.000% \n",
      "[epoch:25, iter:2403] Loss: 0.567 | Acc: 66.000% \n",
      "[epoch:25, iter:2404] Loss: 0.571 | Acc: 65.500% \n",
      "[epoch:25, iter:2405] Loss: 0.558 | Acc: 67.600% \n",
      "[epoch:25, iter:2406] Loss: 0.550 | Acc: 68.667% \n",
      "[epoch:25, iter:2407] Loss: 0.539 | Acc: 71.143% \n",
      "[epoch:25, iter:2408] Loss: 0.528 | Acc: 70.500% \n",
      "[epoch:25, iter:2409] Loss: 0.529 | Acc: 69.778% \n",
      "[epoch:25, iter:2410] Loss: 0.532 | Acc: 69.600% \n",
      "[epoch:25, iter:2411] Loss: 0.532 | Acc: 69.273% \n",
      "[epoch:25, iter:2412] Loss: 0.527 | Acc: 69.667% \n",
      "[epoch:25, iter:2413] Loss: 0.528 | Acc: 69.538% \n",
      "[epoch:25, iter:2414] Loss: 0.543 | Acc: 68.286% \n",
      "[epoch:25, iter:2415] Loss: 0.540 | Acc: 68.267% \n",
      "[epoch:25, iter:2416] Loss: 0.544 | Acc: 68.250% \n",
      "[epoch:25, iter:2417] Loss: 0.545 | Acc: 68.235% \n",
      "[epoch:25, iter:2418] Loss: 0.545 | Acc: 68.556% \n",
      "[epoch:25, iter:2419] Loss: 0.541 | Acc: 68.947% \n",
      "[epoch:25, iter:2420] Loss: 0.537 | Acc: 69.300% \n",
      "[epoch:25, iter:2421] Loss: 0.539 | Acc: 69.143% \n",
      "[epoch:25, iter:2422] Loss: 0.550 | Acc: 68.818% \n",
      "[epoch:25, iter:2423] Loss: 0.547 | Acc: 69.043% \n",
      "[epoch:25, iter:2424] Loss: 0.545 | Acc: 69.250% \n",
      "[epoch:25, iter:2425] Loss: 0.542 | Acc: 69.440% \n",
      "[epoch:25, iter:2426] Loss: 0.541 | Acc: 69.615% \n",
      "[epoch:25, iter:2427] Loss: 0.537 | Acc: 69.778% \n",
      "[epoch:25, iter:2428] Loss: 0.539 | Acc: 69.643% \n",
      "[epoch:25, iter:2429] Loss: 0.537 | Acc: 69.655% \n",
      "[epoch:25, iter:2430] Loss: 0.539 | Acc: 69.267% \n",
      "[epoch:25, iter:2431] Loss: 0.540 | Acc: 69.032% \n",
      "[epoch:25, iter:2432] Loss: 0.537 | Acc: 69.250% \n",
      "[epoch:25, iter:2433] Loss: 0.536 | Acc: 69.273% \n",
      "[epoch:25, iter:2434] Loss: 0.536 | Acc: 69.235% \n",
      "[epoch:25, iter:2435] Loss: 0.540 | Acc: 69.200% \n",
      "[epoch:25, iter:2436] Loss: 0.542 | Acc: 69.111% \n",
      "[epoch:25, iter:2437] Loss: 0.540 | Acc: 69.297% \n",
      "[epoch:25, iter:2438] Loss: 0.538 | Acc: 69.263% \n",
      "[epoch:25, iter:2439] Loss: 0.539 | Acc: 69.231% \n",
      "[epoch:25, iter:2440] Loss: 0.539 | Acc: 69.200% \n",
      "[epoch:25, iter:2441] Loss: 0.538 | Acc: 69.171% \n",
      "[epoch:25, iter:2442] Loss: 0.545 | Acc: 69.429% \n",
      "[epoch:25, iter:2443] Loss: 0.546 | Acc: 69.535% \n",
      "[epoch:25, iter:2444] Loss: 0.548 | Acc: 69.318% \n",
      "[epoch:25, iter:2445] Loss: 0.547 | Acc: 69.289% \n",
      "[epoch:25, iter:2446] Loss: 0.547 | Acc: 69.348% \n",
      "[epoch:25, iter:2447] Loss: 0.547 | Acc: 69.191% \n",
      "[epoch:25, iter:2448] Loss: 0.548 | Acc: 69.208% \n",
      "[epoch:25, iter:2449] Loss: 0.546 | Acc: 69.429% \n",
      "[epoch:25, iter:2450] Loss: 0.545 | Acc: 69.560% \n",
      "[epoch:25, iter:2451] Loss: 0.548 | Acc: 69.451% \n",
      "[epoch:25, iter:2452] Loss: 0.547 | Acc: 69.462% \n",
      "[epoch:25, iter:2453] Loss: 0.549 | Acc: 69.472% \n",
      "[epoch:25, iter:2454] Loss: 0.549 | Acc: 69.370% \n",
      "[epoch:25, iter:2455] Loss: 0.549 | Acc: 69.345% \n",
      "[epoch:25, iter:2456] Loss: 0.550 | Acc: 69.143% \n",
      "[epoch:25, iter:2457] Loss: 0.548 | Acc: 69.404% \n",
      "[epoch:25, iter:2458] Loss: 0.547 | Acc: 69.448% \n",
      "[epoch:25, iter:2459] Loss: 0.547 | Acc: 69.627% \n",
      "[epoch:25, iter:2460] Loss: 0.549 | Acc: 69.400% \n",
      "[epoch:25, iter:2461] Loss: 0.550 | Acc: 69.377% \n",
      "[epoch:25, iter:2462] Loss: 0.549 | Acc: 69.387% \n",
      "[epoch:25, iter:2463] Loss: 0.551 | Acc: 69.270% \n",
      "[epoch:25, iter:2464] Loss: 0.551 | Acc: 69.312% \n",
      "[epoch:25, iter:2465] Loss: 0.551 | Acc: 69.354% \n",
      "[epoch:25, iter:2466] Loss: 0.552 | Acc: 69.273% \n",
      "[epoch:25, iter:2467] Loss: 0.553 | Acc: 69.224% \n",
      "[epoch:25, iter:2468] Loss: 0.554 | Acc: 69.265% \n",
      "[epoch:25, iter:2469] Loss: 0.554 | Acc: 69.304% \n",
      "[epoch:25, iter:2470] Loss: 0.555 | Acc: 69.257% \n",
      "[epoch:25, iter:2471] Loss: 0.555 | Acc: 69.239% \n",
      "[epoch:25, iter:2472] Loss: 0.555 | Acc: 69.333% \n",
      "[epoch:25, iter:2473] Loss: 0.558 | Acc: 69.233% \n",
      "[epoch:25, iter:2474] Loss: 0.558 | Acc: 69.297% \n",
      "[epoch:25, iter:2475] Loss: 0.559 | Acc: 69.200% \n",
      "[epoch:25, iter:2476] Loss: 0.559 | Acc: 69.158% \n",
      "[epoch:25, iter:2477] Loss: 0.558 | Acc: 69.221% \n",
      "[epoch:25, iter:2478] Loss: 0.558 | Acc: 69.179% \n",
      "[epoch:25, iter:2479] Loss: 0.557 | Acc: 69.089% \n",
      "[epoch:25, iter:2480] Loss: 0.557 | Acc: 69.125% \n",
      "[epoch:25, iter:2481] Loss: 0.558 | Acc: 69.185% \n",
      "[epoch:25, iter:2482] Loss: 0.558 | Acc: 69.073% \n",
      "[epoch:25, iter:2483] Loss: 0.557 | Acc: 69.157% \n",
      "[epoch:25, iter:2484] Loss: 0.560 | Acc: 69.095% \n",
      "[epoch:25, iter:2485] Loss: 0.559 | Acc: 69.106% \n",
      "[epoch:25, iter:2486] Loss: 0.558 | Acc: 69.209% \n",
      "[epoch:25, iter:2487] Loss: 0.558 | Acc: 69.195% \n",
      "[epoch:25, iter:2488] Loss: 0.559 | Acc: 69.205% \n",
      "[epoch:25, iter:2489] Loss: 0.559 | Acc: 69.236% \n",
      "[epoch:25, iter:2490] Loss: 0.559 | Acc: 69.356% \n",
      "[epoch:25, iter:2491] Loss: 0.563 | Acc: 69.341% \n",
      "[epoch:25, iter:2492] Loss: 0.562 | Acc: 69.413% \n",
      "[epoch:25, iter:2493] Loss: 0.562 | Acc: 69.462% \n",
      "[epoch:25, iter:2494] Loss: 0.561 | Acc: 69.532% \n",
      "[epoch:25, iter:2495] Loss: 0.562 | Acc: 69.579% \n",
      "[epoch:25, iter:2496] Loss: 0.563 | Acc: 69.521% \n",
      "[epoch:25, iter:2497] Loss: 0.563 | Acc: 69.526% \n",
      "[epoch:25, iter:2498] Loss: 0.562 | Acc: 69.531% \n",
      "[epoch:25, iter:2499] Loss: 0.561 | Acc: 69.657% \n",
      "[epoch:25, iter:2500] Loss: 0.560 | Acc: 69.660% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 69.120%\n",
      "Training set's accuracy (after quantization) is: 73.020%\n",
      "Test set's accuracy (before quantization) is: 69.400%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.647 | Train Acc: 69.120% | Test Loss: 0.650 | Test Acc: 69.400% \n",
      "Quantized Train Loss: 0.540 | Quantized Train Acc: 73.020% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.200% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0e74d4e69f4834bf923f23aebc3158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.134 MB uploaded\\r'), FloatProgress(value=0.009976111527079544, max=1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td></td></tr><tr><td>batch_gradient</td><td></td></tr><tr><td>quantized_accuracy</td><td></td></tr><tr><td>quantized_test_accuracy</td><td></td></tr><tr><td>quantized_test_loss</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>weight_distance</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>69.12</td></tr><tr><td>batch_gradient</td><td>0.25292</td></tr><tr><td>quantized_accuracy</td><td>73.02</td></tr><tr><td>quantized_test_accuracy</td><td>73.2</td></tr><tr><td>quantized_test_loss</td><td>0.54253</td></tr><tr><td>test_accuracy</td><td>69.4</td></tr><tr><td>test_loss</td><td>0.6501</td></tr><tr><td>weight_distance</td><td>1.86721</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Deterministic BinaryConnect</strong> at: <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/1509m89r' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/1509m89r</a><br/> View project at: <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250711_035428-1509m89r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deterministic BinaryConnect\n",
    "net, optimizer = init(project_name=\"LogisticRegression_binary\", opt_name=\"Deterministic BinaryConnect\", batch_size=BATCH_SIZE, architecture=\"MLP\", dataset_name=\"LogisticRegression\", lr=LR)\n",
    "\n",
    "model_copy = copy.deepcopy(net)\n",
    "\n",
    "lr_decay_epochs = [30]\n",
    "for decay_epoch in lr_decay_epochs:\n",
    "    if pre_epoch > decay_epoch:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] *= 0.5\n",
    "\n",
    "# Train\n",
    "for epoch in range(pre_epoch, EPOCH):\n",
    "    print(\"\\nEpoch: %d\" % (epoch + 1))\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    if epoch in lr_decay_epochs:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] *= 0.5\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # prepare dataset\n",
    "        length = len(trainloader)\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # forward & backward\n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs).squeeze()\n",
    "            loss2 = criterion(outputs, labels)\n",
    "            for net_name, net_param in net.named_parameters():\n",
    "                if not net_name.endswith(\".bias\"):\n",
    "                    net_param.data=net.quantize(net_param.data, net.bit_width)\n",
    "        outputs2 = net(inputs).squeeze()\n",
    "        loss = criterion(outputs2, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for (net_name, net_param), (model_copy_name, model_copy_param) in zip(\n",
    "            net.named_parameters(), model_copy.named_parameters()\n",
    "        ):\n",
    "            if not net_name.endswith(\".bias\"):\n",
    "                delta = net_param.data - model_copy.quantize(model_copy_param.data, model_copy.bit_width)\n",
    "                net_param.data = torch.clamp(model_copy_param.data + delta, -1, 1)\n",
    "                model_copy_param.data = torch.clamp(model_copy_param.data + delta, -1, 1)\n",
    "        sum_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        predicted = torch.where(outputs.data>0.5, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        print(\n",
    "            \"[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% \"\n",
    "            % (\n",
    "                epoch + 1,\n",
    "                (i + 1 + (epoch) * length),\n",
    "                sum_loss / (i + 1),\n",
    "                100.0 * correct / total,\n",
    "            )\n",
    "        )\n",
    "    print(\"Waiting Test...\")\n",
    "    # calculate weight_dist and batch_gradient\n",
    "    model_copy=copy.deepcopy(net)\n",
    "    weight_dist=torch.norm(w_star-model_copy.fc1.weight.cpu()).item()\n",
    "    model_copy.to(device)\n",
    "    model_copy.train()\n",
    "    weights = [p for name, p in net.named_parameters() if 'bias' not in name]\n",
    "    bias = [p for name, p in net.named_parameters() if 'bias' in name]\n",
    "    parameters = [{\"params\": weights, \"tag\": \"weights\"}, {\"params\": bias, \"tag\": \"bias\"}]\n",
    "    optimizerx = optim.SGD(parameters, lr=LR)\n",
    "    grad=torch.zeros(model_copy.fc1.weight.shape, device=device)\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_copy(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        grad += model_copy.fc1.weight.grad\n",
    "        optimizerx.step()\n",
    "        model_copy.zero_grad()\n",
    "    grad = grad / len(trainloader)\n",
    "    batch_gradient = torch.norm(grad).item()\n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_acc = net.evaluate(trainloader, criterion, device, eval=False, qt=False)\n",
    "        qtrain_loss, qtrain_acc = net.evaluate(trainloader, criterion, device, eval=False, qt=True)\n",
    "        test_loss, test_acc=net.evaluate(testloader, criterion, device, eval=True, qt=False)\n",
    "        qtest_loss, qtest_acc=net.evaluate(testloader, criterion, device, eval=True, qt=True)\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"test_loss\": test_loss,\n",
    "                \"quantized_test_loss\": qtest_loss,\n",
    "                \"accuracy\": train_acc,\n",
    "                \"quantized_accuracy\": qtrain_acc,\n",
    "                \"test_accuracy\": test_acc,\n",
    "                \"quantized_test_accuracy\": qtest_acc,\n",
    "                \"weight_distance\": weight_dist,\n",
    "                \"batch_gradient\": batch_gradient,\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            \"Train Loss: %.03f | Train Acc: %.3f%% | Test Loss: %.03f | Test Acc: %.3f%% \"\n",
    "            % (\n",
    "                train_loss,\n",
    "                train_acc,\n",
    "                test_loss,\n",
    "                test_acc,\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Quantized Train Loss: %.03f | Quantized Train Acc: %.3f%% | Quantized Test Loss: %.03f | Quantized Test Acc: %.3f%% \"\n",
    "            % (\n",
    "                qtrain_loss,\n",
    "                qtrain_acc,\n",
    "                qtest_loss,\n",
    "                qtest_acc,\n",
    "            )\n",
    "        )\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.21.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lenovo\\Desktop\\CUHK\\Research-2025\\exp\\wandb\\run-20250711_035508-jf7xtj0v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/jf7xtj0v' target=\"_blank\">ProxQuant</a></strong> to <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/jf7xtj0v' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/jf7xtj0v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "[Epoch:1, Iter:1] Loss: 0.744 | Acc: 34.000% \n",
      "[Epoch:1, Iter:2] Loss: 0.679 | Acc: 52.000% \n",
      "[Epoch:1, Iter:3] Loss: 0.648 | Acc: 58.000% \n",
      "[Epoch:1, Iter:4] Loss: 0.638 | Acc: 61.000% \n",
      "[Epoch:1, Iter:5] Loss: 0.627 | Acc: 63.600% \n",
      "[Epoch:1, Iter:6] Loss: 0.637 | Acc: 63.000% \n",
      "[Epoch:1, Iter:7] Loss: 0.634 | Acc: 63.714% \n",
      "[Epoch:1, Iter:8] Loss: 0.643 | Acc: 62.750% \n",
      "[Epoch:1, Iter:9] Loss: 0.642 | Acc: 64.000% \n",
      "[Epoch:1, Iter:10] Loss: 0.638 | Acc: 64.400% \n",
      "[Epoch:1, Iter:11] Loss: 0.631 | Acc: 65.091% \n",
      "[Epoch:1, Iter:12] Loss: 0.630 | Acc: 65.167% \n",
      "[Epoch:1, Iter:13] Loss: 0.629 | Acc: 65.385% \n",
      "[Epoch:1, Iter:14] Loss: 0.621 | Acc: 66.286% \n",
      "[Epoch:1, Iter:15] Loss: 0.612 | Acc: 67.067% \n",
      "[Epoch:1, Iter:16] Loss: 0.609 | Acc: 67.375% \n",
      "[Epoch:1, Iter:17] Loss: 0.609 | Acc: 67.294% \n",
      "[Epoch:1, Iter:18] Loss: 0.609 | Acc: 67.111% \n",
      "[Epoch:1, Iter:19] Loss: 0.610 | Acc: 66.947% \n",
      "[Epoch:1, Iter:20] Loss: 0.608 | Acc: 67.100% \n",
      "[Epoch:1, Iter:21] Loss: 0.610 | Acc: 66.857% \n",
      "[Epoch:1, Iter:22] Loss: 0.606 | Acc: 67.545% \n",
      "[Epoch:1, Iter:23] Loss: 0.602 | Acc: 68.000% \n",
      "[Epoch:1, Iter:24] Loss: 0.604 | Acc: 67.833% \n",
      "[Epoch:1, Iter:25] Loss: 0.602 | Acc: 67.920% \n",
      "[Epoch:1, Iter:26] Loss: 0.600 | Acc: 68.077% \n",
      "[Epoch:1, Iter:27] Loss: 0.597 | Acc: 68.370% \n",
      "[Epoch:1, Iter:28] Loss: 0.599 | Acc: 68.000% \n",
      "[Epoch:1, Iter:29] Loss: 0.599 | Acc: 68.069% \n",
      "[Epoch:1, Iter:30] Loss: 0.597 | Acc: 68.267% \n",
      "[Epoch:1, Iter:31] Loss: 0.597 | Acc: 68.387% \n",
      "[Epoch:1, Iter:32] Loss: 0.593 | Acc: 68.625% \n",
      "[Epoch:1, Iter:33] Loss: 0.590 | Acc: 68.848% \n",
      "[Epoch:1, Iter:34] Loss: 0.588 | Acc: 68.882% \n",
      "[Epoch:1, Iter:35] Loss: 0.588 | Acc: 68.857% \n",
      "[Epoch:1, Iter:36] Loss: 0.589 | Acc: 68.833% \n",
      "[Epoch:1, Iter:37] Loss: 0.589 | Acc: 68.757% \n",
      "[Epoch:1, Iter:38] Loss: 0.588 | Acc: 68.789% \n",
      "[Epoch:1, Iter:39] Loss: 0.588 | Acc: 68.821% \n",
      "[Epoch:1, Iter:40] Loss: 0.589 | Acc: 68.750% \n",
      "[Epoch:1, Iter:41] Loss: 0.589 | Acc: 68.829% \n",
      "[Epoch:1, Iter:42] Loss: 0.588 | Acc: 68.810% \n",
      "[Epoch:1, Iter:43] Loss: 0.586 | Acc: 69.070% \n",
      "[Epoch:1, Iter:44] Loss: 0.586 | Acc: 69.136% \n",
      "[Epoch:1, Iter:45] Loss: 0.584 | Acc: 69.244% \n",
      "[Epoch:1, Iter:46] Loss: 0.586 | Acc: 69.087% \n",
      "[Epoch:1, Iter:47] Loss: 0.584 | Acc: 69.234% \n",
      "[Epoch:1, Iter:48] Loss: 0.586 | Acc: 68.958% \n",
      "[Epoch:1, Iter:49] Loss: 0.586 | Acc: 68.857% \n",
      "[Epoch:1, Iter:50] Loss: 0.586 | Acc: 68.920% \n",
      "[Epoch:1, Iter:51] Loss: 0.583 | Acc: 69.216% \n",
      "[Epoch:1, Iter:52] Loss: 0.581 | Acc: 69.346% \n",
      "[Epoch:1, Iter:53] Loss: 0.581 | Acc: 69.396% \n",
      "[Epoch:1, Iter:54] Loss: 0.581 | Acc: 69.407% \n",
      "[Epoch:1, Iter:55] Loss: 0.581 | Acc: 69.527% \n",
      "[Epoch:1, Iter:56] Loss: 0.580 | Acc: 69.571% \n",
      "[Epoch:1, Iter:57] Loss: 0.580 | Acc: 69.614% \n",
      "[Epoch:1, Iter:58] Loss: 0.579 | Acc: 69.690% \n",
      "[Epoch:1, Iter:59] Loss: 0.578 | Acc: 69.763% \n",
      "[Epoch:1, Iter:60] Loss: 0.578 | Acc: 69.800% \n",
      "[Epoch:1, Iter:61] Loss: 0.577 | Acc: 69.869% \n",
      "[Epoch:1, Iter:62] Loss: 0.577 | Acc: 69.903% \n",
      "[Epoch:1, Iter:63] Loss: 0.577 | Acc: 69.905% \n",
      "[Epoch:1, Iter:64] Loss: 0.577 | Acc: 69.844% \n",
      "[Epoch:1, Iter:65] Loss: 0.576 | Acc: 69.908% \n",
      "[Epoch:1, Iter:66] Loss: 0.576 | Acc: 69.909% \n",
      "[Epoch:1, Iter:67] Loss: 0.576 | Acc: 69.970% \n",
      "[Epoch:1, Iter:68] Loss: 0.575 | Acc: 69.882% \n",
      "[Epoch:1, Iter:69] Loss: 0.576 | Acc: 69.797% \n",
      "[Epoch:1, Iter:70] Loss: 0.575 | Acc: 69.886% \n",
      "[Epoch:1, Iter:71] Loss: 0.574 | Acc: 70.056% \n",
      "[Epoch:1, Iter:72] Loss: 0.573 | Acc: 70.111% \n",
      "[Epoch:1, Iter:73] Loss: 0.573 | Acc: 70.055% \n",
      "[Epoch:1, Iter:74] Loss: 0.574 | Acc: 70.000% \n",
      "[Epoch:1, Iter:75] Loss: 0.574 | Acc: 69.947% \n",
      "[Epoch:1, Iter:76] Loss: 0.575 | Acc: 69.868% \n",
      "[Epoch:1, Iter:77] Loss: 0.575 | Acc: 69.922% \n",
      "[Epoch:1, Iter:78] Loss: 0.574 | Acc: 69.949% \n",
      "[Epoch:1, Iter:79] Loss: 0.573 | Acc: 69.949% \n",
      "[Epoch:1, Iter:80] Loss: 0.573 | Acc: 69.975% \n",
      "[Epoch:1, Iter:81] Loss: 0.573 | Acc: 70.000% \n",
      "[Epoch:1, Iter:82] Loss: 0.574 | Acc: 69.927% \n",
      "[Epoch:1, Iter:83] Loss: 0.574 | Acc: 69.904% \n",
      "[Epoch:1, Iter:84] Loss: 0.573 | Acc: 70.024% \n",
      "[Epoch:1, Iter:85] Loss: 0.572 | Acc: 70.188% \n",
      "[Epoch:1, Iter:86] Loss: 0.570 | Acc: 70.349% \n",
      "[Epoch:1, Iter:87] Loss: 0.570 | Acc: 70.391% \n",
      "[Epoch:1, Iter:88] Loss: 0.570 | Acc: 70.318% \n",
      "[Epoch:1, Iter:89] Loss: 0.570 | Acc: 70.247% \n",
      "[Epoch:1, Iter:90] Loss: 0.569 | Acc: 70.356% \n",
      "[Epoch:1, Iter:91] Loss: 0.570 | Acc: 70.286% \n",
      "[Epoch:1, Iter:92] Loss: 0.569 | Acc: 70.283% \n",
      "[Epoch:1, Iter:93] Loss: 0.570 | Acc: 70.172% \n",
      "[Epoch:1, Iter:94] Loss: 0.570 | Acc: 70.170% \n",
      "[Epoch:1, Iter:95] Loss: 0.570 | Acc: 70.168% \n",
      "[Epoch:1, Iter:96] Loss: 0.569 | Acc: 70.250% \n",
      "[Epoch:1, Iter:97] Loss: 0.570 | Acc: 70.227% \n",
      "[Epoch:1, Iter:98] Loss: 0.569 | Acc: 70.163% \n",
      "[Epoch:1, Iter:99] Loss: 0.569 | Acc: 70.182% \n",
      "[Epoch:1, Iter:100] Loss: 0.569 | Acc: 70.160% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.660%\n",
      "Training set's accuracy (after quantization) is: 73.000%\n",
      "Test set's accuracy (before quantization) is: 72.800%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.544 | Train Acc: 72.660% | Test Loss: 0.545 | Test Acc: 72.800% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 73.000% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 2\n",
      "[Epoch:2, Iter:101] Loss: 0.559 | Acc: 74.000% \n",
      "[Epoch:2, Iter:102] Loss: 0.560 | Acc: 71.000% \n",
      "[Epoch:2, Iter:103] Loss: 0.559 | Acc: 70.667% \n",
      "[Epoch:2, Iter:104] Loss: 0.547 | Acc: 73.000% \n",
      "[Epoch:2, Iter:105] Loss: 0.546 | Acc: 72.000% \n",
      "[Epoch:2, Iter:106] Loss: 0.535 | Acc: 72.667% \n",
      "[Epoch:2, Iter:107] Loss: 0.556 | Acc: 70.571% \n",
      "[Epoch:2, Iter:108] Loss: 0.552 | Acc: 71.500% \n",
      "[Epoch:2, Iter:109] Loss: 0.552 | Acc: 71.333% \n",
      "[Epoch:2, Iter:110] Loss: 0.550 | Acc: 72.000% \n",
      "[Epoch:2, Iter:111] Loss: 0.559 | Acc: 71.455% \n",
      "[Epoch:2, Iter:112] Loss: 0.562 | Acc: 71.167% \n",
      "[Epoch:2, Iter:113] Loss: 0.558 | Acc: 71.385% \n",
      "[Epoch:2, Iter:114] Loss: 0.554 | Acc: 71.857% \n",
      "[Epoch:2, Iter:115] Loss: 0.556 | Acc: 71.600% \n",
      "[Epoch:2, Iter:116] Loss: 0.560 | Acc: 71.000% \n",
      "[Epoch:2, Iter:117] Loss: 0.561 | Acc: 71.059% \n",
      "[Epoch:2, Iter:118] Loss: 0.556 | Acc: 71.556% \n",
      "[Epoch:2, Iter:119] Loss: 0.558 | Acc: 71.158% \n",
      "[Epoch:2, Iter:120] Loss: 0.558 | Acc: 71.200% \n",
      "[Epoch:2, Iter:121] Loss: 0.556 | Acc: 71.524% \n",
      "[Epoch:2, Iter:122] Loss: 0.554 | Acc: 71.727% \n",
      "[Epoch:2, Iter:123] Loss: 0.553 | Acc: 71.826% \n",
      "[Epoch:2, Iter:124] Loss: 0.551 | Acc: 71.917% \n",
      "[Epoch:2, Iter:125] Loss: 0.552 | Acc: 71.920% \n",
      "[Epoch:2, Iter:126] Loss: 0.551 | Acc: 72.077% \n",
      "[Epoch:2, Iter:127] Loss: 0.546 | Acc: 72.519% \n",
      "[Epoch:2, Iter:128] Loss: 0.546 | Acc: 72.357% \n",
      "[Epoch:2, Iter:129] Loss: 0.550 | Acc: 71.862% \n",
      "[Epoch:2, Iter:130] Loss: 0.552 | Acc: 71.600% \n",
      "[Epoch:2, Iter:131] Loss: 0.551 | Acc: 71.871% \n",
      "[Epoch:2, Iter:132] Loss: 0.551 | Acc: 71.750% \n",
      "[Epoch:2, Iter:133] Loss: 0.549 | Acc: 71.818% \n",
      "[Epoch:2, Iter:134] Loss: 0.550 | Acc: 71.882% \n",
      "[Epoch:2, Iter:135] Loss: 0.549 | Acc: 72.057% \n",
      "[Epoch:2, Iter:136] Loss: 0.548 | Acc: 71.889% \n",
      "[Epoch:2, Iter:137] Loss: 0.549 | Acc: 71.838% \n",
      "[Epoch:2, Iter:138] Loss: 0.551 | Acc: 71.684% \n",
      "[Epoch:2, Iter:139] Loss: 0.549 | Acc: 71.949% \n",
      "[Epoch:2, Iter:140] Loss: 0.549 | Acc: 72.050% \n",
      "[Epoch:2, Iter:141] Loss: 0.548 | Acc: 72.098% \n",
      "[Epoch:2, Iter:142] Loss: 0.549 | Acc: 72.095% \n",
      "[Epoch:2, Iter:143] Loss: 0.548 | Acc: 72.140% \n",
      "[Epoch:2, Iter:144] Loss: 0.549 | Acc: 72.136% \n",
      "[Epoch:2, Iter:145] Loss: 0.547 | Acc: 72.311% \n",
      "[Epoch:2, Iter:146] Loss: 0.547 | Acc: 72.348% \n",
      "[Epoch:2, Iter:147] Loss: 0.547 | Acc: 72.340% \n",
      "[Epoch:2, Iter:148] Loss: 0.547 | Acc: 72.250% \n",
      "[Epoch:2, Iter:149] Loss: 0.549 | Acc: 72.082% \n",
      "[Epoch:2, Iter:150] Loss: 0.547 | Acc: 72.240% \n",
      "[Epoch:2, Iter:151] Loss: 0.548 | Acc: 72.196% \n",
      "[Epoch:2, Iter:152] Loss: 0.547 | Acc: 72.038% \n",
      "[Epoch:2, Iter:153] Loss: 0.547 | Acc: 72.075% \n",
      "[Epoch:2, Iter:154] Loss: 0.547 | Acc: 72.074% \n",
      "[Epoch:2, Iter:155] Loss: 0.548 | Acc: 71.927% \n",
      "[Epoch:2, Iter:156] Loss: 0.549 | Acc: 71.964% \n",
      "[Epoch:2, Iter:157] Loss: 0.547 | Acc: 72.105% \n",
      "[Epoch:2, Iter:158] Loss: 0.547 | Acc: 72.103% \n",
      "[Epoch:2, Iter:159] Loss: 0.546 | Acc: 72.068% \n",
      "[Epoch:2, Iter:160] Loss: 0.545 | Acc: 72.100% \n",
      "[Epoch:2, Iter:161] Loss: 0.546 | Acc: 72.098% \n",
      "[Epoch:2, Iter:162] Loss: 0.547 | Acc: 72.065% \n",
      "[Epoch:2, Iter:163] Loss: 0.547 | Acc: 72.095% \n",
      "[Epoch:2, Iter:164] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:2, Iter:165] Loss: 0.549 | Acc: 71.938% \n",
      "[Epoch:2, Iter:166] Loss: 0.548 | Acc: 72.030% \n",
      "[Epoch:2, Iter:167] Loss: 0.547 | Acc: 72.090% \n",
      "[Epoch:2, Iter:168] Loss: 0.546 | Acc: 72.176% \n",
      "[Epoch:2, Iter:169] Loss: 0.546 | Acc: 72.203% \n",
      "[Epoch:2, Iter:170] Loss: 0.546 | Acc: 72.200% \n",
      "[Epoch:2, Iter:171] Loss: 0.545 | Acc: 72.225% \n",
      "[Epoch:2, Iter:172] Loss: 0.546 | Acc: 72.111% \n",
      "[Epoch:2, Iter:173] Loss: 0.548 | Acc: 71.918% \n",
      "[Epoch:2, Iter:174] Loss: 0.548 | Acc: 72.054% \n",
      "[Epoch:2, Iter:175] Loss: 0.547 | Acc: 72.080% \n",
      "[Epoch:2, Iter:176] Loss: 0.545 | Acc: 72.158% \n",
      "[Epoch:2, Iter:177] Loss: 0.546 | Acc: 72.104% \n",
      "[Epoch:2, Iter:178] Loss: 0.545 | Acc: 72.256% \n",
      "[Epoch:2, Iter:179] Loss: 0.545 | Acc: 72.228% \n",
      "[Epoch:2, Iter:180] Loss: 0.548 | Acc: 72.100% \n",
      "[Epoch:2, Iter:181] Loss: 0.548 | Acc: 72.074% \n",
      "[Epoch:2, Iter:182] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:2, Iter:183] Loss: 0.549 | Acc: 72.024% \n",
      "[Epoch:2, Iter:184] Loss: 0.551 | Acc: 71.929% \n",
      "[Epoch:2, Iter:185] Loss: 0.550 | Acc: 72.071% \n",
      "[Epoch:2, Iter:186] Loss: 0.552 | Acc: 71.884% \n",
      "[Epoch:2, Iter:187] Loss: 0.552 | Acc: 71.931% \n",
      "[Epoch:2, Iter:188] Loss: 0.552 | Acc: 71.932% \n",
      "[Epoch:2, Iter:189] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:2, Iter:190] Loss: 0.551 | Acc: 72.022% \n",
      "[Epoch:2, Iter:191] Loss: 0.550 | Acc: 72.154% \n",
      "[Epoch:2, Iter:192] Loss: 0.550 | Acc: 72.174% \n",
      "[Epoch:2, Iter:193] Loss: 0.548 | Acc: 72.301% \n",
      "[Epoch:2, Iter:194] Loss: 0.548 | Acc: 72.340% \n",
      "[Epoch:2, Iter:195] Loss: 0.549 | Acc: 72.274% \n",
      "[Epoch:2, Iter:196] Loss: 0.548 | Acc: 72.333% \n",
      "[Epoch:2, Iter:197] Loss: 0.547 | Acc: 72.454% \n",
      "[Epoch:2, Iter:198] Loss: 0.548 | Acc: 72.306% \n",
      "[Epoch:2, Iter:199] Loss: 0.548 | Acc: 72.343% \n",
      "[Epoch:2, Iter:200] Loss: 0.549 | Acc: 72.340% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 71.680%\n",
      "Training set's accuracy (after quantization) is: 72.540%\n",
      "Test set's accuracy (before quantization) is: 71.300%\n",
      "Test set's accuracy (after quantization) is: 72.200%\n",
      "Train Loss: 0.557 | Train Acc: 71.680% | Test Loss: 0.559 | Test Acc: 71.300% \n",
      "Quantized Train Loss: 0.545 | Quantized Train Acc: 72.540% | Quantized Test Loss: 0.548 | Quantized Test Acc: 72.200% \n",
      "\n",
      "Epoch: 3\n",
      "[Epoch:3, Iter:201] Loss: 0.602 | Acc: 68.000% \n",
      "[Epoch:3, Iter:202] Loss: 0.590 | Acc: 70.000% \n",
      "[Epoch:3, Iter:203] Loss: 0.560 | Acc: 71.333% \n",
      "[Epoch:3, Iter:204] Loss: 0.550 | Acc: 71.000% \n",
      "[Epoch:3, Iter:205] Loss: 0.559 | Acc: 71.200% \n",
      "[Epoch:3, Iter:206] Loss: 0.563 | Acc: 70.667% \n",
      "[Epoch:3, Iter:207] Loss: 0.561 | Acc: 70.000% \n",
      "[Epoch:3, Iter:208] Loss: 0.562 | Acc: 69.000% \n",
      "[Epoch:3, Iter:209] Loss: 0.558 | Acc: 69.556% \n",
      "[Epoch:3, Iter:210] Loss: 0.562 | Acc: 68.600% \n",
      "[Epoch:3, Iter:211] Loss: 0.560 | Acc: 69.091% \n",
      "[Epoch:3, Iter:212] Loss: 0.558 | Acc: 69.333% \n",
      "[Epoch:3, Iter:213] Loss: 0.561 | Acc: 69.846% \n",
      "[Epoch:3, Iter:214] Loss: 0.562 | Acc: 70.143% \n",
      "[Epoch:3, Iter:215] Loss: 0.563 | Acc: 70.400% \n",
      "[Epoch:3, Iter:216] Loss: 0.558 | Acc: 71.000% \n",
      "[Epoch:3, Iter:217] Loss: 0.561 | Acc: 71.059% \n",
      "[Epoch:3, Iter:218] Loss: 0.563 | Acc: 71.222% \n",
      "[Epoch:3, Iter:219] Loss: 0.557 | Acc: 71.579% \n",
      "[Epoch:3, Iter:220] Loss: 0.556 | Acc: 71.500% \n",
      "[Epoch:3, Iter:221] Loss: 0.555 | Acc: 71.810% \n",
      "[Epoch:3, Iter:222] Loss: 0.562 | Acc: 71.364% \n",
      "[Epoch:3, Iter:223] Loss: 0.560 | Acc: 71.826% \n",
      "[Epoch:3, Iter:224] Loss: 0.564 | Acc: 71.333% \n",
      "[Epoch:3, Iter:225] Loss: 0.565 | Acc: 70.960% \n",
      "[Epoch:3, Iter:226] Loss: 0.564 | Acc: 71.000% \n",
      "[Epoch:3, Iter:227] Loss: 0.563 | Acc: 70.963% \n",
      "[Epoch:3, Iter:228] Loss: 0.562 | Acc: 71.000% \n",
      "[Epoch:3, Iter:229] Loss: 0.562 | Acc: 71.103% \n",
      "[Epoch:3, Iter:230] Loss: 0.566 | Acc: 70.933% \n",
      "[Epoch:3, Iter:231] Loss: 0.568 | Acc: 70.581% \n",
      "[Epoch:3, Iter:232] Loss: 0.564 | Acc: 70.750% \n",
      "[Epoch:3, Iter:233] Loss: 0.564 | Acc: 70.788% \n",
      "[Epoch:3, Iter:234] Loss: 0.563 | Acc: 70.706% \n",
      "[Epoch:3, Iter:235] Loss: 0.563 | Acc: 70.857% \n",
      "[Epoch:3, Iter:236] Loss: 0.562 | Acc: 71.167% \n",
      "[Epoch:3, Iter:237] Loss: 0.560 | Acc: 71.351% \n",
      "[Epoch:3, Iter:238] Loss: 0.558 | Acc: 71.474% \n",
      "[Epoch:3, Iter:239] Loss: 0.555 | Acc: 71.538% \n",
      "[Epoch:3, Iter:240] Loss: 0.551 | Acc: 71.800% \n",
      "[Epoch:3, Iter:241] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:3, Iter:242] Loss: 0.550 | Acc: 71.810% \n",
      "[Epoch:3, Iter:243] Loss: 0.550 | Acc: 71.814% \n",
      "[Epoch:3, Iter:244] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:3, Iter:245] Loss: 0.551 | Acc: 71.689% \n",
      "[Epoch:3, Iter:246] Loss: 0.550 | Acc: 71.739% \n",
      "[Epoch:3, Iter:247] Loss: 0.550 | Acc: 71.830% \n",
      "[Epoch:3, Iter:248] Loss: 0.550 | Acc: 71.792% \n",
      "[Epoch:3, Iter:249] Loss: 0.551 | Acc: 71.755% \n",
      "[Epoch:3, Iter:250] Loss: 0.549 | Acc: 71.960% \n",
      "[Epoch:3, Iter:251] Loss: 0.549 | Acc: 71.922% \n",
      "[Epoch:3, Iter:252] Loss: 0.548 | Acc: 72.038% \n",
      "[Epoch:3, Iter:253] Loss: 0.547 | Acc: 72.113% \n",
      "[Epoch:3, Iter:254] Loss: 0.547 | Acc: 72.074% \n",
      "[Epoch:3, Iter:255] Loss: 0.548 | Acc: 72.073% \n",
      "[Epoch:3, Iter:256] Loss: 0.549 | Acc: 71.929% \n",
      "[Epoch:3, Iter:257] Loss: 0.548 | Acc: 71.965% \n",
      "[Epoch:3, Iter:258] Loss: 0.547 | Acc: 72.034% \n",
      "[Epoch:3, Iter:259] Loss: 0.546 | Acc: 72.102% \n",
      "[Epoch:3, Iter:260] Loss: 0.543 | Acc: 72.367% \n",
      "[Epoch:3, Iter:261] Loss: 0.542 | Acc: 72.459% \n",
      "[Epoch:3, Iter:262] Loss: 0.542 | Acc: 72.452% \n",
      "[Epoch:3, Iter:263] Loss: 0.541 | Acc: 72.603% \n",
      "[Epoch:3, Iter:264] Loss: 0.542 | Acc: 72.531% \n",
      "[Epoch:3, Iter:265] Loss: 0.542 | Acc: 72.554% \n",
      "[Epoch:3, Iter:266] Loss: 0.540 | Acc: 72.727% \n",
      "[Epoch:3, Iter:267] Loss: 0.542 | Acc: 72.597% \n",
      "[Epoch:3, Iter:268] Loss: 0.542 | Acc: 72.559% \n",
      "[Epoch:3, Iter:269] Loss: 0.541 | Acc: 72.551% \n",
      "[Epoch:3, Iter:270] Loss: 0.541 | Acc: 72.543% \n",
      "[Epoch:3, Iter:271] Loss: 0.541 | Acc: 72.535% \n",
      "[Epoch:3, Iter:272] Loss: 0.541 | Acc: 72.472% \n",
      "[Epoch:3, Iter:273] Loss: 0.542 | Acc: 72.466% \n",
      "[Epoch:3, Iter:274] Loss: 0.540 | Acc: 72.568% \n",
      "[Epoch:3, Iter:275] Loss: 0.540 | Acc: 72.587% \n",
      "[Epoch:3, Iter:276] Loss: 0.541 | Acc: 72.526% \n",
      "[Epoch:3, Iter:277] Loss: 0.542 | Acc: 72.468% \n",
      "[Epoch:3, Iter:278] Loss: 0.542 | Acc: 72.410% \n",
      "[Epoch:3, Iter:279] Loss: 0.543 | Acc: 72.430% \n",
      "[Epoch:3, Iter:280] Loss: 0.543 | Acc: 72.425% \n",
      "[Epoch:3, Iter:281] Loss: 0.543 | Acc: 72.469% \n",
      "[Epoch:3, Iter:282] Loss: 0.542 | Acc: 72.561% \n",
      "[Epoch:3, Iter:283] Loss: 0.543 | Acc: 72.506% \n",
      "[Epoch:3, Iter:284] Loss: 0.544 | Acc: 72.452% \n",
      "[Epoch:3, Iter:285] Loss: 0.544 | Acc: 72.376% \n",
      "[Epoch:3, Iter:286] Loss: 0.545 | Acc: 72.419% \n",
      "[Epoch:3, Iter:287] Loss: 0.544 | Acc: 72.437% \n",
      "[Epoch:3, Iter:288] Loss: 0.545 | Acc: 72.364% \n",
      "[Epoch:3, Iter:289] Loss: 0.545 | Acc: 72.382% \n",
      "[Epoch:3, Iter:290] Loss: 0.545 | Acc: 72.356% \n",
      "[Epoch:3, Iter:291] Loss: 0.544 | Acc: 72.374% \n",
      "[Epoch:3, Iter:292] Loss: 0.543 | Acc: 72.478% \n",
      "[Epoch:3, Iter:293] Loss: 0.542 | Acc: 72.516% \n",
      "[Epoch:3, Iter:294] Loss: 0.543 | Acc: 72.383% \n",
      "[Epoch:3, Iter:295] Loss: 0.544 | Acc: 72.316% \n",
      "[Epoch:3, Iter:296] Loss: 0.545 | Acc: 72.229% \n",
      "[Epoch:3, Iter:297] Loss: 0.545 | Acc: 72.227% \n",
      "[Epoch:3, Iter:298] Loss: 0.545 | Acc: 72.306% \n",
      "[Epoch:3, Iter:299] Loss: 0.545 | Acc: 72.323% \n",
      "[Epoch:3, Iter:300] Loss: 0.545 | Acc: 72.320% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.900%\n",
      "Training set's accuracy (after quantization) is: 72.700%\n",
      "Test set's accuracy (before quantization) is: 72.500%\n",
      "Test set's accuracy (after quantization) is: 72.400%\n",
      "Train Loss: 0.540 | Train Acc: 72.900% | Test Loss: 0.544 | Test Acc: 72.500% \n",
      "Quantized Train Loss: 0.545 | Quantized Train Acc: 72.700% | Quantized Test Loss: 0.547 | Quantized Test Acc: 72.400% \n",
      "\n",
      "Epoch: 4\n",
      "[Epoch:4, Iter:301] Loss: 0.502 | Acc: 76.000% \n",
      "[Epoch:4, Iter:302] Loss: 0.567 | Acc: 68.000% \n",
      "[Epoch:4, Iter:303] Loss: 0.572 | Acc: 69.333% \n",
      "[Epoch:4, Iter:304] Loss: 0.559 | Acc: 71.000% \n",
      "[Epoch:4, Iter:305] Loss: 0.542 | Acc: 72.400% \n",
      "[Epoch:4, Iter:306] Loss: 0.540 | Acc: 72.000% \n",
      "[Epoch:4, Iter:307] Loss: 0.540 | Acc: 72.000% \n",
      "[Epoch:4, Iter:308] Loss: 0.555 | Acc: 71.750% \n",
      "[Epoch:4, Iter:309] Loss: 0.554 | Acc: 72.222% \n",
      "[Epoch:4, Iter:310] Loss: 0.559 | Acc: 71.400% \n",
      "[Epoch:4, Iter:311] Loss: 0.555 | Acc: 71.455% \n",
      "[Epoch:4, Iter:312] Loss: 0.552 | Acc: 71.667% \n",
      "[Epoch:4, Iter:313] Loss: 0.549 | Acc: 71.538% \n",
      "[Epoch:4, Iter:314] Loss: 0.548 | Acc: 71.714% \n",
      "[Epoch:4, Iter:315] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:4, Iter:316] Loss: 0.559 | Acc: 71.125% \n",
      "[Epoch:4, Iter:317] Loss: 0.558 | Acc: 71.294% \n",
      "[Epoch:4, Iter:318] Loss: 0.557 | Acc: 71.667% \n",
      "[Epoch:4, Iter:319] Loss: 0.558 | Acc: 71.579% \n",
      "[Epoch:4, Iter:320] Loss: 0.559 | Acc: 71.100% \n",
      "[Epoch:4, Iter:321] Loss: 0.555 | Acc: 71.524% \n",
      "[Epoch:4, Iter:322] Loss: 0.556 | Acc: 71.455% \n",
      "[Epoch:4, Iter:323] Loss: 0.554 | Acc: 71.565% \n",
      "[Epoch:4, Iter:324] Loss: 0.551 | Acc: 71.833% \n",
      "[Epoch:4, Iter:325] Loss: 0.552 | Acc: 71.760% \n",
      "[Epoch:4, Iter:326] Loss: 0.553 | Acc: 71.692% \n",
      "[Epoch:4, Iter:327] Loss: 0.554 | Acc: 71.407% \n",
      "[Epoch:4, Iter:328] Loss: 0.550 | Acc: 71.500% \n",
      "[Epoch:4, Iter:329] Loss: 0.546 | Acc: 71.586% \n",
      "[Epoch:4, Iter:330] Loss: 0.544 | Acc: 71.600% \n",
      "[Epoch:4, Iter:331] Loss: 0.540 | Acc: 71.871% \n",
      "[Epoch:4, Iter:332] Loss: 0.535 | Acc: 72.188% \n",
      "[Epoch:4, Iter:333] Loss: 0.538 | Acc: 72.242% \n",
      "[Epoch:4, Iter:334] Loss: 0.537 | Acc: 72.353% \n",
      "[Epoch:4, Iter:335] Loss: 0.537 | Acc: 72.457% \n",
      "[Epoch:4, Iter:336] Loss: 0.541 | Acc: 72.444% \n",
      "[Epoch:4, Iter:337] Loss: 0.538 | Acc: 72.649% \n",
      "[Epoch:4, Iter:338] Loss: 0.539 | Acc: 72.579% \n",
      "[Epoch:4, Iter:339] Loss: 0.538 | Acc: 72.667% \n",
      "[Epoch:4, Iter:340] Loss: 0.537 | Acc: 72.700% \n",
      "[Epoch:4, Iter:341] Loss: 0.537 | Acc: 72.829% \n",
      "[Epoch:4, Iter:342] Loss: 0.536 | Acc: 72.857% \n",
      "[Epoch:4, Iter:343] Loss: 0.534 | Acc: 72.977% \n",
      "[Epoch:4, Iter:344] Loss: 0.533 | Acc: 73.318% \n",
      "[Epoch:4, Iter:345] Loss: 0.533 | Acc: 73.378% \n",
      "[Epoch:4, Iter:346] Loss: 0.533 | Acc: 73.391% \n",
      "[Epoch:4, Iter:347] Loss: 0.535 | Acc: 73.277% \n",
      "[Epoch:4, Iter:348] Loss: 0.535 | Acc: 73.333% \n",
      "[Epoch:4, Iter:349] Loss: 0.536 | Acc: 73.224% \n",
      "[Epoch:4, Iter:350] Loss: 0.536 | Acc: 73.280% \n",
      "[Epoch:4, Iter:351] Loss: 0.536 | Acc: 73.176% \n",
      "[Epoch:4, Iter:352] Loss: 0.539 | Acc: 73.077% \n",
      "[Epoch:4, Iter:353] Loss: 0.540 | Acc: 72.981% \n",
      "[Epoch:4, Iter:354] Loss: 0.538 | Acc: 73.111% \n",
      "[Epoch:4, Iter:355] Loss: 0.540 | Acc: 72.873% \n",
      "[Epoch:4, Iter:356] Loss: 0.540 | Acc: 72.857% \n",
      "[Epoch:4, Iter:357] Loss: 0.540 | Acc: 72.877% \n",
      "[Epoch:4, Iter:358] Loss: 0.541 | Acc: 72.690% \n",
      "[Epoch:4, Iter:359] Loss: 0.541 | Acc: 72.644% \n",
      "[Epoch:4, Iter:360] Loss: 0.539 | Acc: 72.633% \n",
      "[Epoch:4, Iter:361] Loss: 0.538 | Acc: 72.656% \n",
      "[Epoch:4, Iter:362] Loss: 0.539 | Acc: 72.710% \n",
      "[Epoch:4, Iter:363] Loss: 0.541 | Acc: 72.444% \n",
      "[Epoch:4, Iter:364] Loss: 0.541 | Acc: 72.469% \n",
      "[Epoch:4, Iter:365] Loss: 0.542 | Acc: 72.369% \n",
      "[Epoch:4, Iter:366] Loss: 0.542 | Acc: 72.333% \n",
      "[Epoch:4, Iter:367] Loss: 0.542 | Acc: 72.299% \n",
      "[Epoch:4, Iter:368] Loss: 0.541 | Acc: 72.471% \n",
      "[Epoch:4, Iter:369] Loss: 0.540 | Acc: 72.464% \n",
      "[Epoch:4, Iter:370] Loss: 0.540 | Acc: 72.457% \n",
      "[Epoch:4, Iter:371] Loss: 0.540 | Acc: 72.479% \n",
      "[Epoch:4, Iter:372] Loss: 0.541 | Acc: 72.472% \n",
      "[Epoch:4, Iter:373] Loss: 0.540 | Acc: 72.630% \n",
      "[Epoch:4, Iter:374] Loss: 0.541 | Acc: 72.703% \n",
      "[Epoch:4, Iter:375] Loss: 0.541 | Acc: 72.747% \n",
      "[Epoch:4, Iter:376] Loss: 0.542 | Acc: 72.711% \n",
      "[Epoch:4, Iter:377] Loss: 0.543 | Acc: 72.675% \n",
      "[Epoch:4, Iter:378] Loss: 0.543 | Acc: 72.718% \n",
      "[Epoch:4, Iter:379] Loss: 0.543 | Acc: 72.759% \n",
      "[Epoch:4, Iter:380] Loss: 0.543 | Acc: 72.725% \n",
      "[Epoch:4, Iter:381] Loss: 0.544 | Acc: 72.741% \n",
      "[Epoch:4, Iter:382] Loss: 0.543 | Acc: 72.829% \n",
      "[Epoch:4, Iter:383] Loss: 0.540 | Acc: 72.940% \n",
      "[Epoch:4, Iter:384] Loss: 0.541 | Acc: 72.905% \n",
      "[Epoch:4, Iter:385] Loss: 0.541 | Acc: 72.965% \n",
      "[Epoch:4, Iter:386] Loss: 0.540 | Acc: 73.070% \n",
      "[Epoch:4, Iter:387] Loss: 0.540 | Acc: 73.011% \n",
      "[Epoch:4, Iter:388] Loss: 0.540 | Acc: 73.023% \n",
      "[Epoch:4, Iter:389] Loss: 0.540 | Acc: 72.989% \n",
      "[Epoch:4, Iter:390] Loss: 0.539 | Acc: 73.089% \n",
      "[Epoch:4, Iter:391] Loss: 0.542 | Acc: 72.857% \n",
      "[Epoch:4, Iter:392] Loss: 0.543 | Acc: 72.870% \n",
      "[Epoch:4, Iter:393] Loss: 0.544 | Acc: 72.817% \n",
      "[Epoch:4, Iter:394] Loss: 0.543 | Acc: 72.830% \n",
      "[Epoch:4, Iter:395] Loss: 0.543 | Acc: 72.842% \n",
      "[Epoch:4, Iter:396] Loss: 0.543 | Acc: 72.833% \n",
      "[Epoch:4, Iter:397] Loss: 0.542 | Acc: 72.928% \n",
      "[Epoch:4, Iter:398] Loss: 0.543 | Acc: 72.857% \n",
      "[Epoch:4, Iter:399] Loss: 0.544 | Acc: 72.768% \n",
      "[Epoch:4, Iter:400] Loss: 0.543 | Acc: 72.760% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.560%\n",
      "Training set's accuracy (after quantization) is: 72.180%\n",
      "Test set's accuracy (before quantization) is: 72.000%\n",
      "Test set's accuracy (after quantization) is: 71.200%\n",
      "Train Loss: 0.542 | Train Acc: 72.560% | Test Loss: 0.547 | Test Acc: 72.000% \n",
      "Quantized Train Loss: 0.548 | Quantized Train Acc: 72.180% | Quantized Test Loss: 0.551 | Quantized Test Acc: 71.200% \n",
      "\n",
      "Epoch: 5\n",
      "[Epoch:5, Iter:401] Loss: 0.581 | Acc: 68.000% \n",
      "[Epoch:5, Iter:402] Loss: 0.546 | Acc: 74.000% \n",
      "[Epoch:5, Iter:403] Loss: 0.497 | Acc: 77.333% \n",
      "[Epoch:5, Iter:404] Loss: 0.483 | Acc: 80.000% \n",
      "[Epoch:5, Iter:405] Loss: 0.505 | Acc: 78.400% \n",
      "[Epoch:5, Iter:406] Loss: 0.508 | Acc: 78.333% \n",
      "[Epoch:5, Iter:407] Loss: 0.492 | Acc: 78.857% \n",
      "[Epoch:5, Iter:408] Loss: 0.495 | Acc: 78.250% \n",
      "[Epoch:5, Iter:409] Loss: 0.504 | Acc: 77.111% \n",
      "[Epoch:5, Iter:410] Loss: 0.528 | Acc: 76.000% \n",
      "[Epoch:5, Iter:411] Loss: 0.536 | Acc: 75.091% \n",
      "[Epoch:5, Iter:412] Loss: 0.532 | Acc: 75.167% \n",
      "[Epoch:5, Iter:413] Loss: 0.532 | Acc: 75.077% \n",
      "[Epoch:5, Iter:414] Loss: 0.541 | Acc: 74.286% \n",
      "[Epoch:5, Iter:415] Loss: 0.539 | Acc: 74.267% \n",
      "[Epoch:5, Iter:416] Loss: 0.537 | Acc: 74.250% \n",
      "[Epoch:5, Iter:417] Loss: 0.548 | Acc: 73.294% \n",
      "[Epoch:5, Iter:418] Loss: 0.549 | Acc: 73.222% \n",
      "[Epoch:5, Iter:419] Loss: 0.552 | Acc: 72.842% \n",
      "[Epoch:5, Iter:420] Loss: 0.549 | Acc: 73.200% \n",
      "[Epoch:5, Iter:421] Loss: 0.550 | Acc: 73.048% \n",
      "[Epoch:5, Iter:422] Loss: 0.549 | Acc: 72.818% \n",
      "[Epoch:5, Iter:423] Loss: 0.548 | Acc: 72.957% \n",
      "[Epoch:5, Iter:424] Loss: 0.544 | Acc: 73.417% \n",
      "[Epoch:5, Iter:425] Loss: 0.543 | Acc: 73.520% \n",
      "[Epoch:5, Iter:426] Loss: 0.539 | Acc: 73.923% \n",
      "[Epoch:5, Iter:427] Loss: 0.537 | Acc: 74.000% \n",
      "[Epoch:5, Iter:428] Loss: 0.543 | Acc: 73.714% \n",
      "[Epoch:5, Iter:429] Loss: 0.544 | Acc: 73.517% \n",
      "[Epoch:5, Iter:430] Loss: 0.544 | Acc: 73.467% \n",
      "[Epoch:5, Iter:431] Loss: 0.540 | Acc: 73.806% \n",
      "[Epoch:5, Iter:432] Loss: 0.540 | Acc: 73.750% \n",
      "[Epoch:5, Iter:433] Loss: 0.540 | Acc: 73.818% \n",
      "[Epoch:5, Iter:434] Loss: 0.543 | Acc: 73.529% \n",
      "[Epoch:5, Iter:435] Loss: 0.544 | Acc: 73.543% \n",
      "[Epoch:5, Iter:436] Loss: 0.543 | Acc: 73.722% \n",
      "[Epoch:5, Iter:437] Loss: 0.541 | Acc: 74.000% \n",
      "[Epoch:5, Iter:438] Loss: 0.543 | Acc: 73.895% \n",
      "[Epoch:5, Iter:439] Loss: 0.542 | Acc: 73.795% \n",
      "[Epoch:5, Iter:440] Loss: 0.543 | Acc: 73.800% \n",
      "[Epoch:5, Iter:441] Loss: 0.543 | Acc: 73.805% \n",
      "[Epoch:5, Iter:442] Loss: 0.544 | Acc: 73.762% \n",
      "[Epoch:5, Iter:443] Loss: 0.541 | Acc: 73.907% \n",
      "[Epoch:5, Iter:444] Loss: 0.538 | Acc: 74.227% \n",
      "[Epoch:5, Iter:445] Loss: 0.539 | Acc: 74.044% \n",
      "[Epoch:5, Iter:446] Loss: 0.540 | Acc: 73.826% \n",
      "[Epoch:5, Iter:447] Loss: 0.541 | Acc: 73.574% \n",
      "[Epoch:5, Iter:448] Loss: 0.541 | Acc: 73.542% \n",
      "[Epoch:5, Iter:449] Loss: 0.541 | Acc: 73.510% \n",
      "[Epoch:5, Iter:450] Loss: 0.541 | Acc: 73.480% \n",
      "[Epoch:5, Iter:451] Loss: 0.541 | Acc: 73.569% \n",
      "[Epoch:5, Iter:452] Loss: 0.542 | Acc: 73.538% \n",
      "[Epoch:5, Iter:453] Loss: 0.541 | Acc: 73.660% \n",
      "[Epoch:5, Iter:454] Loss: 0.543 | Acc: 73.519% \n",
      "[Epoch:5, Iter:455] Loss: 0.545 | Acc: 73.455% \n",
      "[Epoch:5, Iter:456] Loss: 0.546 | Acc: 73.250% \n",
      "[Epoch:5, Iter:457] Loss: 0.547 | Acc: 73.123% \n",
      "[Epoch:5, Iter:458] Loss: 0.546 | Acc: 73.138% \n",
      "[Epoch:5, Iter:459] Loss: 0.547 | Acc: 73.017% \n",
      "[Epoch:5, Iter:460] Loss: 0.546 | Acc: 73.233% \n",
      "[Epoch:5, Iter:461] Loss: 0.547 | Acc: 73.180% \n",
      "[Epoch:5, Iter:462] Loss: 0.547 | Acc: 73.097% \n",
      "[Epoch:5, Iter:463] Loss: 0.548 | Acc: 73.016% \n",
      "[Epoch:5, Iter:464] Loss: 0.546 | Acc: 73.125% \n",
      "[Epoch:5, Iter:465] Loss: 0.544 | Acc: 73.138% \n",
      "[Epoch:5, Iter:466] Loss: 0.544 | Acc: 73.152% \n",
      "[Epoch:5, Iter:467] Loss: 0.546 | Acc: 73.015% \n",
      "[Epoch:5, Iter:468] Loss: 0.543 | Acc: 73.176% \n",
      "[Epoch:5, Iter:469] Loss: 0.543 | Acc: 73.188% \n",
      "[Epoch:5, Iter:470] Loss: 0.542 | Acc: 73.257% \n",
      "[Epoch:5, Iter:471] Loss: 0.540 | Acc: 73.408% \n",
      "[Epoch:5, Iter:472] Loss: 0.540 | Acc: 73.417% \n",
      "[Epoch:5, Iter:473] Loss: 0.540 | Acc: 73.370% \n",
      "[Epoch:5, Iter:474] Loss: 0.540 | Acc: 73.378% \n",
      "[Epoch:5, Iter:475] Loss: 0.539 | Acc: 73.440% \n",
      "[Epoch:5, Iter:476] Loss: 0.539 | Acc: 73.447% \n",
      "[Epoch:5, Iter:477] Loss: 0.541 | Acc: 73.351% \n",
      "[Epoch:5, Iter:478] Loss: 0.541 | Acc: 73.410% \n",
      "[Epoch:5, Iter:479] Loss: 0.540 | Acc: 73.392% \n",
      "[Epoch:5, Iter:480] Loss: 0.540 | Acc: 73.375% \n",
      "[Epoch:5, Iter:481] Loss: 0.539 | Acc: 73.457% \n",
      "[Epoch:5, Iter:482] Loss: 0.538 | Acc: 73.537% \n",
      "[Epoch:5, Iter:483] Loss: 0.538 | Acc: 73.542% \n",
      "[Epoch:5, Iter:484] Loss: 0.538 | Acc: 73.500% \n",
      "[Epoch:5, Iter:485] Loss: 0.538 | Acc: 73.553% \n",
      "[Epoch:5, Iter:486] Loss: 0.540 | Acc: 73.395% \n",
      "[Epoch:5, Iter:487] Loss: 0.539 | Acc: 73.471% \n",
      "[Epoch:5, Iter:488] Loss: 0.541 | Acc: 73.364% \n",
      "[Epoch:5, Iter:489] Loss: 0.542 | Acc: 73.258% \n",
      "[Epoch:5, Iter:490] Loss: 0.542 | Acc: 73.244% \n",
      "[Epoch:5, Iter:491] Loss: 0.544 | Acc: 73.165% \n",
      "[Epoch:5, Iter:492] Loss: 0.543 | Acc: 73.239% \n",
      "[Epoch:5, Iter:493] Loss: 0.543 | Acc: 73.269% \n",
      "[Epoch:5, Iter:494] Loss: 0.544 | Acc: 73.149% \n",
      "[Epoch:5, Iter:495] Loss: 0.544 | Acc: 73.221% \n",
      "[Epoch:5, Iter:496] Loss: 0.544 | Acc: 73.208% \n",
      "[Epoch:5, Iter:497] Loss: 0.544 | Acc: 73.196% \n",
      "[Epoch:5, Iter:498] Loss: 0.544 | Acc: 73.102% \n",
      "[Epoch:5, Iter:499] Loss: 0.544 | Acc: 73.131% \n",
      "[Epoch:5, Iter:500] Loss: 0.543 | Acc: 73.200% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.820%\n",
      "Training set's accuracy (after quantization) is: 72.000%\n",
      "Test set's accuracy (before quantization) is: 72.500%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.540 | Train Acc: 72.820% | Test Loss: 0.544 | Test Acc: 72.500% \n",
      "Quantized Train Loss: 0.549 | Quantized Train Acc: 72.000% | Quantized Test Loss: 0.552 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 6\n",
      "[Epoch:6, Iter:501] Loss: 0.407 | Acc: 76.000% \n",
      "[Epoch:6, Iter:502] Loss: 0.464 | Acc: 77.000% \n",
      "[Epoch:6, Iter:503] Loss: 0.482 | Acc: 76.667% \n",
      "[Epoch:6, Iter:504] Loss: 0.473 | Acc: 76.000% \n",
      "[Epoch:6, Iter:505] Loss: 0.489 | Acc: 76.400% \n",
      "[Epoch:6, Iter:506] Loss: 0.507 | Acc: 75.667% \n",
      "[Epoch:6, Iter:507] Loss: 0.515 | Acc: 74.571% \n",
      "[Epoch:6, Iter:508] Loss: 0.512 | Acc: 74.000% \n",
      "[Epoch:6, Iter:509] Loss: 0.520 | Acc: 74.000% \n",
      "[Epoch:6, Iter:510] Loss: 0.533 | Acc: 73.400% \n",
      "[Epoch:6, Iter:511] Loss: 0.546 | Acc: 72.364% \n",
      "[Epoch:6, Iter:512] Loss: 0.554 | Acc: 72.167% \n",
      "[Epoch:6, Iter:513] Loss: 0.548 | Acc: 72.308% \n",
      "[Epoch:6, Iter:514] Loss: 0.545 | Acc: 72.429% \n",
      "[Epoch:6, Iter:515] Loss: 0.555 | Acc: 71.467% \n",
      "[Epoch:6, Iter:516] Loss: 0.562 | Acc: 70.750% \n",
      "[Epoch:6, Iter:517] Loss: 0.556 | Acc: 71.412% \n",
      "[Epoch:6, Iter:518] Loss: 0.550 | Acc: 71.778% \n",
      "[Epoch:6, Iter:519] Loss: 0.550 | Acc: 71.579% \n",
      "[Epoch:6, Iter:520] Loss: 0.547 | Acc: 71.900% \n",
      "[Epoch:6, Iter:521] Loss: 0.552 | Acc: 71.810% \n",
      "[Epoch:6, Iter:522] Loss: 0.552 | Acc: 72.000% \n",
      "[Epoch:6, Iter:523] Loss: 0.557 | Acc: 71.913% \n",
      "[Epoch:6, Iter:524] Loss: 0.554 | Acc: 71.917% \n",
      "[Epoch:6, Iter:525] Loss: 0.557 | Acc: 71.920% \n",
      "[Epoch:6, Iter:526] Loss: 0.562 | Acc: 71.769% \n",
      "[Epoch:6, Iter:527] Loss: 0.559 | Acc: 72.000% \n",
      "[Epoch:6, Iter:528] Loss: 0.558 | Acc: 72.214% \n",
      "[Epoch:6, Iter:529] Loss: 0.560 | Acc: 71.931% \n",
      "[Epoch:6, Iter:530] Loss: 0.559 | Acc: 71.933% \n",
      "[Epoch:6, Iter:531] Loss: 0.561 | Acc: 71.742% \n",
      "[Epoch:6, Iter:532] Loss: 0.560 | Acc: 71.938% \n",
      "[Epoch:6, Iter:533] Loss: 0.556 | Acc: 72.182% \n",
      "[Epoch:6, Iter:534] Loss: 0.555 | Acc: 72.176% \n",
      "[Epoch:6, Iter:535] Loss: 0.554 | Acc: 72.229% \n",
      "[Epoch:6, Iter:536] Loss: 0.553 | Acc: 72.278% \n",
      "[Epoch:6, Iter:537] Loss: 0.553 | Acc: 72.324% \n",
      "[Epoch:6, Iter:538] Loss: 0.551 | Acc: 72.526% \n",
      "[Epoch:6, Iter:539] Loss: 0.552 | Acc: 72.615% \n",
      "[Epoch:6, Iter:540] Loss: 0.552 | Acc: 72.650% \n",
      "[Epoch:6, Iter:541] Loss: 0.554 | Acc: 72.488% \n",
      "[Epoch:6, Iter:542] Loss: 0.555 | Acc: 72.524% \n",
      "[Epoch:6, Iter:543] Loss: 0.555 | Acc: 72.372% \n",
      "[Epoch:6, Iter:544] Loss: 0.554 | Acc: 72.273% \n",
      "[Epoch:6, Iter:545] Loss: 0.553 | Acc: 72.222% \n",
      "[Epoch:6, Iter:546] Loss: 0.550 | Acc: 72.391% \n",
      "[Epoch:6, Iter:547] Loss: 0.551 | Acc: 72.340% \n",
      "[Epoch:6, Iter:548] Loss: 0.551 | Acc: 72.333% \n",
      "[Epoch:6, Iter:549] Loss: 0.552 | Acc: 72.286% \n",
      "[Epoch:6, Iter:550] Loss: 0.551 | Acc: 72.280% \n",
      "[Epoch:6, Iter:551] Loss: 0.551 | Acc: 72.039% \n",
      "[Epoch:6, Iter:552] Loss: 0.552 | Acc: 72.000% \n",
      "[Epoch:6, Iter:553] Loss: 0.551 | Acc: 72.038% \n",
      "[Epoch:6, Iter:554] Loss: 0.550 | Acc: 72.111% \n",
      "[Epoch:6, Iter:555] Loss: 0.550 | Acc: 72.109% \n",
      "[Epoch:6, Iter:556] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:6, Iter:557] Loss: 0.554 | Acc: 71.789% \n",
      "[Epoch:6, Iter:558] Loss: 0.553 | Acc: 71.862% \n",
      "[Epoch:6, Iter:559] Loss: 0.552 | Acc: 72.000% \n",
      "[Epoch:6, Iter:560] Loss: 0.552 | Acc: 72.067% \n",
      "[Epoch:6, Iter:561] Loss: 0.551 | Acc: 72.230% \n",
      "[Epoch:6, Iter:562] Loss: 0.550 | Acc: 72.258% \n",
      "[Epoch:6, Iter:563] Loss: 0.549 | Acc: 72.254% \n",
      "[Epoch:6, Iter:564] Loss: 0.548 | Acc: 72.250% \n",
      "[Epoch:6, Iter:565] Loss: 0.548 | Acc: 72.308% \n",
      "[Epoch:6, Iter:566] Loss: 0.547 | Acc: 72.394% \n",
      "[Epoch:6, Iter:567] Loss: 0.548 | Acc: 72.269% \n",
      "[Epoch:6, Iter:568] Loss: 0.548 | Acc: 72.235% \n",
      "[Epoch:6, Iter:569] Loss: 0.549 | Acc: 72.174% \n",
      "[Epoch:6, Iter:570] Loss: 0.550 | Acc: 72.143% \n",
      "[Epoch:6, Iter:571] Loss: 0.550 | Acc: 72.169% \n",
      "[Epoch:6, Iter:572] Loss: 0.550 | Acc: 72.194% \n",
      "[Epoch:6, Iter:573] Loss: 0.549 | Acc: 72.137% \n",
      "[Epoch:6, Iter:574] Loss: 0.550 | Acc: 72.135% \n",
      "[Epoch:6, Iter:575] Loss: 0.548 | Acc: 72.320% \n",
      "[Epoch:6, Iter:576] Loss: 0.547 | Acc: 72.421% \n",
      "[Epoch:6, Iter:577] Loss: 0.546 | Acc: 72.468% \n",
      "[Epoch:6, Iter:578] Loss: 0.545 | Acc: 72.513% \n",
      "[Epoch:6, Iter:579] Loss: 0.546 | Acc: 72.481% \n",
      "[Epoch:6, Iter:580] Loss: 0.547 | Acc: 72.300% \n",
      "[Epoch:6, Iter:581] Loss: 0.548 | Acc: 72.247% \n",
      "[Epoch:6, Iter:582] Loss: 0.548 | Acc: 72.244% \n",
      "[Epoch:6, Iter:583] Loss: 0.548 | Acc: 72.217% \n",
      "[Epoch:6, Iter:584] Loss: 0.548 | Acc: 72.286% \n",
      "[Epoch:6, Iter:585] Loss: 0.548 | Acc: 72.188% \n",
      "[Epoch:6, Iter:586] Loss: 0.548 | Acc: 72.256% \n",
      "[Epoch:6, Iter:587] Loss: 0.547 | Acc: 72.276% \n",
      "[Epoch:6, Iter:588] Loss: 0.547 | Acc: 72.318% \n",
      "[Epoch:6, Iter:589] Loss: 0.547 | Acc: 72.360% \n",
      "[Epoch:6, Iter:590] Loss: 0.547 | Acc: 72.289% \n",
      "[Epoch:6, Iter:591] Loss: 0.547 | Acc: 72.308% \n",
      "[Epoch:6, Iter:592] Loss: 0.547 | Acc: 72.261% \n",
      "[Epoch:6, Iter:593] Loss: 0.546 | Acc: 72.344% \n",
      "[Epoch:6, Iter:594] Loss: 0.547 | Acc: 72.319% \n",
      "[Epoch:6, Iter:595] Loss: 0.547 | Acc: 72.400% \n",
      "[Epoch:6, Iter:596] Loss: 0.548 | Acc: 72.333% \n",
      "[Epoch:6, Iter:597] Loss: 0.547 | Acc: 72.392% \n",
      "[Epoch:6, Iter:598] Loss: 0.547 | Acc: 72.490% \n",
      "[Epoch:6, Iter:599] Loss: 0.546 | Acc: 72.505% \n",
      "[Epoch:6, Iter:600] Loss: 0.545 | Acc: 72.560% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.160%\n",
      "Training set's accuracy (after quantization) is: 72.760%\n",
      "Test set's accuracy (before quantization) is: 72.900%\n",
      "Test set's accuracy (after quantization) is: 72.300%\n",
      "Train Loss: 0.557 | Train Acc: 72.160% | Test Loss: 0.562 | Test Acc: 72.900% \n",
      "Quantized Train Loss: 0.544 | Quantized Train Acc: 72.760% | Quantized Test Loss: 0.546 | Quantized Test Acc: 72.300% \n",
      "\n",
      "Epoch: 7\n",
      "[Epoch:7, Iter:601] Loss: 0.598 | Acc: 76.000% \n",
      "[Epoch:7, Iter:602] Loss: 0.633 | Acc: 72.000% \n",
      "[Epoch:7, Iter:603] Loss: 0.624 | Acc: 70.000% \n",
      "[Epoch:7, Iter:604] Loss: 0.617 | Acc: 70.500% \n",
      "[Epoch:7, Iter:605] Loss: 0.609 | Acc: 69.600% \n",
      "[Epoch:7, Iter:606] Loss: 0.619 | Acc: 68.667% \n",
      "[Epoch:7, Iter:607] Loss: 0.615 | Acc: 69.143% \n",
      "[Epoch:7, Iter:608] Loss: 0.600 | Acc: 70.250% \n",
      "[Epoch:7, Iter:609] Loss: 0.589 | Acc: 71.556% \n",
      "[Epoch:7, Iter:610] Loss: 0.582 | Acc: 71.800% \n",
      "[Epoch:7, Iter:611] Loss: 0.576 | Acc: 72.000% \n",
      "[Epoch:7, Iter:612] Loss: 0.569 | Acc: 72.167% \n",
      "[Epoch:7, Iter:613] Loss: 0.571 | Acc: 72.000% \n",
      "[Epoch:7, Iter:614] Loss: 0.568 | Acc: 72.143% \n",
      "[Epoch:7, Iter:615] Loss: 0.570 | Acc: 71.733% \n",
      "[Epoch:7, Iter:616] Loss: 0.567 | Acc: 71.750% \n",
      "[Epoch:7, Iter:617] Loss: 0.569 | Acc: 71.529% \n",
      "[Epoch:7, Iter:618] Loss: 0.564 | Acc: 71.556% \n",
      "[Epoch:7, Iter:619] Loss: 0.561 | Acc: 71.789% \n",
      "[Epoch:7, Iter:620] Loss: 0.556 | Acc: 72.100% \n",
      "[Epoch:7, Iter:621] Loss: 0.556 | Acc: 72.381% \n",
      "[Epoch:7, Iter:622] Loss: 0.558 | Acc: 72.182% \n",
      "[Epoch:7, Iter:623] Loss: 0.559 | Acc: 72.174% \n",
      "[Epoch:7, Iter:624] Loss: 0.556 | Acc: 72.417% \n",
      "[Epoch:7, Iter:625] Loss: 0.552 | Acc: 72.880% \n",
      "[Epoch:7, Iter:626] Loss: 0.551 | Acc: 72.846% \n",
      "[Epoch:7, Iter:627] Loss: 0.546 | Acc: 73.185% \n",
      "[Epoch:7, Iter:628] Loss: 0.545 | Acc: 73.286% \n",
      "[Epoch:7, Iter:629] Loss: 0.543 | Acc: 73.448% \n",
      "[Epoch:7, Iter:630] Loss: 0.542 | Acc: 73.533% \n",
      "[Epoch:7, Iter:631] Loss: 0.541 | Acc: 73.419% \n",
      "[Epoch:7, Iter:632] Loss: 0.541 | Acc: 73.500% \n",
      "[Epoch:7, Iter:633] Loss: 0.539 | Acc: 73.636% \n",
      "[Epoch:7, Iter:634] Loss: 0.538 | Acc: 73.824% \n",
      "[Epoch:7, Iter:635] Loss: 0.536 | Acc: 73.943% \n",
      "[Epoch:7, Iter:636] Loss: 0.538 | Acc: 73.833% \n",
      "[Epoch:7, Iter:637] Loss: 0.536 | Acc: 74.000% \n",
      "[Epoch:7, Iter:638] Loss: 0.537 | Acc: 73.947% \n",
      "[Epoch:7, Iter:639] Loss: 0.538 | Acc: 73.795% \n",
      "[Epoch:7, Iter:640] Loss: 0.539 | Acc: 73.650% \n",
      "[Epoch:7, Iter:641] Loss: 0.539 | Acc: 73.512% \n",
      "[Epoch:7, Iter:642] Loss: 0.540 | Acc: 73.429% \n",
      "[Epoch:7, Iter:643] Loss: 0.541 | Acc: 73.349% \n",
      "[Epoch:7, Iter:644] Loss: 0.540 | Acc: 73.227% \n",
      "[Epoch:7, Iter:645] Loss: 0.542 | Acc: 73.067% \n",
      "[Epoch:7, Iter:646] Loss: 0.542 | Acc: 73.087% \n",
      "[Epoch:7, Iter:647] Loss: 0.541 | Acc: 72.979% \n",
      "[Epoch:7, Iter:648] Loss: 0.540 | Acc: 73.000% \n",
      "[Epoch:7, Iter:649] Loss: 0.545 | Acc: 72.776% \n",
      "[Epoch:7, Iter:650] Loss: 0.546 | Acc: 72.880% \n",
      "[Epoch:7, Iter:651] Loss: 0.546 | Acc: 72.941% \n",
      "[Epoch:7, Iter:652] Loss: 0.547 | Acc: 72.962% \n",
      "[Epoch:7, Iter:653] Loss: 0.548 | Acc: 72.943% \n",
      "[Epoch:7, Iter:654] Loss: 0.548 | Acc: 72.889% \n",
      "[Epoch:7, Iter:655] Loss: 0.550 | Acc: 72.618% \n",
      "[Epoch:7, Iter:656] Loss: 0.552 | Acc: 72.536% \n",
      "[Epoch:7, Iter:657] Loss: 0.551 | Acc: 72.456% \n",
      "[Epoch:7, Iter:658] Loss: 0.552 | Acc: 72.379% \n",
      "[Epoch:7, Iter:659] Loss: 0.550 | Acc: 72.610% \n",
      "[Epoch:7, Iter:660] Loss: 0.550 | Acc: 72.600% \n",
      "[Epoch:7, Iter:661] Loss: 0.549 | Acc: 72.590% \n",
      "[Epoch:7, Iter:662] Loss: 0.549 | Acc: 72.613% \n",
      "[Epoch:7, Iter:663] Loss: 0.550 | Acc: 72.571% \n",
      "[Epoch:7, Iter:664] Loss: 0.548 | Acc: 72.625% \n",
      "[Epoch:7, Iter:665] Loss: 0.549 | Acc: 72.615% \n",
      "[Epoch:7, Iter:666] Loss: 0.548 | Acc: 72.606% \n",
      "[Epoch:7, Iter:667] Loss: 0.549 | Acc: 72.687% \n",
      "[Epoch:7, Iter:668] Loss: 0.549 | Acc: 72.618% \n",
      "[Epoch:7, Iter:669] Loss: 0.549 | Acc: 72.551% \n",
      "[Epoch:7, Iter:670] Loss: 0.549 | Acc: 72.486% \n",
      "[Epoch:7, Iter:671] Loss: 0.548 | Acc: 72.479% \n",
      "[Epoch:7, Iter:672] Loss: 0.548 | Acc: 72.472% \n",
      "[Epoch:7, Iter:673] Loss: 0.547 | Acc: 72.658% \n",
      "[Epoch:7, Iter:674] Loss: 0.547 | Acc: 72.595% \n",
      "[Epoch:7, Iter:675] Loss: 0.547 | Acc: 72.613% \n",
      "[Epoch:7, Iter:676] Loss: 0.550 | Acc: 72.395% \n",
      "[Epoch:7, Iter:677] Loss: 0.550 | Acc: 72.390% \n",
      "[Epoch:7, Iter:678] Loss: 0.549 | Acc: 72.513% \n",
      "[Epoch:7, Iter:679] Loss: 0.550 | Acc: 72.354% \n",
      "[Epoch:7, Iter:680] Loss: 0.550 | Acc: 72.350% \n",
      "[Epoch:7, Iter:681] Loss: 0.550 | Acc: 72.370% \n",
      "[Epoch:7, Iter:682] Loss: 0.550 | Acc: 72.293% \n",
      "[Epoch:7, Iter:683] Loss: 0.548 | Acc: 72.506% \n",
      "[Epoch:7, Iter:684] Loss: 0.549 | Acc: 72.429% \n",
      "[Epoch:7, Iter:685] Loss: 0.548 | Acc: 72.565% \n",
      "[Epoch:7, Iter:686] Loss: 0.547 | Acc: 72.605% \n",
      "[Epoch:7, Iter:687] Loss: 0.546 | Acc: 72.759% \n",
      "[Epoch:7, Iter:688] Loss: 0.544 | Acc: 72.909% \n",
      "[Epoch:7, Iter:689] Loss: 0.544 | Acc: 72.899% \n",
      "[Epoch:7, Iter:690] Loss: 0.544 | Acc: 72.911% \n",
      "[Epoch:7, Iter:691] Loss: 0.544 | Acc: 72.923% \n",
      "[Epoch:7, Iter:692] Loss: 0.543 | Acc: 72.957% \n",
      "[Epoch:7, Iter:693] Loss: 0.543 | Acc: 72.968% \n",
      "[Epoch:7, Iter:694] Loss: 0.543 | Acc: 72.957% \n",
      "[Epoch:7, Iter:695] Loss: 0.543 | Acc: 73.011% \n",
      "[Epoch:7, Iter:696] Loss: 0.544 | Acc: 72.875% \n",
      "[Epoch:7, Iter:697] Loss: 0.544 | Acc: 72.907% \n",
      "[Epoch:7, Iter:698] Loss: 0.544 | Acc: 72.959% \n",
      "[Epoch:7, Iter:699] Loss: 0.543 | Acc: 73.010% \n",
      "[Epoch:7, Iter:700] Loss: 0.544 | Acc: 72.900% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 71.720%\n",
      "Training set's accuracy (after quantization) is: 71.820%\n",
      "Test set's accuracy (before quantization) is: 71.000%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.555 | Train Acc: 71.720% | Test Loss: 0.559 | Test Acc: 71.000% \n",
      "Quantized Train Loss: 0.557 | Quantized Train Acc: 71.820% | Quantized Test Loss: 0.559 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 8\n",
      "[Epoch:8, Iter:701] Loss: 0.536 | Acc: 68.000% \n",
      "[Epoch:8, Iter:702] Loss: 0.500 | Acc: 73.000% \n",
      "[Epoch:8, Iter:703] Loss: 0.528 | Acc: 73.333% \n",
      "[Epoch:8, Iter:704] Loss: 0.539 | Acc: 71.000% \n",
      "[Epoch:8, Iter:705] Loss: 0.521 | Acc: 72.000% \n",
      "[Epoch:8, Iter:706] Loss: 0.509 | Acc: 74.000% \n",
      "[Epoch:8, Iter:707] Loss: 0.538 | Acc: 72.857% \n",
      "[Epoch:8, Iter:708] Loss: 0.540 | Acc: 73.500% \n",
      "[Epoch:8, Iter:709] Loss: 0.529 | Acc: 74.222% \n",
      "[Epoch:8, Iter:710] Loss: 0.524 | Acc: 74.800% \n",
      "[Epoch:8, Iter:711] Loss: 0.525 | Acc: 74.545% \n",
      "[Epoch:8, Iter:712] Loss: 0.536 | Acc: 73.833% \n",
      "[Epoch:8, Iter:713] Loss: 0.533 | Acc: 74.000% \n",
      "[Epoch:8, Iter:714] Loss: 0.536 | Acc: 74.143% \n",
      "[Epoch:8, Iter:715] Loss: 0.542 | Acc: 73.467% \n",
      "[Epoch:8, Iter:716] Loss: 0.539 | Acc: 73.500% \n",
      "[Epoch:8, Iter:717] Loss: 0.537 | Acc: 73.647% \n",
      "[Epoch:8, Iter:718] Loss: 0.530 | Acc: 73.778% \n",
      "[Epoch:8, Iter:719] Loss: 0.536 | Acc: 73.368% \n",
      "[Epoch:8, Iter:720] Loss: 0.531 | Acc: 74.000% \n",
      "[Epoch:8, Iter:721] Loss: 0.530 | Acc: 74.286% \n",
      "[Epoch:8, Iter:722] Loss: 0.530 | Acc: 74.091% \n",
      "[Epoch:8, Iter:723] Loss: 0.528 | Acc: 74.000% \n",
      "[Epoch:8, Iter:724] Loss: 0.536 | Acc: 73.333% \n",
      "[Epoch:8, Iter:725] Loss: 0.533 | Acc: 73.600% \n",
      "[Epoch:8, Iter:726] Loss: 0.531 | Acc: 74.077% \n",
      "[Epoch:8, Iter:727] Loss: 0.533 | Acc: 74.074% \n",
      "[Epoch:8, Iter:728] Loss: 0.531 | Acc: 74.429% \n",
      "[Epoch:8, Iter:729] Loss: 0.531 | Acc: 74.552% \n",
      "[Epoch:8, Iter:730] Loss: 0.534 | Acc: 74.333% \n",
      "[Epoch:8, Iter:731] Loss: 0.538 | Acc: 74.000% \n",
      "[Epoch:8, Iter:732] Loss: 0.539 | Acc: 73.688% \n",
      "[Epoch:8, Iter:733] Loss: 0.540 | Acc: 73.455% \n",
      "[Epoch:8, Iter:734] Loss: 0.539 | Acc: 73.412% \n",
      "[Epoch:8, Iter:735] Loss: 0.537 | Acc: 73.486% \n",
      "[Epoch:8, Iter:736] Loss: 0.535 | Acc: 73.500% \n",
      "[Epoch:8, Iter:737] Loss: 0.535 | Acc: 73.459% \n",
      "[Epoch:8, Iter:738] Loss: 0.537 | Acc: 73.211% \n",
      "[Epoch:8, Iter:739] Loss: 0.539 | Acc: 73.077% \n",
      "[Epoch:8, Iter:740] Loss: 0.538 | Acc: 73.300% \n",
      "[Epoch:8, Iter:741] Loss: 0.543 | Acc: 72.780% \n",
      "[Epoch:8, Iter:742] Loss: 0.543 | Acc: 72.810% \n",
      "[Epoch:8, Iter:743] Loss: 0.540 | Acc: 73.163% \n",
      "[Epoch:8, Iter:744] Loss: 0.542 | Acc: 73.045% \n",
      "[Epoch:8, Iter:745] Loss: 0.544 | Acc: 72.800% \n",
      "[Epoch:8, Iter:746] Loss: 0.546 | Acc: 72.696% \n",
      "[Epoch:8, Iter:747] Loss: 0.545 | Acc: 72.638% \n",
      "[Epoch:8, Iter:748] Loss: 0.545 | Acc: 72.750% \n",
      "[Epoch:8, Iter:749] Loss: 0.546 | Acc: 72.735% \n",
      "[Epoch:8, Iter:750] Loss: 0.546 | Acc: 72.760% \n",
      "[Epoch:8, Iter:751] Loss: 0.549 | Acc: 72.510% \n",
      "[Epoch:8, Iter:752] Loss: 0.547 | Acc: 72.500% \n",
      "[Epoch:8, Iter:753] Loss: 0.548 | Acc: 72.377% \n",
      "[Epoch:8, Iter:754] Loss: 0.547 | Acc: 72.407% \n",
      "[Epoch:8, Iter:755] Loss: 0.547 | Acc: 72.545% \n",
      "[Epoch:8, Iter:756] Loss: 0.547 | Acc: 72.536% \n",
      "[Epoch:8, Iter:757] Loss: 0.547 | Acc: 72.632% \n",
      "[Epoch:8, Iter:758] Loss: 0.547 | Acc: 72.517% \n",
      "[Epoch:8, Iter:759] Loss: 0.548 | Acc: 72.475% \n",
      "[Epoch:8, Iter:760] Loss: 0.546 | Acc: 72.567% \n",
      "[Epoch:8, Iter:761] Loss: 0.546 | Acc: 72.623% \n",
      "[Epoch:8, Iter:762] Loss: 0.546 | Acc: 72.613% \n",
      "[Epoch:8, Iter:763] Loss: 0.548 | Acc: 72.508% \n",
      "[Epoch:8, Iter:764] Loss: 0.547 | Acc: 72.594% \n",
      "[Epoch:8, Iter:765] Loss: 0.549 | Acc: 72.523% \n",
      "[Epoch:8, Iter:766] Loss: 0.549 | Acc: 72.576% \n",
      "[Epoch:8, Iter:767] Loss: 0.549 | Acc: 72.627% \n",
      "[Epoch:8, Iter:768] Loss: 0.549 | Acc: 72.618% \n",
      "[Epoch:8, Iter:769] Loss: 0.547 | Acc: 72.696% \n",
      "[Epoch:8, Iter:770] Loss: 0.545 | Acc: 72.857% \n",
      "[Epoch:8, Iter:771] Loss: 0.544 | Acc: 72.930% \n",
      "[Epoch:8, Iter:772] Loss: 0.543 | Acc: 72.972% \n",
      "[Epoch:8, Iter:773] Loss: 0.543 | Acc: 72.959% \n",
      "[Epoch:8, Iter:774] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:8, Iter:775] Loss: 0.545 | Acc: 72.933% \n",
      "[Epoch:8, Iter:776] Loss: 0.545 | Acc: 72.842% \n",
      "[Epoch:8, Iter:777] Loss: 0.545 | Acc: 72.909% \n",
      "[Epoch:8, Iter:778] Loss: 0.545 | Acc: 72.974% \n",
      "[Epoch:8, Iter:779] Loss: 0.547 | Acc: 72.759% \n",
      "[Epoch:8, Iter:780] Loss: 0.548 | Acc: 72.650% \n",
      "[Epoch:8, Iter:781] Loss: 0.549 | Acc: 72.568% \n",
      "[Epoch:8, Iter:782] Loss: 0.549 | Acc: 72.561% \n",
      "[Epoch:8, Iter:783] Loss: 0.548 | Acc: 72.627% \n",
      "[Epoch:8, Iter:784] Loss: 0.549 | Acc: 72.571% \n",
      "[Epoch:8, Iter:785] Loss: 0.549 | Acc: 72.635% \n",
      "[Epoch:8, Iter:786] Loss: 0.547 | Acc: 72.721% \n",
      "[Epoch:8, Iter:787] Loss: 0.547 | Acc: 72.759% \n",
      "[Epoch:8, Iter:788] Loss: 0.547 | Acc: 72.841% \n",
      "[Epoch:8, Iter:789] Loss: 0.547 | Acc: 72.742% \n",
      "[Epoch:8, Iter:790] Loss: 0.547 | Acc: 72.689% \n",
      "[Epoch:8, Iter:791] Loss: 0.547 | Acc: 72.703% \n",
      "[Epoch:8, Iter:792] Loss: 0.548 | Acc: 72.696% \n",
      "[Epoch:8, Iter:793] Loss: 0.547 | Acc: 72.667% \n",
      "[Epoch:8, Iter:794] Loss: 0.547 | Acc: 72.723% \n",
      "[Epoch:8, Iter:795] Loss: 0.547 | Acc: 72.695% \n",
      "[Epoch:8, Iter:796] Loss: 0.547 | Acc: 72.771% \n",
      "[Epoch:8, Iter:797] Loss: 0.545 | Acc: 72.907% \n",
      "[Epoch:8, Iter:798] Loss: 0.545 | Acc: 72.918% \n",
      "[Epoch:8, Iter:799] Loss: 0.546 | Acc: 72.848% \n",
      "[Epoch:8, Iter:800] Loss: 0.546 | Acc: 72.820% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.740%\n",
      "Training set's accuracy (after quantization) is: 72.000%\n",
      "Test set's accuracy (before quantization) is: 72.700%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.540 | Train Acc: 72.740% | Test Loss: 0.544 | Test Acc: 72.700% \n",
      "Quantized Train Loss: 0.549 | Quantized Train Acc: 72.000% | Quantized Test Loss: 0.552 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 9\n",
      "[Epoch:9, Iter:801] Loss: 0.503 | Acc: 82.000% \n",
      "[Epoch:9, Iter:802] Loss: 0.532 | Acc: 77.000% \n",
      "[Epoch:9, Iter:803] Loss: 0.549 | Acc: 76.667% \n",
      "[Epoch:9, Iter:804] Loss: 0.545 | Acc: 75.500% \n",
      "[Epoch:9, Iter:805] Loss: 0.550 | Acc: 72.800% \n",
      "[Epoch:9, Iter:806] Loss: 0.556 | Acc: 72.333% \n",
      "[Epoch:9, Iter:807] Loss: 0.569 | Acc: 71.429% \n",
      "[Epoch:9, Iter:808] Loss: 0.562 | Acc: 71.250% \n",
      "[Epoch:9, Iter:809] Loss: 0.549 | Acc: 72.222% \n",
      "[Epoch:9, Iter:810] Loss: 0.549 | Acc: 72.200% \n",
      "[Epoch:9, Iter:811] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:9, Iter:812] Loss: 0.542 | Acc: 73.167% \n",
      "[Epoch:9, Iter:813] Loss: 0.541 | Acc: 73.385% \n",
      "[Epoch:9, Iter:814] Loss: 0.546 | Acc: 73.000% \n",
      "[Epoch:9, Iter:815] Loss: 0.547 | Acc: 73.067% \n",
      "[Epoch:9, Iter:816] Loss: 0.548 | Acc: 73.000% \n",
      "[Epoch:9, Iter:817] Loss: 0.551 | Acc: 72.235% \n",
      "[Epoch:9, Iter:818] Loss: 0.554 | Acc: 71.889% \n",
      "[Epoch:9, Iter:819] Loss: 0.554 | Acc: 71.789% \n",
      "[Epoch:9, Iter:820] Loss: 0.548 | Acc: 72.300% \n",
      "[Epoch:9, Iter:821] Loss: 0.545 | Acc: 72.571% \n",
      "[Epoch:9, Iter:822] Loss: 0.545 | Acc: 72.818% \n",
      "[Epoch:9, Iter:823] Loss: 0.547 | Acc: 72.870% \n",
      "[Epoch:9, Iter:824] Loss: 0.547 | Acc: 72.833% \n",
      "[Epoch:9, Iter:825] Loss: 0.548 | Acc: 72.880% \n",
      "[Epoch:9, Iter:826] Loss: 0.548 | Acc: 72.846% \n",
      "[Epoch:9, Iter:827] Loss: 0.548 | Acc: 72.741% \n",
      "[Epoch:9, Iter:828] Loss: 0.550 | Acc: 72.714% \n",
      "[Epoch:9, Iter:829] Loss: 0.548 | Acc: 73.103% \n",
      "[Epoch:9, Iter:830] Loss: 0.545 | Acc: 73.400% \n",
      "[Epoch:9, Iter:831] Loss: 0.548 | Acc: 73.355% \n",
      "[Epoch:9, Iter:832] Loss: 0.551 | Acc: 73.000% \n",
      "[Epoch:9, Iter:833] Loss: 0.552 | Acc: 72.970% \n",
      "[Epoch:9, Iter:834] Loss: 0.551 | Acc: 73.000% \n",
      "[Epoch:9, Iter:835] Loss: 0.550 | Acc: 73.029% \n",
      "[Epoch:9, Iter:836] Loss: 0.548 | Acc: 73.000% \n",
      "[Epoch:9, Iter:837] Loss: 0.548 | Acc: 72.865% \n",
      "[Epoch:9, Iter:838] Loss: 0.546 | Acc: 72.895% \n",
      "[Epoch:9, Iter:839] Loss: 0.547 | Acc: 72.923% \n",
      "[Epoch:9, Iter:840] Loss: 0.549 | Acc: 72.700% \n",
      "[Epoch:9, Iter:841] Loss: 0.551 | Acc: 72.537% \n",
      "[Epoch:9, Iter:842] Loss: 0.550 | Acc: 72.810% \n",
      "[Epoch:9, Iter:843] Loss: 0.551 | Acc: 72.605% \n",
      "[Epoch:9, Iter:844] Loss: 0.550 | Acc: 72.545% \n",
      "[Epoch:9, Iter:845] Loss: 0.548 | Acc: 72.622% \n",
      "[Epoch:9, Iter:846] Loss: 0.549 | Acc: 72.522% \n",
      "[Epoch:9, Iter:847] Loss: 0.551 | Acc: 72.426% \n",
      "[Epoch:9, Iter:848] Loss: 0.550 | Acc: 72.542% \n",
      "[Epoch:9, Iter:849] Loss: 0.551 | Acc: 72.245% \n",
      "[Epoch:9, Iter:850] Loss: 0.552 | Acc: 72.320% \n",
      "[Epoch:9, Iter:851] Loss: 0.551 | Acc: 72.196% \n",
      "[Epoch:9, Iter:852] Loss: 0.553 | Acc: 72.115% \n",
      "[Epoch:9, Iter:853] Loss: 0.552 | Acc: 72.113% \n",
      "[Epoch:9, Iter:854] Loss: 0.553 | Acc: 72.000% \n",
      "[Epoch:9, Iter:855] Loss: 0.552 | Acc: 72.036% \n",
      "[Epoch:9, Iter:856] Loss: 0.550 | Acc: 72.143% \n",
      "[Epoch:9, Iter:857] Loss: 0.551 | Acc: 72.105% \n",
      "[Epoch:9, Iter:858] Loss: 0.550 | Acc: 72.172% \n",
      "[Epoch:9, Iter:859] Loss: 0.548 | Acc: 72.203% \n",
      "[Epoch:9, Iter:860] Loss: 0.549 | Acc: 72.200% \n",
      "[Epoch:9, Iter:861] Loss: 0.551 | Acc: 72.131% \n",
      "[Epoch:9, Iter:862] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:9, Iter:863] Loss: 0.551 | Acc: 72.032% \n",
      "[Epoch:9, Iter:864] Loss: 0.552 | Acc: 72.000% \n",
      "[Epoch:9, Iter:865] Loss: 0.551 | Acc: 72.062% \n",
      "[Epoch:9, Iter:866] Loss: 0.550 | Acc: 72.121% \n",
      "[Epoch:9, Iter:867] Loss: 0.550 | Acc: 72.060% \n",
      "[Epoch:9, Iter:868] Loss: 0.549 | Acc: 72.147% \n",
      "[Epoch:9, Iter:869] Loss: 0.550 | Acc: 72.000% \n",
      "[Epoch:9, Iter:870] Loss: 0.548 | Acc: 72.143% \n",
      "[Epoch:9, Iter:871] Loss: 0.548 | Acc: 72.141% \n",
      "[Epoch:9, Iter:872] Loss: 0.547 | Acc: 72.250% \n",
      "[Epoch:9, Iter:873] Loss: 0.549 | Acc: 72.110% \n",
      "[Epoch:9, Iter:874] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:9, Iter:875] Loss: 0.550 | Acc: 71.840% \n",
      "[Epoch:9, Iter:876] Loss: 0.550 | Acc: 71.816% \n",
      "[Epoch:9, Iter:877] Loss: 0.549 | Acc: 71.948% \n",
      "[Epoch:9, Iter:878] Loss: 0.549 | Acc: 71.974% \n",
      "[Epoch:9, Iter:879] Loss: 0.550 | Acc: 71.975% \n",
      "[Epoch:9, Iter:880] Loss: 0.547 | Acc: 72.100% \n",
      "[Epoch:9, Iter:881] Loss: 0.547 | Acc: 72.099% \n",
      "[Epoch:9, Iter:882] Loss: 0.547 | Acc: 72.195% \n",
      "[Epoch:9, Iter:883] Loss: 0.547 | Acc: 72.217% \n",
      "[Epoch:9, Iter:884] Loss: 0.546 | Acc: 72.262% \n",
      "[Epoch:9, Iter:885] Loss: 0.546 | Acc: 72.306% \n",
      "[Epoch:9, Iter:886] Loss: 0.546 | Acc: 72.233% \n",
      "[Epoch:9, Iter:887] Loss: 0.546 | Acc: 72.253% \n",
      "[Epoch:9, Iter:888] Loss: 0.547 | Acc: 72.182% \n",
      "[Epoch:9, Iter:889] Loss: 0.547 | Acc: 72.157% \n",
      "[Epoch:9, Iter:890] Loss: 0.547 | Acc: 72.222% \n",
      "[Epoch:9, Iter:891] Loss: 0.548 | Acc: 72.286% \n",
      "[Epoch:9, Iter:892] Loss: 0.548 | Acc: 72.239% \n",
      "[Epoch:9, Iter:893] Loss: 0.547 | Acc: 72.301% \n",
      "[Epoch:9, Iter:894] Loss: 0.548 | Acc: 72.277% \n",
      "[Epoch:9, Iter:895] Loss: 0.548 | Acc: 72.232% \n",
      "[Epoch:9, Iter:896] Loss: 0.548 | Acc: 72.312% \n",
      "[Epoch:9, Iter:897] Loss: 0.546 | Acc: 72.371% \n",
      "[Epoch:9, Iter:898] Loss: 0.546 | Acc: 72.286% \n",
      "[Epoch:9, Iter:899] Loss: 0.547 | Acc: 72.242% \n",
      "[Epoch:9, Iter:900] Loss: 0.546 | Acc: 72.340% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.100%\n",
      "Training set's accuracy (after quantization) is: 72.760%\n",
      "Test set's accuracy (before quantization) is: 73.200%\n",
      "Test set's accuracy (after quantization) is: 72.300%\n",
      "Train Loss: 0.542 | Train Acc: 73.100% | Test Loss: 0.545 | Test Acc: 73.200% \n",
      "Quantized Train Loss: 0.544 | Quantized Train Acc: 72.760% | Quantized Test Loss: 0.546 | Quantized Test Acc: 72.300% \n",
      "\n",
      "Epoch: 10\n",
      "[Epoch:10, Iter:901] Loss: 0.530 | Acc: 74.000% \n",
      "[Epoch:10, Iter:902] Loss: 0.580 | Acc: 72.000% \n",
      "[Epoch:10, Iter:903] Loss: 0.616 | Acc: 66.000% \n",
      "[Epoch:10, Iter:904] Loss: 0.581 | Acc: 69.500% \n",
      "[Epoch:10, Iter:905] Loss: 0.567 | Acc: 70.400% \n",
      "[Epoch:10, Iter:906] Loss: 0.567 | Acc: 69.667% \n",
      "[Epoch:10, Iter:907] Loss: 0.567 | Acc: 70.286% \n",
      "[Epoch:10, Iter:908] Loss: 0.557 | Acc: 72.000% \n",
      "[Epoch:10, Iter:909] Loss: 0.556 | Acc: 72.222% \n",
      "[Epoch:10, Iter:910] Loss: 0.555 | Acc: 72.400% \n",
      "[Epoch:10, Iter:911] Loss: 0.561 | Acc: 71.455% \n",
      "[Epoch:10, Iter:912] Loss: 0.567 | Acc: 71.333% \n",
      "[Epoch:10, Iter:913] Loss: 0.560 | Acc: 71.846% \n",
      "[Epoch:10, Iter:914] Loss: 0.563 | Acc: 71.571% \n",
      "[Epoch:10, Iter:915] Loss: 0.561 | Acc: 71.600% \n",
      "[Epoch:10, Iter:916] Loss: 0.562 | Acc: 71.250% \n",
      "[Epoch:10, Iter:917] Loss: 0.553 | Acc: 71.765% \n",
      "[Epoch:10, Iter:918] Loss: 0.552 | Acc: 71.778% \n",
      "[Epoch:10, Iter:919] Loss: 0.550 | Acc: 71.684% \n",
      "[Epoch:10, Iter:920] Loss: 0.547 | Acc: 71.800% \n",
      "[Epoch:10, Iter:921] Loss: 0.548 | Acc: 71.619% \n",
      "[Epoch:10, Iter:922] Loss: 0.551 | Acc: 71.636% \n",
      "[Epoch:10, Iter:923] Loss: 0.552 | Acc: 71.739% \n",
      "[Epoch:10, Iter:924] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:10, Iter:925] Loss: 0.552 | Acc: 71.840% \n",
      "[Epoch:10, Iter:926] Loss: 0.553 | Acc: 71.923% \n",
      "[Epoch:10, Iter:927] Loss: 0.552 | Acc: 72.074% \n",
      "[Epoch:10, Iter:928] Loss: 0.553 | Acc: 71.929% \n",
      "[Epoch:10, Iter:929] Loss: 0.556 | Acc: 71.517% \n",
      "[Epoch:10, Iter:930] Loss: 0.553 | Acc: 71.600% \n",
      "[Epoch:10, Iter:931] Loss: 0.552 | Acc: 71.548% \n",
      "[Epoch:10, Iter:932] Loss: 0.551 | Acc: 71.500% \n",
      "[Epoch:10, Iter:933] Loss: 0.549 | Acc: 71.636% \n",
      "[Epoch:10, Iter:934] Loss: 0.547 | Acc: 72.000% \n",
      "[Epoch:10, Iter:935] Loss: 0.548 | Acc: 72.114% \n",
      "[Epoch:10, Iter:936] Loss: 0.550 | Acc: 71.778% \n",
      "[Epoch:10, Iter:937] Loss: 0.552 | Acc: 71.784% \n",
      "[Epoch:10, Iter:938] Loss: 0.554 | Acc: 71.737% \n",
      "[Epoch:10, Iter:939] Loss: 0.552 | Acc: 72.000% \n",
      "[Epoch:10, Iter:940] Loss: 0.555 | Acc: 71.650% \n",
      "[Epoch:10, Iter:941] Loss: 0.552 | Acc: 71.902% \n",
      "[Epoch:10, Iter:942] Loss: 0.553 | Acc: 71.857% \n",
      "[Epoch:10, Iter:943] Loss: 0.553 | Acc: 71.814% \n",
      "[Epoch:10, Iter:944] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:10, Iter:945] Loss: 0.553 | Acc: 71.911% \n",
      "[Epoch:10, Iter:946] Loss: 0.554 | Acc: 71.783% \n",
      "[Epoch:10, Iter:947] Loss: 0.555 | Acc: 71.702% \n",
      "[Epoch:10, Iter:948] Loss: 0.554 | Acc: 71.792% \n",
      "[Epoch:10, Iter:949] Loss: 0.553 | Acc: 71.959% \n",
      "[Epoch:10, Iter:950] Loss: 0.552 | Acc: 71.960% \n",
      "[Epoch:10, Iter:951] Loss: 0.552 | Acc: 71.922% \n",
      "[Epoch:10, Iter:952] Loss: 0.551 | Acc: 72.115% \n",
      "[Epoch:10, Iter:953] Loss: 0.551 | Acc: 72.075% \n",
      "[Epoch:10, Iter:954] Loss: 0.551 | Acc: 71.963% \n",
      "[Epoch:10, Iter:955] Loss: 0.553 | Acc: 71.818% \n",
      "[Epoch:10, Iter:956] Loss: 0.551 | Acc: 71.929% \n",
      "[Epoch:10, Iter:957] Loss: 0.551 | Acc: 71.895% \n",
      "[Epoch:10, Iter:958] Loss: 0.551 | Acc: 71.931% \n",
      "[Epoch:10, Iter:959] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:10, Iter:960] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:10, Iter:961] Loss: 0.549 | Acc: 72.098% \n",
      "[Epoch:10, Iter:962] Loss: 0.549 | Acc: 72.097% \n",
      "[Epoch:10, Iter:963] Loss: 0.549 | Acc: 72.095% \n",
      "[Epoch:10, Iter:964] Loss: 0.550 | Acc: 72.000% \n",
      "[Epoch:10, Iter:965] Loss: 0.549 | Acc: 72.092% \n",
      "[Epoch:10, Iter:966] Loss: 0.548 | Acc: 72.303% \n",
      "[Epoch:10, Iter:967] Loss: 0.549 | Acc: 72.149% \n",
      "[Epoch:10, Iter:968] Loss: 0.549 | Acc: 72.118% \n",
      "[Epoch:10, Iter:969] Loss: 0.548 | Acc: 72.145% \n",
      "[Epoch:10, Iter:970] Loss: 0.549 | Acc: 72.171% \n",
      "[Epoch:10, Iter:971] Loss: 0.547 | Acc: 72.310% \n",
      "[Epoch:10, Iter:972] Loss: 0.546 | Acc: 72.417% \n",
      "[Epoch:10, Iter:973] Loss: 0.546 | Acc: 72.466% \n",
      "[Epoch:10, Iter:974] Loss: 0.546 | Acc: 72.486% \n",
      "[Epoch:10, Iter:975] Loss: 0.546 | Acc: 72.480% \n",
      "[Epoch:10, Iter:976] Loss: 0.546 | Acc: 72.447% \n",
      "[Epoch:10, Iter:977] Loss: 0.546 | Acc: 72.494% \n",
      "[Epoch:10, Iter:978] Loss: 0.547 | Acc: 72.462% \n",
      "[Epoch:10, Iter:979] Loss: 0.547 | Acc: 72.506% \n",
      "[Epoch:10, Iter:980] Loss: 0.548 | Acc: 72.500% \n",
      "[Epoch:10, Iter:981] Loss: 0.549 | Acc: 72.494% \n",
      "[Epoch:10, Iter:982] Loss: 0.549 | Acc: 72.390% \n",
      "[Epoch:10, Iter:983] Loss: 0.549 | Acc: 72.386% \n",
      "[Epoch:10, Iter:984] Loss: 0.548 | Acc: 72.310% \n",
      "[Epoch:10, Iter:985] Loss: 0.547 | Acc: 72.400% \n",
      "[Epoch:10, Iter:986] Loss: 0.548 | Acc: 72.395% \n",
      "[Epoch:10, Iter:987] Loss: 0.548 | Acc: 72.391% \n",
      "[Epoch:10, Iter:988] Loss: 0.548 | Acc: 72.432% \n",
      "[Epoch:10, Iter:989] Loss: 0.547 | Acc: 72.539% \n",
      "[Epoch:10, Iter:990] Loss: 0.548 | Acc: 72.400% \n",
      "[Epoch:10, Iter:991] Loss: 0.548 | Acc: 72.440% \n",
      "[Epoch:10, Iter:992] Loss: 0.547 | Acc: 72.500% \n",
      "[Epoch:10, Iter:993] Loss: 0.547 | Acc: 72.538% \n",
      "[Epoch:10, Iter:994] Loss: 0.546 | Acc: 72.681% \n",
      "[Epoch:10, Iter:995] Loss: 0.545 | Acc: 72.758% \n",
      "[Epoch:10, Iter:996] Loss: 0.546 | Acc: 72.646% \n",
      "[Epoch:10, Iter:997] Loss: 0.544 | Acc: 72.763% \n",
      "[Epoch:10, Iter:998] Loss: 0.545 | Acc: 72.673% \n",
      "[Epoch:10, Iter:999] Loss: 0.546 | Acc: 72.525% \n",
      "[Epoch:10, Iter:1000] Loss: 0.546 | Acc: 72.580% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.360%\n",
      "Training set's accuracy (after quantization) is: 72.000%\n",
      "Test set's accuracy (before quantization) is: 71.600%\n",
      "Test set's accuracy (after quantization) is: 70.800%\n",
      "Train Loss: 0.549 | Train Acc: 72.360% | Test Loss: 0.553 | Test Acc: 71.600% \n",
      "Quantized Train Loss: 0.549 | Quantized Train Acc: 72.000% | Quantized Test Loss: 0.552 | Quantized Test Acc: 70.800% \n",
      "\n",
      "Epoch: 11\n",
      "[Epoch:11, Iter:1001] Loss: 0.650 | Acc: 62.000% \n",
      "[Epoch:11, Iter:1002] Loss: 0.592 | Acc: 70.000% \n",
      "[Epoch:11, Iter:1003] Loss: 0.526 | Acc: 74.000% \n",
      "[Epoch:11, Iter:1004] Loss: 0.543 | Acc: 73.500% \n",
      "[Epoch:11, Iter:1005] Loss: 0.548 | Acc: 73.200% \n",
      "[Epoch:11, Iter:1006] Loss: 0.558 | Acc: 72.667% \n",
      "[Epoch:11, Iter:1007] Loss: 0.560 | Acc: 72.571% \n",
      "[Epoch:11, Iter:1008] Loss: 0.563 | Acc: 72.750% \n",
      "[Epoch:11, Iter:1009] Loss: 0.554 | Acc: 74.000% \n",
      "[Epoch:11, Iter:1010] Loss: 0.558 | Acc: 73.000% \n",
      "[Epoch:11, Iter:1011] Loss: 0.557 | Acc: 72.364% \n",
      "[Epoch:11, Iter:1012] Loss: 0.555 | Acc: 72.333% \n",
      "[Epoch:11, Iter:1013] Loss: 0.548 | Acc: 72.769% \n",
      "[Epoch:11, Iter:1014] Loss: 0.548 | Acc: 72.571% \n",
      "[Epoch:11, Iter:1015] Loss: 0.549 | Acc: 71.867% \n",
      "[Epoch:11, Iter:1016] Loss: 0.546 | Acc: 72.250% \n",
      "[Epoch:11, Iter:1017] Loss: 0.542 | Acc: 72.588% \n",
      "[Epoch:11, Iter:1018] Loss: 0.542 | Acc: 72.333% \n",
      "[Epoch:11, Iter:1019] Loss: 0.537 | Acc: 72.737% \n",
      "[Epoch:11, Iter:1020] Loss: 0.540 | Acc: 72.700% \n",
      "[Epoch:11, Iter:1021] Loss: 0.539 | Acc: 72.952% \n",
      "[Epoch:11, Iter:1022] Loss: 0.534 | Acc: 73.364% \n",
      "[Epoch:11, Iter:1023] Loss: 0.533 | Acc: 73.739% \n",
      "[Epoch:11, Iter:1024] Loss: 0.533 | Acc: 73.583% \n",
      "[Epoch:11, Iter:1025] Loss: 0.533 | Acc: 73.760% \n",
      "[Epoch:11, Iter:1026] Loss: 0.533 | Acc: 73.846% \n",
      "[Epoch:11, Iter:1027] Loss: 0.532 | Acc: 73.926% \n",
      "[Epoch:11, Iter:1028] Loss: 0.534 | Acc: 73.786% \n",
      "[Epoch:11, Iter:1029] Loss: 0.534 | Acc: 73.931% \n",
      "[Epoch:11, Iter:1030] Loss: 0.532 | Acc: 74.067% \n",
      "[Epoch:11, Iter:1031] Loss: 0.535 | Acc: 74.000% \n",
      "[Epoch:11, Iter:1032] Loss: 0.535 | Acc: 73.938% \n",
      "[Epoch:11, Iter:1033] Loss: 0.535 | Acc: 73.697% \n",
      "[Epoch:11, Iter:1034] Loss: 0.537 | Acc: 73.588% \n",
      "[Epoch:11, Iter:1035] Loss: 0.534 | Acc: 73.829% \n",
      "[Epoch:11, Iter:1036] Loss: 0.533 | Acc: 73.944% \n",
      "[Epoch:11, Iter:1037] Loss: 0.534 | Acc: 73.946% \n",
      "[Epoch:11, Iter:1038] Loss: 0.534 | Acc: 73.895% \n",
      "[Epoch:11, Iter:1039] Loss: 0.534 | Acc: 73.795% \n",
      "[Epoch:11, Iter:1040] Loss: 0.536 | Acc: 73.750% \n",
      "[Epoch:11, Iter:1041] Loss: 0.536 | Acc: 73.805% \n",
      "[Epoch:11, Iter:1042] Loss: 0.534 | Acc: 73.952% \n",
      "[Epoch:11, Iter:1043] Loss: 0.536 | Acc: 73.907% \n",
      "[Epoch:11, Iter:1044] Loss: 0.535 | Acc: 73.864% \n",
      "[Epoch:11, Iter:1045] Loss: 0.535 | Acc: 73.911% \n",
      "[Epoch:11, Iter:1046] Loss: 0.535 | Acc: 73.826% \n",
      "[Epoch:11, Iter:1047] Loss: 0.535 | Acc: 73.872% \n",
      "[Epoch:11, Iter:1048] Loss: 0.535 | Acc: 73.875% \n",
      "[Epoch:11, Iter:1049] Loss: 0.536 | Acc: 73.673% \n",
      "[Epoch:11, Iter:1050] Loss: 0.538 | Acc: 73.480% \n",
      "[Epoch:11, Iter:1051] Loss: 0.539 | Acc: 73.412% \n",
      "[Epoch:11, Iter:1052] Loss: 0.541 | Acc: 73.308% \n",
      "[Epoch:11, Iter:1053] Loss: 0.540 | Acc: 73.434% \n",
      "[Epoch:11, Iter:1054] Loss: 0.540 | Acc: 73.407% \n",
      "[Epoch:11, Iter:1055] Loss: 0.540 | Acc: 73.455% \n",
      "[Epoch:11, Iter:1056] Loss: 0.540 | Acc: 73.500% \n",
      "[Epoch:11, Iter:1057] Loss: 0.541 | Acc: 73.404% \n",
      "[Epoch:11, Iter:1058] Loss: 0.540 | Acc: 73.310% \n",
      "[Epoch:11, Iter:1059] Loss: 0.541 | Acc: 73.254% \n",
      "[Epoch:11, Iter:1060] Loss: 0.544 | Acc: 73.067% \n",
      "[Epoch:11, Iter:1061] Loss: 0.545 | Acc: 72.885% \n",
      "[Epoch:11, Iter:1062] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:11, Iter:1063] Loss: 0.543 | Acc: 73.111% \n",
      "[Epoch:11, Iter:1064] Loss: 0.543 | Acc: 73.125% \n",
      "[Epoch:11, Iter:1065] Loss: 0.540 | Acc: 73.292% \n",
      "[Epoch:11, Iter:1066] Loss: 0.541 | Acc: 73.121% \n",
      "[Epoch:11, Iter:1067] Loss: 0.542 | Acc: 73.134% \n",
      "[Epoch:11, Iter:1068] Loss: 0.541 | Acc: 73.235% \n",
      "[Epoch:11, Iter:1069] Loss: 0.541 | Acc: 73.188% \n",
      "[Epoch:11, Iter:1070] Loss: 0.540 | Acc: 73.229% \n",
      "[Epoch:11, Iter:1071] Loss: 0.541 | Acc: 73.155% \n",
      "[Epoch:11, Iter:1072] Loss: 0.542 | Acc: 73.056% \n",
      "[Epoch:11, Iter:1073] Loss: 0.543 | Acc: 72.959% \n",
      "[Epoch:11, Iter:1074] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:11, Iter:1075] Loss: 0.543 | Acc: 72.880% \n",
      "[Epoch:11, Iter:1076] Loss: 0.544 | Acc: 72.632% \n",
      "[Epoch:11, Iter:1077] Loss: 0.546 | Acc: 72.442% \n",
      "[Epoch:11, Iter:1078] Loss: 0.546 | Acc: 72.410% \n",
      "[Epoch:11, Iter:1079] Loss: 0.545 | Acc: 72.582% \n",
      "[Epoch:11, Iter:1080] Loss: 0.544 | Acc: 72.675% \n",
      "[Epoch:11, Iter:1081] Loss: 0.543 | Acc: 72.691% \n",
      "[Epoch:11, Iter:1082] Loss: 0.543 | Acc: 72.683% \n",
      "[Epoch:11, Iter:1083] Loss: 0.545 | Acc: 72.578% \n",
      "[Epoch:11, Iter:1084] Loss: 0.544 | Acc: 72.762% \n",
      "[Epoch:11, Iter:1085] Loss: 0.544 | Acc: 72.776% \n",
      "[Epoch:11, Iter:1086] Loss: 0.544 | Acc: 72.744% \n",
      "[Epoch:11, Iter:1087] Loss: 0.544 | Acc: 72.782% \n",
      "[Epoch:11, Iter:1088] Loss: 0.544 | Acc: 72.705% \n",
      "[Epoch:11, Iter:1089] Loss: 0.544 | Acc: 72.809% \n",
      "[Epoch:11, Iter:1090] Loss: 0.544 | Acc: 72.800% \n",
      "[Epoch:11, Iter:1091] Loss: 0.544 | Acc: 72.791% \n",
      "[Epoch:11, Iter:1092] Loss: 0.544 | Acc: 72.761% \n",
      "[Epoch:11, Iter:1093] Loss: 0.544 | Acc: 72.774% \n",
      "[Epoch:11, Iter:1094] Loss: 0.545 | Acc: 72.681% \n",
      "[Epoch:11, Iter:1095] Loss: 0.544 | Acc: 72.716% \n",
      "[Epoch:11, Iter:1096] Loss: 0.544 | Acc: 72.729% \n",
      "[Epoch:11, Iter:1097] Loss: 0.545 | Acc: 72.701% \n",
      "[Epoch:11, Iter:1098] Loss: 0.545 | Acc: 72.694% \n",
      "[Epoch:11, Iter:1099] Loss: 0.545 | Acc: 72.707% \n",
      "[Epoch:11, Iter:1100] Loss: 0.545 | Acc: 72.680% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.140%\n",
      "Training set's accuracy (after quantization) is: 72.120%\n",
      "Test set's accuracy (before quantization) is: 71.500%\n",
      "Test set's accuracy (after quantization) is: 70.900%\n",
      "Train Loss: 0.551 | Train Acc: 72.140% | Test Loss: 0.554 | Test Acc: 71.500% \n",
      "Quantized Train Loss: 0.549 | Quantized Train Acc: 72.120% | Quantized Test Loss: 0.551 | Quantized Test Acc: 70.900% \n",
      "\n",
      "Epoch: 12\n",
      "[Epoch:12, Iter:1101] Loss: 0.562 | Acc: 74.000% \n",
      "[Epoch:12, Iter:1102] Loss: 0.534 | Acc: 75.000% \n",
      "[Epoch:12, Iter:1103] Loss: 0.556 | Acc: 73.333% \n",
      "[Epoch:12, Iter:1104] Loss: 0.568 | Acc: 71.000% \n",
      "[Epoch:12, Iter:1105] Loss: 0.563 | Acc: 72.000% \n",
      "[Epoch:12, Iter:1106] Loss: 0.570 | Acc: 71.000% \n",
      "[Epoch:12, Iter:1107] Loss: 0.571 | Acc: 70.571% \n",
      "[Epoch:12, Iter:1108] Loss: 0.547 | Acc: 72.500% \n",
      "[Epoch:12, Iter:1109] Loss: 0.534 | Acc: 73.333% \n",
      "[Epoch:12, Iter:1110] Loss: 0.530 | Acc: 73.400% \n",
      "[Epoch:12, Iter:1111] Loss: 0.536 | Acc: 73.636% \n",
      "[Epoch:12, Iter:1112] Loss: 0.534 | Acc: 74.000% \n",
      "[Epoch:12, Iter:1113] Loss: 0.538 | Acc: 73.385% \n",
      "[Epoch:12, Iter:1114] Loss: 0.533 | Acc: 74.000% \n",
      "[Epoch:12, Iter:1115] Loss: 0.530 | Acc: 74.267% \n",
      "[Epoch:12, Iter:1116] Loss: 0.532 | Acc: 73.875% \n",
      "[Epoch:12, Iter:1117] Loss: 0.537 | Acc: 73.412% \n",
      "[Epoch:12, Iter:1118] Loss: 0.537 | Acc: 73.333% \n",
      "[Epoch:12, Iter:1119] Loss: 0.541 | Acc: 73.263% \n",
      "[Epoch:12, Iter:1120] Loss: 0.541 | Acc: 73.200% \n",
      "[Epoch:12, Iter:1121] Loss: 0.542 | Acc: 73.048% \n",
      "[Epoch:12, Iter:1122] Loss: 0.543 | Acc: 72.818% \n",
      "[Epoch:12, Iter:1123] Loss: 0.540 | Acc: 73.391% \n",
      "[Epoch:12, Iter:1124] Loss: 0.538 | Acc: 73.500% \n",
      "[Epoch:12, Iter:1125] Loss: 0.542 | Acc: 73.120% \n",
      "[Epoch:12, Iter:1126] Loss: 0.544 | Acc: 72.769% \n",
      "[Epoch:12, Iter:1127] Loss: 0.546 | Acc: 72.741% \n",
      "[Epoch:12, Iter:1128] Loss: 0.544 | Acc: 72.571% \n",
      "[Epoch:12, Iter:1129] Loss: 0.548 | Acc: 72.414% \n",
      "[Epoch:12, Iter:1130] Loss: 0.548 | Acc: 72.467% \n",
      "[Epoch:12, Iter:1131] Loss: 0.545 | Acc: 72.839% \n",
      "[Epoch:12, Iter:1132] Loss: 0.551 | Acc: 72.562% \n",
      "[Epoch:12, Iter:1133] Loss: 0.549 | Acc: 72.667% \n",
      "[Epoch:12, Iter:1134] Loss: 0.546 | Acc: 72.941% \n",
      "[Epoch:12, Iter:1135] Loss: 0.547 | Acc: 72.857% \n",
      "[Epoch:12, Iter:1136] Loss: 0.550 | Acc: 72.667% \n",
      "[Epoch:12, Iter:1137] Loss: 0.551 | Acc: 72.757% \n",
      "[Epoch:12, Iter:1138] Loss: 0.551 | Acc: 72.737% \n",
      "[Epoch:12, Iter:1139] Loss: 0.550 | Acc: 72.769% \n",
      "[Epoch:12, Iter:1140] Loss: 0.547 | Acc: 73.000% \n",
      "[Epoch:12, Iter:1141] Loss: 0.548 | Acc: 72.976% \n",
      "[Epoch:12, Iter:1142] Loss: 0.546 | Acc: 73.095% \n",
      "[Epoch:12, Iter:1143] Loss: 0.547 | Acc: 73.023% \n",
      "[Epoch:12, Iter:1144] Loss: 0.546 | Acc: 73.045% \n",
      "[Epoch:12, Iter:1145] Loss: 0.543 | Acc: 73.244% \n",
      "[Epoch:12, Iter:1146] Loss: 0.544 | Acc: 73.174% \n",
      "[Epoch:12, Iter:1147] Loss: 0.545 | Acc: 73.106% \n",
      "[Epoch:12, Iter:1148] Loss: 0.545 | Acc: 73.125% \n",
      "[Epoch:12, Iter:1149] Loss: 0.544 | Acc: 73.102% \n",
      "[Epoch:12, Iter:1150] Loss: 0.545 | Acc: 72.960% \n",
      "[Epoch:12, Iter:1151] Loss: 0.544 | Acc: 73.059% \n",
      "[Epoch:12, Iter:1152] Loss: 0.544 | Acc: 73.038% \n",
      "[Epoch:12, Iter:1153] Loss: 0.543 | Acc: 72.943% \n",
      "[Epoch:12, Iter:1154] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:12, Iter:1155] Loss: 0.542 | Acc: 73.127% \n",
      "[Epoch:12, Iter:1156] Loss: 0.545 | Acc: 72.893% \n",
      "[Epoch:12, Iter:1157] Loss: 0.547 | Acc: 72.912% \n",
      "[Epoch:12, Iter:1158] Loss: 0.548 | Acc: 72.862% \n",
      "[Epoch:12, Iter:1159] Loss: 0.546 | Acc: 72.881% \n",
      "[Epoch:12, Iter:1160] Loss: 0.546 | Acc: 72.933% \n",
      "[Epoch:12, Iter:1161] Loss: 0.545 | Acc: 72.918% \n",
      "[Epoch:12, Iter:1162] Loss: 0.549 | Acc: 72.613% \n",
      "[Epoch:12, Iter:1163] Loss: 0.549 | Acc: 72.571% \n",
      "[Epoch:12, Iter:1164] Loss: 0.549 | Acc: 72.625% \n",
      "[Epoch:12, Iter:1165] Loss: 0.548 | Acc: 72.646% \n",
      "[Epoch:12, Iter:1166] Loss: 0.550 | Acc: 72.576% \n",
      "[Epoch:12, Iter:1167] Loss: 0.549 | Acc: 72.746% \n",
      "[Epoch:12, Iter:1168] Loss: 0.548 | Acc: 72.853% \n",
      "[Epoch:12, Iter:1169] Loss: 0.548 | Acc: 72.870% \n",
      "[Epoch:12, Iter:1170] Loss: 0.547 | Acc: 72.800% \n",
      "[Epoch:12, Iter:1171] Loss: 0.547 | Acc: 72.845% \n",
      "[Epoch:12, Iter:1172] Loss: 0.547 | Acc: 72.833% \n",
      "[Epoch:12, Iter:1173] Loss: 0.548 | Acc: 72.767% \n",
      "[Epoch:12, Iter:1174] Loss: 0.548 | Acc: 72.757% \n",
      "[Epoch:12, Iter:1175] Loss: 0.547 | Acc: 72.827% \n",
      "[Epoch:12, Iter:1176] Loss: 0.547 | Acc: 72.842% \n",
      "[Epoch:12, Iter:1177] Loss: 0.545 | Acc: 72.961% \n",
      "[Epoch:12, Iter:1178] Loss: 0.544 | Acc: 73.000% \n",
      "[Epoch:12, Iter:1179] Loss: 0.544 | Acc: 73.013% \n",
      "[Epoch:12, Iter:1180] Loss: 0.544 | Acc: 73.025% \n",
      "[Epoch:12, Iter:1181] Loss: 0.544 | Acc: 73.037% \n",
      "[Epoch:12, Iter:1182] Loss: 0.544 | Acc: 72.951% \n",
      "[Epoch:12, Iter:1183] Loss: 0.544 | Acc: 72.988% \n",
      "[Epoch:12, Iter:1184] Loss: 0.544 | Acc: 72.976% \n",
      "[Epoch:12, Iter:1185] Loss: 0.545 | Acc: 72.871% \n",
      "[Epoch:12, Iter:1186] Loss: 0.543 | Acc: 72.930% \n",
      "[Epoch:12, Iter:1187] Loss: 0.543 | Acc: 72.943% \n",
      "[Epoch:12, Iter:1188] Loss: 0.545 | Acc: 72.886% \n",
      "[Epoch:12, Iter:1189] Loss: 0.544 | Acc: 72.966% \n",
      "[Epoch:12, Iter:1190] Loss: 0.545 | Acc: 72.867% \n",
      "[Epoch:12, Iter:1191] Loss: 0.545 | Acc: 72.857% \n",
      "[Epoch:12, Iter:1192] Loss: 0.545 | Acc: 72.870% \n",
      "[Epoch:12, Iter:1193] Loss: 0.543 | Acc: 73.011% \n",
      "[Epoch:12, Iter:1194] Loss: 0.544 | Acc: 72.872% \n",
      "[Epoch:12, Iter:1195] Loss: 0.544 | Acc: 72.947% \n",
      "[Epoch:12, Iter:1196] Loss: 0.545 | Acc: 72.833% \n",
      "[Epoch:12, Iter:1197] Loss: 0.545 | Acc: 72.763% \n",
      "[Epoch:12, Iter:1198] Loss: 0.545 | Acc: 72.776% \n",
      "[Epoch:12, Iter:1199] Loss: 0.545 | Acc: 72.788% \n",
      "[Epoch:12, Iter:1200] Loss: 0.545 | Acc: 72.800% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.700%\n",
      "Training set's accuracy (after quantization) is: 72.460%\n",
      "Test set's accuracy (before quantization) is: 71.900%\n",
      "Test set's accuracy (after quantization) is: 72.300%\n",
      "Train Loss: 0.543 | Train Acc: 72.700% | Test Loss: 0.546 | Test Acc: 71.900% \n",
      "Quantized Train Loss: 0.545 | Quantized Train Acc: 72.460% | Quantized Test Loss: 0.548 | Quantized Test Acc: 72.300% \n",
      "\n",
      "Epoch: 13\n",
      "[Epoch:13, Iter:1201] Loss: 0.656 | Acc: 58.000% \n",
      "[Epoch:13, Iter:1202] Loss: 0.627 | Acc: 59.000% \n",
      "[Epoch:13, Iter:1203] Loss: 0.579 | Acc: 67.333% \n",
      "[Epoch:13, Iter:1204] Loss: 0.564 | Acc: 70.000% \n",
      "[Epoch:13, Iter:1205] Loss: 0.564 | Acc: 70.400% \n",
      "[Epoch:13, Iter:1206] Loss: 0.574 | Acc: 70.667% \n",
      "[Epoch:13, Iter:1207] Loss: 0.577 | Acc: 70.000% \n",
      "[Epoch:13, Iter:1208] Loss: 0.578 | Acc: 70.000% \n",
      "[Epoch:13, Iter:1209] Loss: 0.572 | Acc: 70.667% \n",
      "[Epoch:13, Iter:1210] Loss: 0.570 | Acc: 71.000% \n",
      "[Epoch:13, Iter:1211] Loss: 0.570 | Acc: 71.091% \n",
      "[Epoch:13, Iter:1212] Loss: 0.571 | Acc: 71.167% \n",
      "[Epoch:13, Iter:1213] Loss: 0.566 | Acc: 71.231% \n",
      "[Epoch:13, Iter:1214] Loss: 0.565 | Acc: 71.286% \n",
      "[Epoch:13, Iter:1215] Loss: 0.570 | Acc: 70.933% \n",
      "[Epoch:13, Iter:1216] Loss: 0.567 | Acc: 70.875% \n",
      "[Epoch:13, Iter:1217] Loss: 0.569 | Acc: 70.941% \n",
      "[Epoch:13, Iter:1218] Loss: 0.571 | Acc: 70.889% \n",
      "[Epoch:13, Iter:1219] Loss: 0.567 | Acc: 70.947% \n",
      "[Epoch:13, Iter:1220] Loss: 0.564 | Acc: 71.100% \n",
      "[Epoch:13, Iter:1221] Loss: 0.563 | Acc: 71.238% \n",
      "[Epoch:13, Iter:1222] Loss: 0.562 | Acc: 71.182% \n",
      "[Epoch:13, Iter:1223] Loss: 0.559 | Acc: 71.391% \n",
      "[Epoch:13, Iter:1224] Loss: 0.558 | Acc: 71.667% \n",
      "[Epoch:13, Iter:1225] Loss: 0.557 | Acc: 71.600% \n",
      "[Epoch:13, Iter:1226] Loss: 0.559 | Acc: 71.615% \n",
      "[Epoch:13, Iter:1227] Loss: 0.559 | Acc: 71.407% \n",
      "[Epoch:13, Iter:1228] Loss: 0.559 | Acc: 71.429% \n",
      "[Epoch:13, Iter:1229] Loss: 0.556 | Acc: 71.655% \n",
      "[Epoch:13, Iter:1230] Loss: 0.560 | Acc: 71.267% \n",
      "[Epoch:13, Iter:1231] Loss: 0.559 | Acc: 71.548% \n",
      "[Epoch:13, Iter:1232] Loss: 0.557 | Acc: 71.750% \n",
      "[Epoch:13, Iter:1233] Loss: 0.554 | Acc: 72.061% \n",
      "[Epoch:13, Iter:1234] Loss: 0.550 | Acc: 72.529% \n",
      "[Epoch:13, Iter:1235] Loss: 0.549 | Acc: 72.800% \n",
      "[Epoch:13, Iter:1236] Loss: 0.550 | Acc: 72.611% \n",
      "[Epoch:13, Iter:1237] Loss: 0.549 | Acc: 72.649% \n",
      "[Epoch:13, Iter:1238] Loss: 0.549 | Acc: 72.632% \n",
      "[Epoch:13, Iter:1239] Loss: 0.546 | Acc: 72.872% \n",
      "[Epoch:13, Iter:1240] Loss: 0.544 | Acc: 72.950% \n",
      "[Epoch:13, Iter:1241] Loss: 0.546 | Acc: 72.780% \n",
      "[Epoch:13, Iter:1242] Loss: 0.545 | Acc: 72.667% \n",
      "[Epoch:13, Iter:1243] Loss: 0.546 | Acc: 72.512% \n",
      "[Epoch:13, Iter:1244] Loss: 0.544 | Acc: 72.727% \n",
      "[Epoch:13, Iter:1245] Loss: 0.545 | Acc: 72.711% \n",
      "[Epoch:13, Iter:1246] Loss: 0.542 | Acc: 73.000% \n",
      "[Epoch:13, Iter:1247] Loss: 0.542 | Acc: 72.894% \n",
      "[Epoch:13, Iter:1248] Loss: 0.542 | Acc: 72.917% \n",
      "[Epoch:13, Iter:1249] Loss: 0.543 | Acc: 72.694% \n",
      "[Epoch:13, Iter:1250] Loss: 0.542 | Acc: 72.800% \n",
      "[Epoch:13, Iter:1251] Loss: 0.546 | Acc: 72.745% \n",
      "[Epoch:13, Iter:1252] Loss: 0.547 | Acc: 72.692% \n",
      "[Epoch:13, Iter:1253] Loss: 0.547 | Acc: 72.717% \n",
      "[Epoch:13, Iter:1254] Loss: 0.547 | Acc: 72.741% \n",
      "[Epoch:13, Iter:1255] Loss: 0.548 | Acc: 72.655% \n",
      "[Epoch:13, Iter:1256] Loss: 0.548 | Acc: 72.643% \n",
      "[Epoch:13, Iter:1257] Loss: 0.547 | Acc: 72.702% \n",
      "[Epoch:13, Iter:1258] Loss: 0.547 | Acc: 72.724% \n",
      "[Epoch:13, Iter:1259] Loss: 0.547 | Acc: 72.644% \n",
      "[Epoch:13, Iter:1260] Loss: 0.548 | Acc: 72.500% \n",
      "[Epoch:13, Iter:1261] Loss: 0.549 | Acc: 72.459% \n",
      "[Epoch:13, Iter:1262] Loss: 0.549 | Acc: 72.419% \n",
      "[Epoch:13, Iter:1263] Loss: 0.547 | Acc: 72.508% \n",
      "[Epoch:13, Iter:1264] Loss: 0.547 | Acc: 72.500% \n",
      "[Epoch:13, Iter:1265] Loss: 0.547 | Acc: 72.523% \n",
      "[Epoch:13, Iter:1266] Loss: 0.547 | Acc: 72.485% \n",
      "[Epoch:13, Iter:1267] Loss: 0.547 | Acc: 72.418% \n",
      "[Epoch:13, Iter:1268] Loss: 0.547 | Acc: 72.353% \n",
      "[Epoch:13, Iter:1269] Loss: 0.545 | Acc: 72.522% \n",
      "[Epoch:13, Iter:1270] Loss: 0.543 | Acc: 72.657% \n",
      "[Epoch:13, Iter:1271] Loss: 0.543 | Acc: 72.732% \n",
      "[Epoch:13, Iter:1272] Loss: 0.542 | Acc: 72.806% \n",
      "[Epoch:13, Iter:1273] Loss: 0.543 | Acc: 72.712% \n",
      "[Epoch:13, Iter:1274] Loss: 0.542 | Acc: 72.811% \n",
      "[Epoch:13, Iter:1275] Loss: 0.542 | Acc: 72.880% \n",
      "[Epoch:13, Iter:1276] Loss: 0.541 | Acc: 72.947% \n",
      "[Epoch:13, Iter:1277] Loss: 0.542 | Acc: 72.857% \n",
      "[Epoch:13, Iter:1278] Loss: 0.542 | Acc: 72.923% \n",
      "[Epoch:13, Iter:1279] Loss: 0.543 | Acc: 72.835% \n",
      "[Epoch:13, Iter:1280] Loss: 0.543 | Acc: 72.825% \n",
      "[Epoch:13, Iter:1281] Loss: 0.543 | Acc: 72.840% \n",
      "[Epoch:13, Iter:1282] Loss: 0.545 | Acc: 72.659% \n",
      "[Epoch:13, Iter:1283] Loss: 0.547 | Acc: 72.530% \n",
      "[Epoch:13, Iter:1284] Loss: 0.548 | Acc: 72.476% \n",
      "[Epoch:13, Iter:1285] Loss: 0.547 | Acc: 72.541% \n",
      "[Epoch:13, Iter:1286] Loss: 0.549 | Acc: 72.488% \n",
      "[Epoch:13, Iter:1287] Loss: 0.549 | Acc: 72.529% \n",
      "[Epoch:13, Iter:1288] Loss: 0.549 | Acc: 72.455% \n",
      "[Epoch:13, Iter:1289] Loss: 0.549 | Acc: 72.494% \n",
      "[Epoch:13, Iter:1290] Loss: 0.549 | Acc: 72.578% \n",
      "[Epoch:13, Iter:1291] Loss: 0.549 | Acc: 72.571% \n",
      "[Epoch:13, Iter:1292] Loss: 0.548 | Acc: 72.674% \n",
      "[Epoch:13, Iter:1293] Loss: 0.549 | Acc: 72.581% \n",
      "[Epoch:13, Iter:1294] Loss: 0.548 | Acc: 72.617% \n",
      "[Epoch:13, Iter:1295] Loss: 0.547 | Acc: 72.716% \n",
      "[Epoch:13, Iter:1296] Loss: 0.546 | Acc: 72.708% \n",
      "[Epoch:13, Iter:1297] Loss: 0.547 | Acc: 72.722% \n",
      "[Epoch:13, Iter:1298] Loss: 0.546 | Acc: 72.776% \n",
      "[Epoch:13, Iter:1299] Loss: 0.546 | Acc: 72.768% \n",
      "[Epoch:13, Iter:1300] Loss: 0.545 | Acc: 72.840% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.900%\n",
      "Training set's accuracy (after quantization) is: 72.620%\n",
      "Test set's accuracy (before quantization) is: 73.200%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.541 | Train Acc: 72.900% | Test Loss: 0.545 | Test Acc: 73.200% \n",
      "Quantized Train Loss: 0.542 | Quantized Train Acc: 72.620% | Quantized Test Loss: 0.544 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 14\n",
      "[Epoch:14, Iter:1301] Loss: 0.651 | Acc: 64.000% \n",
      "[Epoch:14, Iter:1302] Loss: 0.577 | Acc: 70.000% \n",
      "[Epoch:14, Iter:1303] Loss: 0.596 | Acc: 70.667% \n",
      "[Epoch:14, Iter:1304] Loss: 0.621 | Acc: 69.500% \n",
      "[Epoch:14, Iter:1305] Loss: 0.605 | Acc: 69.200% \n",
      "[Epoch:14, Iter:1306] Loss: 0.583 | Acc: 70.333% \n",
      "[Epoch:14, Iter:1307] Loss: 0.588 | Acc: 69.714% \n",
      "[Epoch:14, Iter:1308] Loss: 0.579 | Acc: 70.250% \n",
      "[Epoch:14, Iter:1309] Loss: 0.572 | Acc: 70.667% \n",
      "[Epoch:14, Iter:1310] Loss: 0.579 | Acc: 70.000% \n",
      "[Epoch:14, Iter:1311] Loss: 0.576 | Acc: 70.727% \n",
      "[Epoch:14, Iter:1312] Loss: 0.577 | Acc: 70.167% \n",
      "[Epoch:14, Iter:1313] Loss: 0.582 | Acc: 70.615% \n",
      "[Epoch:14, Iter:1314] Loss: 0.574 | Acc: 71.429% \n",
      "[Epoch:14, Iter:1315] Loss: 0.572 | Acc: 71.333% \n",
      "[Epoch:14, Iter:1316] Loss: 0.573 | Acc: 71.125% \n",
      "[Epoch:14, Iter:1317] Loss: 0.576 | Acc: 70.824% \n",
      "[Epoch:14, Iter:1318] Loss: 0.578 | Acc: 70.111% \n",
      "[Epoch:14, Iter:1319] Loss: 0.576 | Acc: 70.000% \n",
      "[Epoch:14, Iter:1320] Loss: 0.573 | Acc: 70.300% \n",
      "[Epoch:14, Iter:1321] Loss: 0.571 | Acc: 70.571% \n",
      "[Epoch:14, Iter:1322] Loss: 0.574 | Acc: 70.455% \n",
      "[Epoch:14, Iter:1323] Loss: 0.573 | Acc: 70.522% \n",
      "[Epoch:14, Iter:1324] Loss: 0.570 | Acc: 70.833% \n",
      "[Epoch:14, Iter:1325] Loss: 0.567 | Acc: 70.720% \n",
      "[Epoch:14, Iter:1326] Loss: 0.565 | Acc: 70.846% \n",
      "[Epoch:14, Iter:1327] Loss: 0.562 | Acc: 71.037% \n",
      "[Epoch:14, Iter:1328] Loss: 0.562 | Acc: 71.214% \n",
      "[Epoch:14, Iter:1329] Loss: 0.560 | Acc: 71.448% \n",
      "[Epoch:14, Iter:1330] Loss: 0.559 | Acc: 71.400% \n",
      "[Epoch:14, Iter:1331] Loss: 0.557 | Acc: 71.613% \n",
      "[Epoch:14, Iter:1332] Loss: 0.561 | Acc: 71.188% \n",
      "[Epoch:14, Iter:1333] Loss: 0.559 | Acc: 71.333% \n",
      "[Epoch:14, Iter:1334] Loss: 0.559 | Acc: 71.294% \n",
      "[Epoch:14, Iter:1335] Loss: 0.558 | Acc: 71.429% \n",
      "[Epoch:14, Iter:1336] Loss: 0.558 | Acc: 71.556% \n",
      "[Epoch:14, Iter:1337] Loss: 0.561 | Acc: 71.243% \n",
      "[Epoch:14, Iter:1338] Loss: 0.560 | Acc: 71.263% \n",
      "[Epoch:14, Iter:1339] Loss: 0.556 | Acc: 71.487% \n",
      "[Epoch:14, Iter:1340] Loss: 0.553 | Acc: 71.800% \n",
      "[Epoch:14, Iter:1341] Loss: 0.552 | Acc: 71.902% \n",
      "[Epoch:14, Iter:1342] Loss: 0.551 | Acc: 71.952% \n",
      "[Epoch:14, Iter:1343] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:14, Iter:1344] Loss: 0.550 | Acc: 72.045% \n",
      "[Epoch:14, Iter:1345] Loss: 0.549 | Acc: 72.178% \n",
      "[Epoch:14, Iter:1346] Loss: 0.550 | Acc: 72.130% \n",
      "[Epoch:14, Iter:1347] Loss: 0.550 | Acc: 72.128% \n",
      "[Epoch:14, Iter:1348] Loss: 0.547 | Acc: 72.250% \n",
      "[Epoch:14, Iter:1349] Loss: 0.547 | Acc: 72.327% \n",
      "[Epoch:14, Iter:1350] Loss: 0.545 | Acc: 72.360% \n",
      "[Epoch:14, Iter:1351] Loss: 0.546 | Acc: 72.392% \n",
      "[Epoch:14, Iter:1352] Loss: 0.547 | Acc: 72.308% \n",
      "[Epoch:14, Iter:1353] Loss: 0.546 | Acc: 72.566% \n",
      "[Epoch:14, Iter:1354] Loss: 0.546 | Acc: 72.556% \n",
      "[Epoch:14, Iter:1355] Loss: 0.548 | Acc: 72.400% \n",
      "[Epoch:14, Iter:1356] Loss: 0.547 | Acc: 72.321% \n",
      "[Epoch:14, Iter:1357] Loss: 0.549 | Acc: 72.246% \n",
      "[Epoch:14, Iter:1358] Loss: 0.548 | Acc: 72.345% \n",
      "[Epoch:14, Iter:1359] Loss: 0.548 | Acc: 72.339% \n",
      "[Epoch:14, Iter:1360] Loss: 0.547 | Acc: 72.433% \n",
      "[Epoch:14, Iter:1361] Loss: 0.546 | Acc: 72.492% \n",
      "[Epoch:14, Iter:1362] Loss: 0.546 | Acc: 72.484% \n",
      "[Epoch:14, Iter:1363] Loss: 0.545 | Acc: 72.571% \n",
      "[Epoch:14, Iter:1364] Loss: 0.543 | Acc: 72.688% \n",
      "[Epoch:14, Iter:1365] Loss: 0.544 | Acc: 72.738% \n",
      "[Epoch:14, Iter:1366] Loss: 0.545 | Acc: 72.758% \n",
      "[Epoch:14, Iter:1367] Loss: 0.543 | Acc: 72.896% \n",
      "[Epoch:14, Iter:1368] Loss: 0.545 | Acc: 72.794% \n",
      "[Epoch:14, Iter:1369] Loss: 0.545 | Acc: 72.754% \n",
      "[Epoch:14, Iter:1370] Loss: 0.544 | Acc: 72.771% \n",
      "[Epoch:14, Iter:1371] Loss: 0.546 | Acc: 72.620% \n",
      "[Epoch:14, Iter:1372] Loss: 0.546 | Acc: 72.667% \n",
      "[Epoch:14, Iter:1373] Loss: 0.544 | Acc: 72.767% \n",
      "[Epoch:14, Iter:1374] Loss: 0.546 | Acc: 72.568% \n",
      "[Epoch:14, Iter:1375] Loss: 0.546 | Acc: 72.613% \n",
      "[Epoch:14, Iter:1376] Loss: 0.547 | Acc: 72.579% \n",
      "[Epoch:14, Iter:1377] Loss: 0.546 | Acc: 72.571% \n",
      "[Epoch:14, Iter:1378] Loss: 0.546 | Acc: 72.615% \n",
      "[Epoch:14, Iter:1379] Loss: 0.548 | Acc: 72.430% \n",
      "[Epoch:14, Iter:1380] Loss: 0.548 | Acc: 72.375% \n",
      "[Epoch:14, Iter:1381] Loss: 0.548 | Acc: 72.395% \n",
      "[Epoch:14, Iter:1382] Loss: 0.549 | Acc: 72.415% \n",
      "[Epoch:14, Iter:1383] Loss: 0.549 | Acc: 72.337% \n",
      "[Epoch:14, Iter:1384] Loss: 0.548 | Acc: 72.452% \n",
      "[Epoch:14, Iter:1385] Loss: 0.547 | Acc: 72.471% \n",
      "[Epoch:14, Iter:1386] Loss: 0.547 | Acc: 72.419% \n",
      "[Epoch:14, Iter:1387] Loss: 0.548 | Acc: 72.483% \n",
      "[Epoch:14, Iter:1388] Loss: 0.548 | Acc: 72.523% \n",
      "[Epoch:14, Iter:1389] Loss: 0.548 | Acc: 72.494% \n",
      "[Epoch:14, Iter:1390] Loss: 0.548 | Acc: 72.400% \n",
      "[Epoch:14, Iter:1391] Loss: 0.547 | Acc: 72.505% \n",
      "[Epoch:14, Iter:1392] Loss: 0.548 | Acc: 72.522% \n",
      "[Epoch:14, Iter:1393] Loss: 0.547 | Acc: 72.516% \n",
      "[Epoch:14, Iter:1394] Loss: 0.547 | Acc: 72.511% \n",
      "[Epoch:14, Iter:1395] Loss: 0.547 | Acc: 72.505% \n",
      "[Epoch:14, Iter:1396] Loss: 0.547 | Acc: 72.542% \n",
      "[Epoch:14, Iter:1397] Loss: 0.546 | Acc: 72.515% \n",
      "[Epoch:14, Iter:1398] Loss: 0.545 | Acc: 72.551% \n",
      "[Epoch:14, Iter:1399] Loss: 0.545 | Acc: 72.485% \n",
      "[Epoch:14, Iter:1400] Loss: 0.545 | Acc: 72.520% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.600%\n",
      "Training set's accuracy (after quantization) is: 72.940%\n",
      "Test set's accuracy (before quantization) is: 72.800%\n",
      "Test set's accuracy (after quantization) is: 73.300%\n",
      "Train Loss: 0.545 | Train Acc: 72.600% | Test Loss: 0.549 | Test Acc: 72.800% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.940% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.300% \n",
      "\n",
      "Epoch: 15\n",
      "[Epoch:15, Iter:1401] Loss: 0.535 | Acc: 70.000% \n",
      "[Epoch:15, Iter:1402] Loss: 0.549 | Acc: 71.000% \n",
      "[Epoch:15, Iter:1403] Loss: 0.554 | Acc: 68.667% \n",
      "[Epoch:15, Iter:1404] Loss: 0.546 | Acc: 69.000% \n",
      "[Epoch:15, Iter:1405] Loss: 0.550 | Acc: 69.600% \n",
      "[Epoch:15, Iter:1406] Loss: 0.553 | Acc: 69.667% \n",
      "[Epoch:15, Iter:1407] Loss: 0.547 | Acc: 70.571% \n",
      "[Epoch:15, Iter:1408] Loss: 0.541 | Acc: 71.500% \n",
      "[Epoch:15, Iter:1409] Loss: 0.544 | Acc: 70.667% \n",
      "[Epoch:15, Iter:1410] Loss: 0.541 | Acc: 71.000% \n",
      "[Epoch:15, Iter:1411] Loss: 0.534 | Acc: 72.182% \n",
      "[Epoch:15, Iter:1412] Loss: 0.531 | Acc: 72.333% \n",
      "[Epoch:15, Iter:1413] Loss: 0.536 | Acc: 71.846% \n",
      "[Epoch:15, Iter:1414] Loss: 0.541 | Acc: 71.429% \n",
      "[Epoch:15, Iter:1415] Loss: 0.539 | Acc: 71.333% \n",
      "[Epoch:15, Iter:1416] Loss: 0.537 | Acc: 71.750% \n",
      "[Epoch:15, Iter:1417] Loss: 0.534 | Acc: 72.000% \n",
      "[Epoch:15, Iter:1418] Loss: 0.533 | Acc: 71.778% \n",
      "[Epoch:15, Iter:1419] Loss: 0.529 | Acc: 71.895% \n",
      "[Epoch:15, Iter:1420] Loss: 0.534 | Acc: 71.600% \n",
      "[Epoch:15, Iter:1421] Loss: 0.535 | Acc: 71.905% \n",
      "[Epoch:15, Iter:1422] Loss: 0.535 | Acc: 71.909% \n",
      "[Epoch:15, Iter:1423] Loss: 0.539 | Acc: 71.478% \n",
      "[Epoch:15, Iter:1424] Loss: 0.541 | Acc: 71.750% \n",
      "[Epoch:15, Iter:1425] Loss: 0.544 | Acc: 71.440% \n",
      "[Epoch:15, Iter:1426] Loss: 0.549 | Acc: 71.077% \n",
      "[Epoch:15, Iter:1427] Loss: 0.548 | Acc: 70.963% \n",
      "[Epoch:15, Iter:1428] Loss: 0.549 | Acc: 71.214% \n",
      "[Epoch:15, Iter:1429] Loss: 0.547 | Acc: 71.448% \n",
      "[Epoch:15, Iter:1430] Loss: 0.546 | Acc: 71.600% \n",
      "[Epoch:15, Iter:1431] Loss: 0.546 | Acc: 71.613% \n",
      "[Epoch:15, Iter:1432] Loss: 0.544 | Acc: 71.750% \n",
      "[Epoch:15, Iter:1433] Loss: 0.546 | Acc: 71.636% \n",
      "[Epoch:15, Iter:1434] Loss: 0.545 | Acc: 71.706% \n",
      "[Epoch:15, Iter:1435] Loss: 0.546 | Acc: 71.714% \n",
      "[Epoch:15, Iter:1436] Loss: 0.546 | Acc: 71.722% \n",
      "[Epoch:15, Iter:1437] Loss: 0.546 | Acc: 71.730% \n",
      "[Epoch:15, Iter:1438] Loss: 0.541 | Acc: 72.158% \n",
      "[Epoch:15, Iter:1439] Loss: 0.541 | Acc: 72.154% \n",
      "[Epoch:15, Iter:1440] Loss: 0.543 | Acc: 72.000% \n",
      "[Epoch:15, Iter:1441] Loss: 0.542 | Acc: 72.049% \n",
      "[Epoch:15, Iter:1442] Loss: 0.543 | Acc: 72.000% \n",
      "[Epoch:15, Iter:1443] Loss: 0.543 | Acc: 72.000% \n",
      "[Epoch:15, Iter:1444] Loss: 0.540 | Acc: 72.273% \n",
      "[Epoch:15, Iter:1445] Loss: 0.544 | Acc: 72.000% \n",
      "[Epoch:15, Iter:1446] Loss: 0.544 | Acc: 72.043% \n",
      "[Epoch:15, Iter:1447] Loss: 0.545 | Acc: 72.085% \n",
      "[Epoch:15, Iter:1448] Loss: 0.545 | Acc: 72.167% \n",
      "[Epoch:15, Iter:1449] Loss: 0.544 | Acc: 72.245% \n",
      "[Epoch:15, Iter:1450] Loss: 0.541 | Acc: 72.440% \n",
      "[Epoch:15, Iter:1451] Loss: 0.542 | Acc: 72.353% \n",
      "[Epoch:15, Iter:1452] Loss: 0.542 | Acc: 72.269% \n",
      "[Epoch:15, Iter:1453] Loss: 0.543 | Acc: 72.226% \n",
      "[Epoch:15, Iter:1454] Loss: 0.542 | Acc: 72.185% \n",
      "[Epoch:15, Iter:1455] Loss: 0.541 | Acc: 72.364% \n",
      "[Epoch:15, Iter:1456] Loss: 0.541 | Acc: 72.357% \n",
      "[Epoch:15, Iter:1457] Loss: 0.540 | Acc: 72.561% \n",
      "[Epoch:15, Iter:1458] Loss: 0.539 | Acc: 72.724% \n",
      "[Epoch:15, Iter:1459] Loss: 0.539 | Acc: 72.610% \n",
      "[Epoch:15, Iter:1460] Loss: 0.540 | Acc: 72.567% \n",
      "[Epoch:15, Iter:1461] Loss: 0.542 | Acc: 72.557% \n",
      "[Epoch:15, Iter:1462] Loss: 0.542 | Acc: 72.645% \n",
      "[Epoch:15, Iter:1463] Loss: 0.544 | Acc: 72.730% \n",
      "[Epoch:15, Iter:1464] Loss: 0.545 | Acc: 72.625% \n",
      "[Epoch:15, Iter:1465] Loss: 0.545 | Acc: 72.615% \n",
      "[Epoch:15, Iter:1466] Loss: 0.546 | Acc: 72.636% \n",
      "[Epoch:15, Iter:1467] Loss: 0.545 | Acc: 72.746% \n",
      "[Epoch:15, Iter:1468] Loss: 0.546 | Acc: 72.618% \n",
      "[Epoch:15, Iter:1469] Loss: 0.545 | Acc: 72.725% \n",
      "[Epoch:15, Iter:1470] Loss: 0.544 | Acc: 72.800% \n",
      "[Epoch:15, Iter:1471] Loss: 0.545 | Acc: 72.704% \n",
      "[Epoch:15, Iter:1472] Loss: 0.545 | Acc: 72.750% \n",
      "[Epoch:15, Iter:1473] Loss: 0.545 | Acc: 72.795% \n",
      "[Epoch:15, Iter:1474] Loss: 0.547 | Acc: 72.703% \n",
      "[Epoch:15, Iter:1475] Loss: 0.548 | Acc: 72.667% \n",
      "[Epoch:15, Iter:1476] Loss: 0.547 | Acc: 72.684% \n",
      "[Epoch:15, Iter:1477] Loss: 0.548 | Acc: 72.545% \n",
      "[Epoch:15, Iter:1478] Loss: 0.547 | Acc: 72.538% \n",
      "[Epoch:15, Iter:1479] Loss: 0.547 | Acc: 72.430% \n",
      "[Epoch:15, Iter:1480] Loss: 0.547 | Acc: 72.300% \n",
      "[Epoch:15, Iter:1481] Loss: 0.548 | Acc: 72.272% \n",
      "[Epoch:15, Iter:1482] Loss: 0.548 | Acc: 72.293% \n",
      "[Epoch:15, Iter:1483] Loss: 0.549 | Acc: 72.265% \n",
      "[Epoch:15, Iter:1484] Loss: 0.548 | Acc: 72.381% \n",
      "[Epoch:15, Iter:1485] Loss: 0.548 | Acc: 72.400% \n",
      "[Epoch:15, Iter:1486] Loss: 0.548 | Acc: 72.372% \n",
      "[Epoch:15, Iter:1487] Loss: 0.548 | Acc: 72.345% \n",
      "[Epoch:15, Iter:1488] Loss: 0.547 | Acc: 72.386% \n",
      "[Epoch:15, Iter:1489] Loss: 0.545 | Acc: 72.517% \n",
      "[Epoch:15, Iter:1490] Loss: 0.544 | Acc: 72.600% \n",
      "[Epoch:15, Iter:1491] Loss: 0.543 | Acc: 72.703% \n",
      "[Epoch:15, Iter:1492] Loss: 0.544 | Acc: 72.630% \n",
      "[Epoch:15, Iter:1493] Loss: 0.544 | Acc: 72.667% \n",
      "[Epoch:15, Iter:1494] Loss: 0.545 | Acc: 72.702% \n",
      "[Epoch:15, Iter:1495] Loss: 0.544 | Acc: 72.695% \n",
      "[Epoch:15, Iter:1496] Loss: 0.544 | Acc: 72.688% \n",
      "[Epoch:15, Iter:1497] Loss: 0.544 | Acc: 72.701% \n",
      "[Epoch:15, Iter:1498] Loss: 0.545 | Acc: 72.673% \n",
      "[Epoch:15, Iter:1499] Loss: 0.546 | Acc: 72.606% \n",
      "[Epoch:15, Iter:1500] Loss: 0.545 | Acc: 72.700% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.900%\n",
      "Training set's accuracy (after quantization) is: 72.760%\n",
      "Test set's accuracy (before quantization) is: 73.100%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.540 | Train Acc: 72.900% | Test Loss: 0.544 | Test Acc: 73.100% \n",
      "Quantized Train Loss: 0.542 | Quantized Train Acc: 72.760% | Quantized Test Loss: 0.545 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 16\n",
      "[Epoch:16, Iter:1501] Loss: 0.713 | Acc: 62.000% \n",
      "[Epoch:16, Iter:1502] Loss: 0.658 | Acc: 65.000% \n",
      "[Epoch:16, Iter:1503] Loss: 0.602 | Acc: 71.333% \n",
      "[Epoch:16, Iter:1504] Loss: 0.554 | Acc: 74.500% \n",
      "[Epoch:16, Iter:1505] Loss: 0.550 | Acc: 74.400% \n",
      "[Epoch:16, Iter:1506] Loss: 0.556 | Acc: 73.333% \n",
      "[Epoch:16, Iter:1507] Loss: 0.545 | Acc: 74.000% \n",
      "[Epoch:16, Iter:1508] Loss: 0.558 | Acc: 73.000% \n",
      "[Epoch:16, Iter:1509] Loss: 0.559 | Acc: 73.111% \n",
      "[Epoch:16, Iter:1510] Loss: 0.562 | Acc: 72.800% \n",
      "[Epoch:16, Iter:1511] Loss: 0.564 | Acc: 72.727% \n",
      "[Epoch:16, Iter:1512] Loss: 0.560 | Acc: 73.167% \n",
      "[Epoch:16, Iter:1513] Loss: 0.565 | Acc: 72.462% \n",
      "[Epoch:16, Iter:1514] Loss: 0.568 | Acc: 71.857% \n",
      "[Epoch:16, Iter:1515] Loss: 0.563 | Acc: 72.133% \n",
      "[Epoch:16, Iter:1516] Loss: 0.562 | Acc: 72.250% \n",
      "[Epoch:16, Iter:1517] Loss: 0.562 | Acc: 72.118% \n",
      "[Epoch:16, Iter:1518] Loss: 0.568 | Acc: 71.556% \n",
      "[Epoch:16, Iter:1519] Loss: 0.568 | Acc: 71.158% \n",
      "[Epoch:16, Iter:1520] Loss: 0.569 | Acc: 70.700% \n",
      "[Epoch:16, Iter:1521] Loss: 0.572 | Acc: 70.381% \n",
      "[Epoch:16, Iter:1522] Loss: 0.572 | Acc: 70.455% \n",
      "[Epoch:16, Iter:1523] Loss: 0.573 | Acc: 70.435% \n",
      "[Epoch:16, Iter:1524] Loss: 0.567 | Acc: 70.750% \n",
      "[Epoch:16, Iter:1525] Loss: 0.565 | Acc: 70.960% \n",
      "[Epoch:16, Iter:1526] Loss: 0.568 | Acc: 70.923% \n",
      "[Epoch:16, Iter:1527] Loss: 0.565 | Acc: 71.111% \n",
      "[Epoch:16, Iter:1528] Loss: 0.567 | Acc: 71.071% \n",
      "[Epoch:16, Iter:1529] Loss: 0.570 | Acc: 70.897% \n",
      "[Epoch:16, Iter:1530] Loss: 0.573 | Acc: 70.733% \n",
      "[Epoch:16, Iter:1531] Loss: 0.570 | Acc: 70.968% \n",
      "[Epoch:16, Iter:1532] Loss: 0.568 | Acc: 71.250% \n",
      "[Epoch:16, Iter:1533] Loss: 0.568 | Acc: 71.273% \n",
      "[Epoch:16, Iter:1534] Loss: 0.571 | Acc: 71.000% \n",
      "[Epoch:16, Iter:1535] Loss: 0.570 | Acc: 71.257% \n",
      "[Epoch:16, Iter:1536] Loss: 0.570 | Acc: 71.167% \n",
      "[Epoch:16, Iter:1537] Loss: 0.566 | Acc: 71.459% \n",
      "[Epoch:16, Iter:1538] Loss: 0.562 | Acc: 71.684% \n",
      "[Epoch:16, Iter:1539] Loss: 0.560 | Acc: 71.897% \n",
      "[Epoch:16, Iter:1540] Loss: 0.560 | Acc: 72.000% \n",
      "[Epoch:16, Iter:1541] Loss: 0.562 | Acc: 71.854% \n",
      "[Epoch:16, Iter:1542] Loss: 0.560 | Acc: 71.857% \n",
      "[Epoch:16, Iter:1543] Loss: 0.559 | Acc: 71.907% \n",
      "[Epoch:16, Iter:1544] Loss: 0.558 | Acc: 71.864% \n",
      "[Epoch:16, Iter:1545] Loss: 0.557 | Acc: 71.911% \n",
      "[Epoch:16, Iter:1546] Loss: 0.556 | Acc: 71.957% \n",
      "[Epoch:16, Iter:1547] Loss: 0.556 | Acc: 71.830% \n",
      "[Epoch:16, Iter:1548] Loss: 0.559 | Acc: 71.583% \n",
      "[Epoch:16, Iter:1549] Loss: 0.560 | Acc: 71.469% \n",
      "[Epoch:16, Iter:1550] Loss: 0.559 | Acc: 71.680% \n",
      "[Epoch:16, Iter:1551] Loss: 0.559 | Acc: 71.608% \n",
      "[Epoch:16, Iter:1552] Loss: 0.557 | Acc: 71.731% \n",
      "[Epoch:16, Iter:1553] Loss: 0.557 | Acc: 71.887% \n",
      "[Epoch:16, Iter:1554] Loss: 0.556 | Acc: 72.000% \n",
      "[Epoch:16, Iter:1555] Loss: 0.555 | Acc: 72.073% \n",
      "[Epoch:16, Iter:1556] Loss: 0.553 | Acc: 72.250% \n",
      "[Epoch:16, Iter:1557] Loss: 0.553 | Acc: 72.246% \n",
      "[Epoch:16, Iter:1558] Loss: 0.555 | Acc: 72.034% \n",
      "[Epoch:16, Iter:1559] Loss: 0.555 | Acc: 72.237% \n",
      "[Epoch:16, Iter:1560] Loss: 0.555 | Acc: 72.200% \n",
      "[Epoch:16, Iter:1561] Loss: 0.554 | Acc: 72.328% \n",
      "[Epoch:16, Iter:1562] Loss: 0.554 | Acc: 72.226% \n",
      "[Epoch:16, Iter:1563] Loss: 0.554 | Acc: 72.254% \n",
      "[Epoch:16, Iter:1564] Loss: 0.554 | Acc: 72.312% \n",
      "[Epoch:16, Iter:1565] Loss: 0.554 | Acc: 72.277% \n",
      "[Epoch:16, Iter:1566] Loss: 0.553 | Acc: 72.273% \n",
      "[Epoch:16, Iter:1567] Loss: 0.553 | Acc: 72.328% \n",
      "[Epoch:16, Iter:1568] Loss: 0.552 | Acc: 72.294% \n",
      "[Epoch:16, Iter:1569] Loss: 0.552 | Acc: 72.377% \n",
      "[Epoch:16, Iter:1570] Loss: 0.550 | Acc: 72.429% \n",
      "[Epoch:16, Iter:1571] Loss: 0.550 | Acc: 72.366% \n",
      "[Epoch:16, Iter:1572] Loss: 0.549 | Acc: 72.528% \n",
      "[Epoch:16, Iter:1573] Loss: 0.548 | Acc: 72.575% \n",
      "[Epoch:16, Iter:1574] Loss: 0.547 | Acc: 72.649% \n",
      "[Epoch:16, Iter:1575] Loss: 0.546 | Acc: 72.693% \n",
      "[Epoch:16, Iter:1576] Loss: 0.545 | Acc: 72.763% \n",
      "[Epoch:16, Iter:1577] Loss: 0.545 | Acc: 72.753% \n",
      "[Epoch:16, Iter:1578] Loss: 0.547 | Acc: 72.692% \n",
      "[Epoch:16, Iter:1579] Loss: 0.547 | Acc: 72.759% \n",
      "[Epoch:16, Iter:1580] Loss: 0.547 | Acc: 72.675% \n",
      "[Epoch:16, Iter:1581] Loss: 0.546 | Acc: 72.667% \n",
      "[Epoch:16, Iter:1582] Loss: 0.544 | Acc: 72.780% \n",
      "[Epoch:16, Iter:1583] Loss: 0.545 | Acc: 72.651% \n",
      "[Epoch:16, Iter:1584] Loss: 0.546 | Acc: 72.571% \n",
      "[Epoch:16, Iter:1585] Loss: 0.546 | Acc: 72.541% \n",
      "[Epoch:16, Iter:1586] Loss: 0.545 | Acc: 72.721% \n",
      "[Epoch:16, Iter:1587] Loss: 0.544 | Acc: 72.759% \n",
      "[Epoch:16, Iter:1588] Loss: 0.544 | Acc: 72.705% \n",
      "[Epoch:16, Iter:1589] Loss: 0.543 | Acc: 72.787% \n",
      "[Epoch:16, Iter:1590] Loss: 0.544 | Acc: 72.778% \n",
      "[Epoch:16, Iter:1591] Loss: 0.544 | Acc: 72.813% \n",
      "[Epoch:16, Iter:1592] Loss: 0.544 | Acc: 72.783% \n",
      "[Epoch:16, Iter:1593] Loss: 0.544 | Acc: 72.817% \n",
      "[Epoch:16, Iter:1594] Loss: 0.544 | Acc: 72.787% \n",
      "[Epoch:16, Iter:1595] Loss: 0.544 | Acc: 72.779% \n",
      "[Epoch:16, Iter:1596] Loss: 0.544 | Acc: 72.771% \n",
      "[Epoch:16, Iter:1597] Loss: 0.544 | Acc: 72.763% \n",
      "[Epoch:16, Iter:1598] Loss: 0.545 | Acc: 72.653% \n",
      "[Epoch:16, Iter:1599] Loss: 0.545 | Acc: 72.586% \n",
      "[Epoch:16, Iter:1600] Loss: 0.545 | Acc: 72.580% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.820%\n",
      "Training set's accuracy (after quantization) is: 72.760%\n",
      "Test set's accuracy (before quantization) is: 72.900%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.541 | Train Acc: 72.820% | Test Loss: 0.543 | Test Acc: 72.900% \n",
      "Quantized Train Loss: 0.542 | Quantized Train Acc: 72.760% | Quantized Test Loss: 0.545 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 17\n",
      "[Epoch:17, Iter:1601] Loss: 0.474 | Acc: 78.000% \n",
      "[Epoch:17, Iter:1602] Loss: 0.567 | Acc: 74.000% \n",
      "[Epoch:17, Iter:1603] Loss: 0.606 | Acc: 72.000% \n",
      "[Epoch:17, Iter:1604] Loss: 0.604 | Acc: 70.500% \n",
      "[Epoch:17, Iter:1605] Loss: 0.626 | Acc: 68.000% \n",
      "[Epoch:17, Iter:1606] Loss: 0.609 | Acc: 68.667% \n",
      "[Epoch:17, Iter:1607] Loss: 0.601 | Acc: 69.714% \n",
      "[Epoch:17, Iter:1608] Loss: 0.604 | Acc: 68.250% \n",
      "[Epoch:17, Iter:1609] Loss: 0.597 | Acc: 69.111% \n",
      "[Epoch:17, Iter:1610] Loss: 0.581 | Acc: 70.400% \n",
      "[Epoch:17, Iter:1611] Loss: 0.578 | Acc: 70.364% \n",
      "[Epoch:17, Iter:1612] Loss: 0.571 | Acc: 71.000% \n",
      "[Epoch:17, Iter:1613] Loss: 0.575 | Acc: 70.615% \n",
      "[Epoch:17, Iter:1614] Loss: 0.561 | Acc: 71.571% \n",
      "[Epoch:17, Iter:1615] Loss: 0.574 | Acc: 70.533% \n",
      "[Epoch:17, Iter:1616] Loss: 0.576 | Acc: 70.250% \n",
      "[Epoch:17, Iter:1617] Loss: 0.565 | Acc: 70.941% \n",
      "[Epoch:17, Iter:1618] Loss: 0.573 | Acc: 70.444% \n",
      "[Epoch:17, Iter:1619] Loss: 0.572 | Acc: 70.105% \n",
      "[Epoch:17, Iter:1620] Loss: 0.570 | Acc: 70.200% \n",
      "[Epoch:17, Iter:1621] Loss: 0.569 | Acc: 70.286% \n",
      "[Epoch:17, Iter:1622] Loss: 0.566 | Acc: 70.636% \n",
      "[Epoch:17, Iter:1623] Loss: 0.566 | Acc: 70.609% \n",
      "[Epoch:17, Iter:1624] Loss: 0.564 | Acc: 70.833% \n",
      "[Epoch:17, Iter:1625] Loss: 0.569 | Acc: 70.640% \n",
      "[Epoch:17, Iter:1626] Loss: 0.564 | Acc: 71.077% \n",
      "[Epoch:17, Iter:1627] Loss: 0.561 | Acc: 71.259% \n",
      "[Epoch:17, Iter:1628] Loss: 0.563 | Acc: 71.071% \n",
      "[Epoch:17, Iter:1629] Loss: 0.558 | Acc: 71.586% \n",
      "[Epoch:17, Iter:1630] Loss: 0.557 | Acc: 71.733% \n",
      "[Epoch:17, Iter:1631] Loss: 0.556 | Acc: 71.806% \n",
      "[Epoch:17, Iter:1632] Loss: 0.562 | Acc: 71.438% \n",
      "[Epoch:17, Iter:1633] Loss: 0.559 | Acc: 71.758% \n",
      "[Epoch:17, Iter:1634] Loss: 0.558 | Acc: 71.706% \n",
      "[Epoch:17, Iter:1635] Loss: 0.558 | Acc: 71.714% \n",
      "[Epoch:17, Iter:1636] Loss: 0.556 | Acc: 71.889% \n",
      "[Epoch:17, Iter:1637] Loss: 0.556 | Acc: 71.892% \n",
      "[Epoch:17, Iter:1638] Loss: 0.555 | Acc: 71.895% \n",
      "[Epoch:17, Iter:1639] Loss: 0.555 | Acc: 71.692% \n",
      "[Epoch:17, Iter:1640] Loss: 0.553 | Acc: 71.850% \n",
      "[Epoch:17, Iter:1641] Loss: 0.555 | Acc: 71.561% \n",
      "[Epoch:17, Iter:1642] Loss: 0.554 | Acc: 71.476% \n",
      "[Epoch:17, Iter:1643] Loss: 0.553 | Acc: 71.581% \n",
      "[Epoch:17, Iter:1644] Loss: 0.555 | Acc: 71.455% \n",
      "[Epoch:17, Iter:1645] Loss: 0.554 | Acc: 71.467% \n",
      "[Epoch:17, Iter:1646] Loss: 0.554 | Acc: 71.478% \n",
      "[Epoch:17, Iter:1647] Loss: 0.553 | Acc: 71.617% \n",
      "[Epoch:17, Iter:1648] Loss: 0.552 | Acc: 71.708% \n",
      "[Epoch:17, Iter:1649] Loss: 0.551 | Acc: 71.673% \n",
      "[Epoch:17, Iter:1650] Loss: 0.553 | Acc: 71.520% \n",
      "[Epoch:17, Iter:1651] Loss: 0.552 | Acc: 71.569% \n",
      "[Epoch:17, Iter:1652] Loss: 0.552 | Acc: 71.615% \n",
      "[Epoch:17, Iter:1653] Loss: 0.550 | Acc: 71.811% \n",
      "[Epoch:17, Iter:1654] Loss: 0.551 | Acc: 71.630% \n",
      "[Epoch:17, Iter:1655] Loss: 0.550 | Acc: 71.745% \n",
      "[Epoch:17, Iter:1656] Loss: 0.550 | Acc: 71.714% \n",
      "[Epoch:17, Iter:1657] Loss: 0.550 | Acc: 71.684% \n",
      "[Epoch:17, Iter:1658] Loss: 0.551 | Acc: 71.621% \n",
      "[Epoch:17, Iter:1659] Loss: 0.551 | Acc: 71.729% \n",
      "[Epoch:17, Iter:1660] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:17, Iter:1661] Loss: 0.551 | Acc: 71.934% \n",
      "[Epoch:17, Iter:1662] Loss: 0.552 | Acc: 71.839% \n",
      "[Epoch:17, Iter:1663] Loss: 0.551 | Acc: 71.873% \n",
      "[Epoch:17, Iter:1664] Loss: 0.550 | Acc: 71.969% \n",
      "[Epoch:17, Iter:1665] Loss: 0.550 | Acc: 71.969% \n",
      "[Epoch:17, Iter:1666] Loss: 0.551 | Acc: 71.970% \n",
      "[Epoch:17, Iter:1667] Loss: 0.549 | Acc: 72.179% \n",
      "[Epoch:17, Iter:1668] Loss: 0.548 | Acc: 72.235% \n",
      "[Epoch:17, Iter:1669] Loss: 0.548 | Acc: 72.203% \n",
      "[Epoch:17, Iter:1670] Loss: 0.548 | Acc: 72.257% \n",
      "[Epoch:17, Iter:1671] Loss: 0.548 | Acc: 72.225% \n",
      "[Epoch:17, Iter:1672] Loss: 0.549 | Acc: 72.111% \n",
      "[Epoch:17, Iter:1673] Loss: 0.548 | Acc: 72.110% \n",
      "[Epoch:17, Iter:1674] Loss: 0.550 | Acc: 72.108% \n",
      "[Epoch:17, Iter:1675] Loss: 0.549 | Acc: 72.240% \n",
      "[Epoch:17, Iter:1676] Loss: 0.548 | Acc: 72.289% \n",
      "[Epoch:17, Iter:1677] Loss: 0.547 | Acc: 72.312% \n",
      "[Epoch:17, Iter:1678] Loss: 0.546 | Acc: 72.385% \n",
      "[Epoch:17, Iter:1679] Loss: 0.546 | Acc: 72.456% \n",
      "[Epoch:17, Iter:1680] Loss: 0.546 | Acc: 72.425% \n",
      "[Epoch:17, Iter:1681] Loss: 0.545 | Acc: 72.444% \n",
      "[Epoch:17, Iter:1682] Loss: 0.547 | Acc: 72.317% \n",
      "[Epoch:17, Iter:1683] Loss: 0.546 | Acc: 72.410% \n",
      "[Epoch:17, Iter:1684] Loss: 0.546 | Acc: 72.452% \n",
      "[Epoch:17, Iter:1685] Loss: 0.545 | Acc: 72.471% \n",
      "[Epoch:17, Iter:1686] Loss: 0.547 | Acc: 72.372% \n",
      "[Epoch:17, Iter:1687] Loss: 0.548 | Acc: 72.368% \n",
      "[Epoch:17, Iter:1688] Loss: 0.548 | Acc: 72.341% \n",
      "[Epoch:17, Iter:1689] Loss: 0.547 | Acc: 72.382% \n",
      "[Epoch:17, Iter:1690] Loss: 0.548 | Acc: 72.289% \n",
      "[Epoch:17, Iter:1691] Loss: 0.547 | Acc: 72.286% \n",
      "[Epoch:17, Iter:1692] Loss: 0.547 | Acc: 72.283% \n",
      "[Epoch:17, Iter:1693] Loss: 0.547 | Acc: 72.323% \n",
      "[Epoch:17, Iter:1694] Loss: 0.545 | Acc: 72.404% \n",
      "[Epoch:17, Iter:1695] Loss: 0.546 | Acc: 72.400% \n",
      "[Epoch:17, Iter:1696] Loss: 0.544 | Acc: 72.500% \n",
      "[Epoch:17, Iter:1697] Loss: 0.544 | Acc: 72.495% \n",
      "[Epoch:17, Iter:1698] Loss: 0.545 | Acc: 72.408% \n",
      "[Epoch:17, Iter:1699] Loss: 0.546 | Acc: 72.364% \n",
      "[Epoch:17, Iter:1700] Loss: 0.545 | Acc: 72.340% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.660%\n",
      "Training set's accuracy (after quantization) is: 72.700%\n",
      "Test set's accuracy (before quantization) is: 72.700%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.540 | Train Acc: 72.660% | Test Loss: 0.543 | Test Acc: 72.700% \n",
      "Quantized Train Loss: 0.542 | Quantized Train Acc: 72.700% | Quantized Test Loss: 0.545 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 18\n",
      "[Epoch:18, Iter:1701] Loss: 0.562 | Acc: 66.000% \n",
      "[Epoch:18, Iter:1702] Loss: 0.586 | Acc: 69.000% \n",
      "[Epoch:18, Iter:1703] Loss: 0.593 | Acc: 68.000% \n",
      "[Epoch:18, Iter:1704] Loss: 0.585 | Acc: 69.500% \n",
      "[Epoch:18, Iter:1705] Loss: 0.572 | Acc: 70.400% \n",
      "[Epoch:18, Iter:1706] Loss: 0.558 | Acc: 71.333% \n",
      "[Epoch:18, Iter:1707] Loss: 0.556 | Acc: 71.143% \n",
      "[Epoch:18, Iter:1708] Loss: 0.556 | Acc: 71.250% \n",
      "[Epoch:18, Iter:1709] Loss: 0.568 | Acc: 70.444% \n",
      "[Epoch:18, Iter:1710] Loss: 0.570 | Acc: 70.600% \n",
      "[Epoch:18, Iter:1711] Loss: 0.572 | Acc: 71.091% \n",
      "[Epoch:18, Iter:1712] Loss: 0.572 | Acc: 71.167% \n",
      "[Epoch:18, Iter:1713] Loss: 0.564 | Acc: 71.846% \n",
      "[Epoch:18, Iter:1714] Loss: 0.554 | Acc: 72.857% \n",
      "[Epoch:18, Iter:1715] Loss: 0.557 | Acc: 72.667% \n",
      "[Epoch:18, Iter:1716] Loss: 0.560 | Acc: 72.625% \n",
      "[Epoch:18, Iter:1717] Loss: 0.558 | Acc: 73.059% \n",
      "[Epoch:18, Iter:1718] Loss: 0.560 | Acc: 72.778% \n",
      "[Epoch:18, Iter:1719] Loss: 0.559 | Acc: 72.947% \n",
      "[Epoch:18, Iter:1720] Loss: 0.559 | Acc: 72.600% \n",
      "[Epoch:18, Iter:1721] Loss: 0.556 | Acc: 72.857% \n",
      "[Epoch:18, Iter:1722] Loss: 0.552 | Acc: 72.727% \n",
      "[Epoch:18, Iter:1723] Loss: 0.549 | Acc: 73.043% \n",
      "[Epoch:18, Iter:1724] Loss: 0.546 | Acc: 73.167% \n",
      "[Epoch:18, Iter:1725] Loss: 0.545 | Acc: 73.360% \n",
      "[Epoch:18, Iter:1726] Loss: 0.553 | Acc: 73.077% \n",
      "[Epoch:18, Iter:1727] Loss: 0.553 | Acc: 73.037% \n",
      "[Epoch:18, Iter:1728] Loss: 0.553 | Acc: 72.786% \n",
      "[Epoch:18, Iter:1729] Loss: 0.553 | Acc: 72.690% \n",
      "[Epoch:18, Iter:1730] Loss: 0.554 | Acc: 72.667% \n",
      "[Epoch:18, Iter:1731] Loss: 0.551 | Acc: 72.968% \n",
      "[Epoch:18, Iter:1732] Loss: 0.552 | Acc: 72.875% \n",
      "[Epoch:18, Iter:1733] Loss: 0.555 | Acc: 72.606% \n",
      "[Epoch:18, Iter:1734] Loss: 0.553 | Acc: 72.824% \n",
      "[Epoch:18, Iter:1735] Loss: 0.550 | Acc: 73.086% \n",
      "[Epoch:18, Iter:1736] Loss: 0.546 | Acc: 73.389% \n",
      "[Epoch:18, Iter:1737] Loss: 0.548 | Acc: 73.405% \n",
      "[Epoch:18, Iter:1738] Loss: 0.546 | Acc: 73.474% \n",
      "[Epoch:18, Iter:1739] Loss: 0.543 | Acc: 73.692% \n",
      "[Epoch:18, Iter:1740] Loss: 0.546 | Acc: 73.450% \n",
      "[Epoch:18, Iter:1741] Loss: 0.546 | Acc: 73.415% \n",
      "[Epoch:18, Iter:1742] Loss: 0.546 | Acc: 73.381% \n",
      "[Epoch:18, Iter:1743] Loss: 0.547 | Acc: 73.442% \n",
      "[Epoch:18, Iter:1744] Loss: 0.546 | Acc: 73.318% \n",
      "[Epoch:18, Iter:1745] Loss: 0.547 | Acc: 73.156% \n",
      "[Epoch:18, Iter:1746] Loss: 0.547 | Acc: 73.174% \n",
      "[Epoch:18, Iter:1747] Loss: 0.548 | Acc: 73.106% \n",
      "[Epoch:18, Iter:1748] Loss: 0.547 | Acc: 73.208% \n",
      "[Epoch:18, Iter:1749] Loss: 0.547 | Acc: 73.102% \n",
      "[Epoch:18, Iter:1750] Loss: 0.547 | Acc: 73.080% \n",
      "[Epoch:18, Iter:1751] Loss: 0.547 | Acc: 73.137% \n",
      "[Epoch:18, Iter:1752] Loss: 0.546 | Acc: 73.192% \n",
      "[Epoch:18, Iter:1753] Loss: 0.546 | Acc: 73.170% \n",
      "[Epoch:18, Iter:1754] Loss: 0.546 | Acc: 73.259% \n",
      "[Epoch:18, Iter:1755] Loss: 0.547 | Acc: 73.055% \n",
      "[Epoch:18, Iter:1756] Loss: 0.547 | Acc: 73.071% \n",
      "[Epoch:18, Iter:1757] Loss: 0.547 | Acc: 73.018% \n",
      "[Epoch:18, Iter:1758] Loss: 0.548 | Acc: 72.931% \n",
      "[Epoch:18, Iter:1759] Loss: 0.548 | Acc: 72.949% \n",
      "[Epoch:18, Iter:1760] Loss: 0.547 | Acc: 73.033% \n",
      "[Epoch:18, Iter:1761] Loss: 0.549 | Acc: 72.918% \n",
      "[Epoch:18, Iter:1762] Loss: 0.549 | Acc: 72.806% \n",
      "[Epoch:18, Iter:1763] Loss: 0.549 | Acc: 72.921% \n",
      "[Epoch:18, Iter:1764] Loss: 0.550 | Acc: 72.938% \n",
      "[Epoch:18, Iter:1765] Loss: 0.550 | Acc: 72.923% \n",
      "[Epoch:18, Iter:1766] Loss: 0.549 | Acc: 72.970% \n",
      "[Epoch:18, Iter:1767] Loss: 0.549 | Acc: 73.015% \n",
      "[Epoch:18, Iter:1768] Loss: 0.550 | Acc: 72.912% \n",
      "[Epoch:18, Iter:1769] Loss: 0.550 | Acc: 72.870% \n",
      "[Epoch:18, Iter:1770] Loss: 0.550 | Acc: 72.771% \n",
      "[Epoch:18, Iter:1771] Loss: 0.550 | Acc: 72.845% \n",
      "[Epoch:18, Iter:1772] Loss: 0.549 | Acc: 72.889% \n",
      "[Epoch:18, Iter:1773] Loss: 0.548 | Acc: 72.904% \n",
      "[Epoch:18, Iter:1774] Loss: 0.549 | Acc: 72.757% \n",
      "[Epoch:18, Iter:1775] Loss: 0.549 | Acc: 72.773% \n",
      "[Epoch:18, Iter:1776] Loss: 0.548 | Acc: 72.842% \n",
      "[Epoch:18, Iter:1777] Loss: 0.547 | Acc: 72.909% \n",
      "[Epoch:18, Iter:1778] Loss: 0.547 | Acc: 72.974% \n",
      "[Epoch:18, Iter:1779] Loss: 0.546 | Acc: 73.038% \n",
      "[Epoch:18, Iter:1780] Loss: 0.546 | Acc: 73.075% \n",
      "[Epoch:18, Iter:1781] Loss: 0.546 | Acc: 73.086% \n",
      "[Epoch:18, Iter:1782] Loss: 0.545 | Acc: 73.195% \n",
      "[Epoch:18, Iter:1783] Loss: 0.545 | Acc: 73.133% \n",
      "[Epoch:18, Iter:1784] Loss: 0.545 | Acc: 73.119% \n",
      "[Epoch:18, Iter:1785] Loss: 0.544 | Acc: 73.153% \n",
      "[Epoch:18, Iter:1786] Loss: 0.545 | Acc: 73.116% \n",
      "[Epoch:18, Iter:1787] Loss: 0.544 | Acc: 73.149% \n",
      "[Epoch:18, Iter:1788] Loss: 0.545 | Acc: 73.114% \n",
      "[Epoch:18, Iter:1789] Loss: 0.545 | Acc: 73.101% \n",
      "[Epoch:18, Iter:1790] Loss: 0.545 | Acc: 73.111% \n",
      "[Epoch:18, Iter:1791] Loss: 0.546 | Acc: 73.033% \n",
      "[Epoch:18, Iter:1792] Loss: 0.546 | Acc: 73.000% \n",
      "[Epoch:18, Iter:1793] Loss: 0.546 | Acc: 72.968% \n",
      "[Epoch:18, Iter:1794] Loss: 0.546 | Acc: 72.894% \n",
      "[Epoch:18, Iter:1795] Loss: 0.546 | Acc: 72.926% \n",
      "[Epoch:18, Iter:1796] Loss: 0.545 | Acc: 72.979% \n",
      "[Epoch:18, Iter:1797] Loss: 0.545 | Acc: 73.031% \n",
      "[Epoch:18, Iter:1798] Loss: 0.545 | Acc: 72.980% \n",
      "[Epoch:18, Iter:1799] Loss: 0.546 | Acc: 72.949% \n",
      "[Epoch:18, Iter:1800] Loss: 0.546 | Acc: 72.940% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.420%\n",
      "Training set's accuracy (after quantization) is: 72.680%\n",
      "Test set's accuracy (before quantization) is: 72.000%\n",
      "Test set's accuracy (after quantization) is: 72.400%\n",
      "Train Loss: 0.546 | Train Acc: 72.420% | Test Loss: 0.549 | Test Acc: 72.000% \n",
      "Quantized Train Loss: 0.544 | Quantized Train Acc: 72.680% | Quantized Test Loss: 0.546 | Quantized Test Acc: 72.400% \n",
      "\n",
      "Epoch: 19\n",
      "[Epoch:19, Iter:1801] Loss: 0.526 | Acc: 72.000% \n",
      "[Epoch:19, Iter:1802] Loss: 0.560 | Acc: 71.000% \n",
      "[Epoch:19, Iter:1803] Loss: 0.544 | Acc: 73.333% \n",
      "[Epoch:19, Iter:1804] Loss: 0.567 | Acc: 72.000% \n",
      "[Epoch:19, Iter:1805] Loss: 0.563 | Acc: 73.600% \n",
      "[Epoch:19, Iter:1806] Loss: 0.572 | Acc: 72.333% \n",
      "[Epoch:19, Iter:1807] Loss: 0.570 | Acc: 72.286% \n",
      "[Epoch:19, Iter:1808] Loss: 0.565 | Acc: 72.000% \n",
      "[Epoch:19, Iter:1809] Loss: 0.556 | Acc: 72.889% \n",
      "[Epoch:19, Iter:1810] Loss: 0.558 | Acc: 72.400% \n",
      "[Epoch:19, Iter:1811] Loss: 0.563 | Acc: 71.636% \n",
      "[Epoch:19, Iter:1812] Loss: 0.562 | Acc: 72.167% \n",
      "[Epoch:19, Iter:1813] Loss: 0.557 | Acc: 72.000% \n",
      "[Epoch:19, Iter:1814] Loss: 0.562 | Acc: 71.143% \n",
      "[Epoch:19, Iter:1815] Loss: 0.557 | Acc: 71.867% \n",
      "[Epoch:19, Iter:1816] Loss: 0.555 | Acc: 72.250% \n",
      "[Epoch:19, Iter:1817] Loss: 0.550 | Acc: 72.588% \n",
      "[Epoch:19, Iter:1818] Loss: 0.555 | Acc: 72.000% \n",
      "[Epoch:19, Iter:1819] Loss: 0.549 | Acc: 72.842% \n",
      "[Epoch:19, Iter:1820] Loss: 0.550 | Acc: 72.800% \n",
      "[Epoch:19, Iter:1821] Loss: 0.546 | Acc: 73.238% \n",
      "[Epoch:19, Iter:1822] Loss: 0.546 | Acc: 73.182% \n",
      "[Epoch:19, Iter:1823] Loss: 0.543 | Acc: 73.304% \n",
      "[Epoch:19, Iter:1824] Loss: 0.541 | Acc: 73.333% \n",
      "[Epoch:19, Iter:1825] Loss: 0.545 | Acc: 73.120% \n",
      "[Epoch:19, Iter:1826] Loss: 0.549 | Acc: 72.615% \n",
      "[Epoch:19, Iter:1827] Loss: 0.550 | Acc: 72.444% \n",
      "[Epoch:19, Iter:1828] Loss: 0.548 | Acc: 72.429% \n",
      "[Epoch:19, Iter:1829] Loss: 0.549 | Acc: 72.414% \n",
      "[Epoch:19, Iter:1830] Loss: 0.548 | Acc: 72.400% \n",
      "[Epoch:19, Iter:1831] Loss: 0.547 | Acc: 72.516% \n",
      "[Epoch:19, Iter:1832] Loss: 0.547 | Acc: 72.375% \n",
      "[Epoch:19, Iter:1833] Loss: 0.548 | Acc: 72.303% \n",
      "[Epoch:19, Iter:1834] Loss: 0.551 | Acc: 72.235% \n",
      "[Epoch:19, Iter:1835] Loss: 0.548 | Acc: 72.514% \n",
      "[Epoch:19, Iter:1836] Loss: 0.548 | Acc: 72.444% \n",
      "[Epoch:19, Iter:1837] Loss: 0.546 | Acc: 72.649% \n",
      "[Epoch:19, Iter:1838] Loss: 0.545 | Acc: 72.632% \n",
      "[Epoch:19, Iter:1839] Loss: 0.544 | Acc: 72.667% \n",
      "[Epoch:19, Iter:1840] Loss: 0.544 | Acc: 72.650% \n",
      "[Epoch:19, Iter:1841] Loss: 0.543 | Acc: 72.732% \n",
      "[Epoch:19, Iter:1842] Loss: 0.542 | Acc: 72.952% \n",
      "[Epoch:19, Iter:1843] Loss: 0.541 | Acc: 73.116% \n",
      "[Epoch:19, Iter:1844] Loss: 0.544 | Acc: 72.909% \n",
      "[Epoch:19, Iter:1845] Loss: 0.545 | Acc: 72.756% \n",
      "[Epoch:19, Iter:1846] Loss: 0.545 | Acc: 72.739% \n",
      "[Epoch:19, Iter:1847] Loss: 0.544 | Acc: 72.936% \n",
      "[Epoch:19, Iter:1848] Loss: 0.546 | Acc: 72.625% \n",
      "[Epoch:19, Iter:1849] Loss: 0.549 | Acc: 72.449% \n",
      "[Epoch:19, Iter:1850] Loss: 0.550 | Acc: 72.480% \n",
      "[Epoch:19, Iter:1851] Loss: 0.551 | Acc: 72.471% \n",
      "[Epoch:19, Iter:1852] Loss: 0.551 | Acc: 72.500% \n",
      "[Epoch:19, Iter:1853] Loss: 0.550 | Acc: 72.642% \n",
      "[Epoch:19, Iter:1854] Loss: 0.548 | Acc: 72.778% \n",
      "[Epoch:19, Iter:1855] Loss: 0.548 | Acc: 72.836% \n",
      "[Epoch:19, Iter:1856] Loss: 0.549 | Acc: 72.821% \n",
      "[Epoch:19, Iter:1857] Loss: 0.547 | Acc: 72.877% \n",
      "[Epoch:19, Iter:1858] Loss: 0.550 | Acc: 72.655% \n",
      "[Epoch:19, Iter:1859] Loss: 0.550 | Acc: 72.576% \n",
      "[Epoch:19, Iter:1860] Loss: 0.550 | Acc: 72.533% \n",
      "[Epoch:19, Iter:1861] Loss: 0.550 | Acc: 72.590% \n",
      "[Epoch:19, Iter:1862] Loss: 0.548 | Acc: 72.710% \n",
      "[Epoch:19, Iter:1863] Loss: 0.548 | Acc: 72.698% \n",
      "[Epoch:19, Iter:1864] Loss: 0.548 | Acc: 72.750% \n",
      "[Epoch:19, Iter:1865] Loss: 0.548 | Acc: 72.708% \n",
      "[Epoch:19, Iter:1866] Loss: 0.547 | Acc: 72.667% \n",
      "[Epoch:19, Iter:1867] Loss: 0.547 | Acc: 72.687% \n",
      "[Epoch:19, Iter:1868] Loss: 0.547 | Acc: 72.676% \n",
      "[Epoch:19, Iter:1869] Loss: 0.549 | Acc: 72.551% \n",
      "[Epoch:19, Iter:1870] Loss: 0.550 | Acc: 72.457% \n",
      "[Epoch:19, Iter:1871] Loss: 0.549 | Acc: 72.451% \n",
      "[Epoch:19, Iter:1872] Loss: 0.550 | Acc: 72.333% \n",
      "[Epoch:19, Iter:1873] Loss: 0.550 | Acc: 72.301% \n",
      "[Epoch:19, Iter:1874] Loss: 0.549 | Acc: 72.324% \n",
      "[Epoch:19, Iter:1875] Loss: 0.550 | Acc: 72.293% \n",
      "[Epoch:19, Iter:1876] Loss: 0.549 | Acc: 72.289% \n",
      "[Epoch:19, Iter:1877] Loss: 0.549 | Acc: 72.364% \n",
      "[Epoch:19, Iter:1878] Loss: 0.548 | Acc: 72.385% \n",
      "[Epoch:19, Iter:1879] Loss: 0.549 | Acc: 72.329% \n",
      "[Epoch:19, Iter:1880] Loss: 0.548 | Acc: 72.350% \n",
      "[Epoch:19, Iter:1881] Loss: 0.547 | Acc: 72.494% \n",
      "[Epoch:19, Iter:1882] Loss: 0.547 | Acc: 72.512% \n",
      "[Epoch:19, Iter:1883] Loss: 0.546 | Acc: 72.530% \n",
      "[Epoch:19, Iter:1884] Loss: 0.545 | Acc: 72.667% \n",
      "[Epoch:19, Iter:1885] Loss: 0.545 | Acc: 72.753% \n",
      "[Epoch:19, Iter:1886] Loss: 0.545 | Acc: 72.721% \n",
      "[Epoch:19, Iter:1887] Loss: 0.544 | Acc: 72.759% \n",
      "[Epoch:19, Iter:1888] Loss: 0.544 | Acc: 72.682% \n",
      "[Epoch:19, Iter:1889] Loss: 0.545 | Acc: 72.607% \n",
      "[Epoch:19, Iter:1890] Loss: 0.546 | Acc: 72.444% \n",
      "[Epoch:19, Iter:1891] Loss: 0.546 | Acc: 72.418% \n",
      "[Epoch:19, Iter:1892] Loss: 0.547 | Acc: 72.391% \n",
      "[Epoch:19, Iter:1893] Loss: 0.546 | Acc: 72.473% \n",
      "[Epoch:19, Iter:1894] Loss: 0.546 | Acc: 72.426% \n",
      "[Epoch:19, Iter:1895] Loss: 0.546 | Acc: 72.379% \n",
      "[Epoch:19, Iter:1896] Loss: 0.546 | Acc: 72.333% \n",
      "[Epoch:19, Iter:1897] Loss: 0.545 | Acc: 72.392% \n",
      "[Epoch:19, Iter:1898] Loss: 0.545 | Acc: 72.429% \n",
      "[Epoch:19, Iter:1899] Loss: 0.545 | Acc: 72.424% \n",
      "[Epoch:19, Iter:1900] Loss: 0.544 | Acc: 72.480% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.140%\n",
      "Training set's accuracy (after quantization) is: 72.860%\n",
      "Test set's accuracy (before quantization) is: 72.600%\n",
      "Test set's accuracy (after quantization) is: 73.400%\n",
      "Train Loss: 0.552 | Train Acc: 72.140% | Test Loss: 0.555 | Test Acc: 72.600% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.860% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.400% \n",
      "\n",
      "Epoch: 20\n",
      "[Epoch:20, Iter:1901] Loss: 0.624 | Acc: 66.000% \n",
      "[Epoch:20, Iter:1902] Loss: 0.604 | Acc: 66.000% \n",
      "[Epoch:20, Iter:1903] Loss: 0.559 | Acc: 70.667% \n",
      "[Epoch:20, Iter:1904] Loss: 0.564 | Acc: 71.500% \n",
      "[Epoch:20, Iter:1905] Loss: 0.577 | Acc: 70.000% \n",
      "[Epoch:20, Iter:1906] Loss: 0.584 | Acc: 68.667% \n",
      "[Epoch:20, Iter:1907] Loss: 0.591 | Acc: 67.714% \n",
      "[Epoch:20, Iter:1908] Loss: 0.595 | Acc: 68.000% \n",
      "[Epoch:20, Iter:1909] Loss: 0.601 | Acc: 67.333% \n",
      "[Epoch:20, Iter:1910] Loss: 0.591 | Acc: 68.000% \n",
      "[Epoch:20, Iter:1911] Loss: 0.586 | Acc: 68.909% \n",
      "[Epoch:20, Iter:1912] Loss: 0.582 | Acc: 69.500% \n",
      "[Epoch:20, Iter:1913] Loss: 0.581 | Acc: 69.385% \n",
      "[Epoch:20, Iter:1914] Loss: 0.576 | Acc: 69.571% \n",
      "[Epoch:20, Iter:1915] Loss: 0.568 | Acc: 70.133% \n",
      "[Epoch:20, Iter:1916] Loss: 0.557 | Acc: 70.750% \n",
      "[Epoch:20, Iter:1917] Loss: 0.559 | Acc: 70.706% \n",
      "[Epoch:20, Iter:1918] Loss: 0.556 | Acc: 70.556% \n",
      "[Epoch:20, Iter:1919] Loss: 0.550 | Acc: 70.842% \n",
      "[Epoch:20, Iter:1920] Loss: 0.549 | Acc: 71.000% \n",
      "[Epoch:20, Iter:1921] Loss: 0.547 | Acc: 71.333% \n",
      "[Epoch:20, Iter:1922] Loss: 0.549 | Acc: 71.273% \n",
      "[Epoch:20, Iter:1923] Loss: 0.552 | Acc: 71.043% \n",
      "[Epoch:20, Iter:1924] Loss: 0.549 | Acc: 71.333% \n",
      "[Epoch:20, Iter:1925] Loss: 0.546 | Acc: 71.440% \n",
      "[Epoch:20, Iter:1926] Loss: 0.550 | Acc: 71.231% \n",
      "[Epoch:20, Iter:1927] Loss: 0.550 | Acc: 71.259% \n",
      "[Epoch:20, Iter:1928] Loss: 0.550 | Acc: 71.429% \n",
      "[Epoch:20, Iter:1929] Loss: 0.554 | Acc: 70.966% \n",
      "[Epoch:20, Iter:1930] Loss: 0.551 | Acc: 71.333% \n",
      "[Epoch:20, Iter:1931] Loss: 0.554 | Acc: 71.161% \n",
      "[Epoch:20, Iter:1932] Loss: 0.555 | Acc: 71.062% \n",
      "[Epoch:20, Iter:1933] Loss: 0.557 | Acc: 71.030% \n",
      "[Epoch:20, Iter:1934] Loss: 0.556 | Acc: 71.176% \n",
      "[Epoch:20, Iter:1935] Loss: 0.553 | Acc: 71.429% \n",
      "[Epoch:20, Iter:1936] Loss: 0.554 | Acc: 71.500% \n",
      "[Epoch:20, Iter:1937] Loss: 0.557 | Acc: 71.405% \n",
      "[Epoch:20, Iter:1938] Loss: 0.557 | Acc: 71.316% \n",
      "[Epoch:20, Iter:1939] Loss: 0.556 | Acc: 71.436% \n",
      "[Epoch:20, Iter:1940] Loss: 0.554 | Acc: 71.600% \n",
      "[Epoch:20, Iter:1941] Loss: 0.552 | Acc: 71.659% \n",
      "[Epoch:20, Iter:1942] Loss: 0.553 | Acc: 71.524% \n",
      "[Epoch:20, Iter:1943] Loss: 0.550 | Acc: 71.814% \n",
      "[Epoch:20, Iter:1944] Loss: 0.546 | Acc: 72.136% \n",
      "[Epoch:20, Iter:1945] Loss: 0.548 | Acc: 71.911% \n",
      "[Epoch:20, Iter:1946] Loss: 0.547 | Acc: 71.826% \n",
      "[Epoch:20, Iter:1947] Loss: 0.548 | Acc: 71.745% \n",
      "[Epoch:20, Iter:1948] Loss: 0.549 | Acc: 71.708% \n",
      "[Epoch:20, Iter:1949] Loss: 0.548 | Acc: 71.837% \n",
      "[Epoch:20, Iter:1950] Loss: 0.546 | Acc: 72.040% \n",
      "[Epoch:20, Iter:1951] Loss: 0.547 | Acc: 71.961% \n",
      "[Epoch:20, Iter:1952] Loss: 0.547 | Acc: 72.038% \n",
      "[Epoch:20, Iter:1953] Loss: 0.547 | Acc: 72.075% \n",
      "[Epoch:20, Iter:1954] Loss: 0.547 | Acc: 72.037% \n",
      "[Epoch:20, Iter:1955] Loss: 0.550 | Acc: 71.673% \n",
      "[Epoch:20, Iter:1956] Loss: 0.552 | Acc: 71.536% \n",
      "[Epoch:20, Iter:1957] Loss: 0.550 | Acc: 71.614% \n",
      "[Epoch:20, Iter:1958] Loss: 0.550 | Acc: 71.655% \n",
      "[Epoch:20, Iter:1959] Loss: 0.550 | Acc: 71.729% \n",
      "[Epoch:20, Iter:1960] Loss: 0.551 | Acc: 71.667% \n",
      "[Epoch:20, Iter:1961] Loss: 0.551 | Acc: 71.705% \n",
      "[Epoch:20, Iter:1962] Loss: 0.550 | Acc: 71.774% \n",
      "[Epoch:20, Iter:1963] Loss: 0.550 | Acc: 71.714% \n",
      "[Epoch:20, Iter:1964] Loss: 0.549 | Acc: 71.812% \n",
      "[Epoch:20, Iter:1965] Loss: 0.549 | Acc: 71.785% \n",
      "[Epoch:20, Iter:1966] Loss: 0.550 | Acc: 71.758% \n",
      "[Epoch:20, Iter:1967] Loss: 0.550 | Acc: 71.821% \n",
      "[Epoch:20, Iter:1968] Loss: 0.549 | Acc: 71.794% \n",
      "[Epoch:20, Iter:1969] Loss: 0.550 | Acc: 71.739% \n",
      "[Epoch:20, Iter:1970] Loss: 0.551 | Acc: 71.771% \n",
      "[Epoch:20, Iter:1971] Loss: 0.550 | Acc: 71.803% \n",
      "[Epoch:20, Iter:1972] Loss: 0.552 | Acc: 71.722% \n",
      "[Epoch:20, Iter:1973] Loss: 0.552 | Acc: 71.671% \n",
      "[Epoch:20, Iter:1974] Loss: 0.552 | Acc: 71.730% \n",
      "[Epoch:20, Iter:1975] Loss: 0.550 | Acc: 71.920% \n",
      "[Epoch:20, Iter:1976] Loss: 0.551 | Acc: 71.763% \n",
      "[Epoch:20, Iter:1977] Loss: 0.552 | Acc: 71.688% \n",
      "[Epoch:20, Iter:1978] Loss: 0.551 | Acc: 71.769% \n",
      "[Epoch:20, Iter:1979] Loss: 0.551 | Acc: 71.747% \n",
      "[Epoch:20, Iter:1980] Loss: 0.550 | Acc: 71.725% \n",
      "[Epoch:20, Iter:1981] Loss: 0.549 | Acc: 71.827% \n",
      "[Epoch:20, Iter:1982] Loss: 0.549 | Acc: 71.805% \n",
      "[Epoch:20, Iter:1983] Loss: 0.550 | Acc: 71.735% \n",
      "[Epoch:20, Iter:1984] Loss: 0.549 | Acc: 71.810% \n",
      "[Epoch:20, Iter:1985] Loss: 0.548 | Acc: 71.859% \n",
      "[Epoch:20, Iter:1986] Loss: 0.549 | Acc: 71.814% \n",
      "[Epoch:20, Iter:1987] Loss: 0.549 | Acc: 71.908% \n",
      "[Epoch:20, Iter:1988] Loss: 0.548 | Acc: 71.864% \n",
      "[Epoch:20, Iter:1989] Loss: 0.548 | Acc: 71.843% \n",
      "[Epoch:20, Iter:1990] Loss: 0.547 | Acc: 71.889% \n",
      "[Epoch:20, Iter:1991] Loss: 0.548 | Acc: 71.912% \n",
      "[Epoch:20, Iter:1992] Loss: 0.547 | Acc: 71.978% \n",
      "[Epoch:20, Iter:1993] Loss: 0.547 | Acc: 71.935% \n",
      "[Epoch:20, Iter:1994] Loss: 0.546 | Acc: 71.957% \n",
      "[Epoch:20, Iter:1995] Loss: 0.546 | Acc: 72.021% \n",
      "[Epoch:20, Iter:1996] Loss: 0.546 | Acc: 72.042% \n",
      "[Epoch:20, Iter:1997] Loss: 0.546 | Acc: 72.041% \n",
      "[Epoch:20, Iter:1998] Loss: 0.546 | Acc: 72.122% \n",
      "[Epoch:20, Iter:1999] Loss: 0.546 | Acc: 72.202% \n",
      "[Epoch:20, Iter:2000] Loss: 0.545 | Acc: 72.300% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.120%\n",
      "Training set's accuracy (after quantization) is: 72.940%\n",
      "Test set's accuracy (before quantization) is: 72.400%\n",
      "Test set's accuracy (after quantization) is: 73.700%\n",
      "Train Loss: 0.552 | Train Acc: 72.120% | Test Loss: 0.555 | Test Acc: 72.400% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.940% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.700% \n",
      "\n",
      "Epoch: 21\n",
      "[Epoch:21, Iter:2001] Loss: 0.527 | Acc: 80.000% \n",
      "[Epoch:21, Iter:2002] Loss: 0.605 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2003] Loss: 0.591 | Acc: 71.333% \n",
      "[Epoch:21, Iter:2004] Loss: 0.568 | Acc: 72.500% \n",
      "[Epoch:21, Iter:2005] Loss: 0.575 | Acc: 72.800% \n",
      "[Epoch:21, Iter:2006] Loss: 0.570 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2007] Loss: 0.562 | Acc: 71.429% \n",
      "[Epoch:21, Iter:2008] Loss: 0.559 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2009] Loss: 0.551 | Acc: 72.222% \n",
      "[Epoch:21, Iter:2010] Loss: 0.559 | Acc: 71.600% \n",
      "[Epoch:21, Iter:2011] Loss: 0.561 | Acc: 71.818% \n",
      "[Epoch:21, Iter:2012] Loss: 0.555 | Acc: 72.500% \n",
      "[Epoch:21, Iter:2013] Loss: 0.552 | Acc: 72.615% \n",
      "[Epoch:21, Iter:2014] Loss: 0.551 | Acc: 72.286% \n",
      "[Epoch:21, Iter:2015] Loss: 0.547 | Acc: 72.667% \n",
      "[Epoch:21, Iter:2016] Loss: 0.541 | Acc: 73.125% \n",
      "[Epoch:21, Iter:2017] Loss: 0.538 | Acc: 73.765% \n",
      "[Epoch:21, Iter:2018] Loss: 0.540 | Acc: 73.556% \n",
      "[Epoch:21, Iter:2019] Loss: 0.546 | Acc: 73.263% \n",
      "[Epoch:21, Iter:2020] Loss: 0.546 | Acc: 73.100% \n",
      "[Epoch:21, Iter:2021] Loss: 0.541 | Acc: 73.524% \n",
      "[Epoch:21, Iter:2022] Loss: 0.540 | Acc: 73.636% \n",
      "[Epoch:21, Iter:2023] Loss: 0.540 | Acc: 73.565% \n",
      "[Epoch:21, Iter:2024] Loss: 0.541 | Acc: 73.417% \n",
      "[Epoch:21, Iter:2025] Loss: 0.543 | Acc: 73.040% \n",
      "[Epoch:21, Iter:2026] Loss: 0.541 | Acc: 73.154% \n",
      "[Epoch:21, Iter:2027] Loss: 0.541 | Acc: 73.333% \n",
      "[Epoch:21, Iter:2028] Loss: 0.540 | Acc: 73.214% \n",
      "[Epoch:21, Iter:2029] Loss: 0.537 | Acc: 73.448% \n",
      "[Epoch:21, Iter:2030] Loss: 0.540 | Acc: 73.067% \n",
      "[Epoch:21, Iter:2031] Loss: 0.541 | Acc: 73.097% \n",
      "[Epoch:21, Iter:2032] Loss: 0.544 | Acc: 73.062% \n",
      "[Epoch:21, Iter:2033] Loss: 0.543 | Acc: 73.152% \n",
      "[Epoch:21, Iter:2034] Loss: 0.539 | Acc: 73.412% \n",
      "[Epoch:21, Iter:2035] Loss: 0.544 | Acc: 73.029% \n",
      "[Epoch:21, Iter:2036] Loss: 0.544 | Acc: 73.111% \n",
      "[Epoch:21, Iter:2037] Loss: 0.546 | Acc: 72.865% \n",
      "[Epoch:21, Iter:2038] Loss: 0.546 | Acc: 72.842% \n",
      "[Epoch:21, Iter:2039] Loss: 0.549 | Acc: 72.462% \n",
      "[Epoch:21, Iter:2040] Loss: 0.550 | Acc: 72.300% \n",
      "[Epoch:21, Iter:2041] Loss: 0.548 | Acc: 72.341% \n",
      "[Epoch:21, Iter:2042] Loss: 0.547 | Acc: 72.286% \n",
      "[Epoch:21, Iter:2043] Loss: 0.549 | Acc: 72.186% \n",
      "[Epoch:21, Iter:2044] Loss: 0.549 | Acc: 72.227% \n",
      "[Epoch:21, Iter:2045] Loss: 0.548 | Acc: 72.400% \n",
      "[Epoch:21, Iter:2046] Loss: 0.547 | Acc: 72.435% \n",
      "[Epoch:21, Iter:2047] Loss: 0.548 | Acc: 72.426% \n",
      "[Epoch:21, Iter:2048] Loss: 0.551 | Acc: 72.042% \n",
      "[Epoch:21, Iter:2049] Loss: 0.551 | Acc: 71.959% \n",
      "[Epoch:21, Iter:2050] Loss: 0.551 | Acc: 71.960% \n",
      "[Epoch:21, Iter:2051] Loss: 0.551 | Acc: 71.804% \n",
      "[Epoch:21, Iter:2052] Loss: 0.550 | Acc: 71.885% \n",
      "[Epoch:21, Iter:2053] Loss: 0.552 | Acc: 71.774% \n",
      "[Epoch:21, Iter:2054] Loss: 0.550 | Acc: 71.815% \n",
      "[Epoch:21, Iter:2055] Loss: 0.551 | Acc: 71.782% \n",
      "[Epoch:21, Iter:2056] Loss: 0.550 | Acc: 71.929% \n",
      "[Epoch:21, Iter:2057] Loss: 0.550 | Acc: 71.930% \n",
      "[Epoch:21, Iter:2058] Loss: 0.548 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2059] Loss: 0.547 | Acc: 72.102% \n",
      "[Epoch:21, Iter:2060] Loss: 0.547 | Acc: 72.167% \n",
      "[Epoch:21, Iter:2061] Loss: 0.547 | Acc: 72.131% \n",
      "[Epoch:21, Iter:2062] Loss: 0.548 | Acc: 72.032% \n",
      "[Epoch:21, Iter:2063] Loss: 0.547 | Acc: 72.095% \n",
      "[Epoch:21, Iter:2064] Loss: 0.546 | Acc: 72.156% \n",
      "[Epoch:21, Iter:2065] Loss: 0.547 | Acc: 72.154% \n",
      "[Epoch:21, Iter:2066] Loss: 0.547 | Acc: 72.061% \n",
      "[Epoch:21, Iter:2067] Loss: 0.548 | Acc: 72.030% \n",
      "[Epoch:21, Iter:2068] Loss: 0.548 | Acc: 72.118% \n",
      "[Epoch:21, Iter:2069] Loss: 0.548 | Acc: 72.087% \n",
      "[Epoch:21, Iter:2070] Loss: 0.548 | Acc: 72.029% \n",
      "[Epoch:21, Iter:2071] Loss: 0.548 | Acc: 72.028% \n",
      "[Epoch:21, Iter:2072] Loss: 0.549 | Acc: 71.944% \n",
      "[Epoch:21, Iter:2073] Loss: 0.548 | Acc: 72.055% \n",
      "[Epoch:21, Iter:2074] Loss: 0.549 | Acc: 71.973% \n",
      "[Epoch:21, Iter:2075] Loss: 0.549 | Acc: 72.053% \n",
      "[Epoch:21, Iter:2076] Loss: 0.549 | Acc: 72.026% \n",
      "[Epoch:21, Iter:2077] Loss: 0.549 | Acc: 72.026% \n",
      "[Epoch:21, Iter:2078] Loss: 0.549 | Acc: 72.051% \n",
      "[Epoch:21, Iter:2079] Loss: 0.550 | Acc: 72.025% \n",
      "[Epoch:21, Iter:2080] Loss: 0.549 | Acc: 72.100% \n",
      "[Epoch:21, Iter:2081] Loss: 0.549 | Acc: 72.049% \n",
      "[Epoch:21, Iter:2082] Loss: 0.551 | Acc: 71.951% \n",
      "[Epoch:21, Iter:2083] Loss: 0.551 | Acc: 71.976% \n",
      "[Epoch:21, Iter:2084] Loss: 0.551 | Acc: 71.952% \n",
      "[Epoch:21, Iter:2085] Loss: 0.550 | Acc: 71.953% \n",
      "[Epoch:21, Iter:2086] Loss: 0.551 | Acc: 71.884% \n",
      "[Epoch:21, Iter:2087] Loss: 0.550 | Acc: 71.931% \n",
      "[Epoch:21, Iter:2088] Loss: 0.551 | Acc: 71.864% \n",
      "[Epoch:21, Iter:2089] Loss: 0.550 | Acc: 71.888% \n",
      "[Epoch:21, Iter:2090] Loss: 0.550 | Acc: 71.978% \n",
      "[Epoch:21, Iter:2091] Loss: 0.550 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2092] Loss: 0.550 | Acc: 71.913% \n",
      "[Epoch:21, Iter:2093] Loss: 0.550 | Acc: 71.871% \n",
      "[Epoch:21, Iter:2094] Loss: 0.549 | Acc: 71.936% \n",
      "[Epoch:21, Iter:2095] Loss: 0.548 | Acc: 72.084% \n",
      "[Epoch:21, Iter:2096] Loss: 0.547 | Acc: 72.042% \n",
      "[Epoch:21, Iter:2097] Loss: 0.547 | Acc: 72.021% \n",
      "[Epoch:21, Iter:2098] Loss: 0.547 | Acc: 72.000% \n",
      "[Epoch:21, Iter:2099] Loss: 0.547 | Acc: 72.040% \n",
      "[Epoch:21, Iter:2100] Loss: 0.547 | Acc: 72.040% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.860%\n",
      "Training set's accuracy (after quantization) is: 73.000%\n",
      "Test set's accuracy (before quantization) is: 72.800%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.543 | Train Acc: 72.860% | Test Loss: 0.546 | Test Acc: 72.800% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 73.000% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 22\n",
      "[Epoch:22, Iter:2101] Loss: 0.513 | Acc: 74.000% \n",
      "[Epoch:22, Iter:2102] Loss: 0.513 | Acc: 73.000% \n",
      "[Epoch:22, Iter:2103] Loss: 0.483 | Acc: 76.667% \n",
      "[Epoch:22, Iter:2104] Loss: 0.493 | Acc: 75.500% \n",
      "[Epoch:22, Iter:2105] Loss: 0.488 | Acc: 75.600% \n",
      "[Epoch:22, Iter:2106] Loss: 0.521 | Acc: 73.333% \n",
      "[Epoch:22, Iter:2107] Loss: 0.539 | Acc: 71.714% \n",
      "[Epoch:22, Iter:2108] Loss: 0.537 | Acc: 72.000% \n",
      "[Epoch:22, Iter:2109] Loss: 0.533 | Acc: 72.222% \n",
      "[Epoch:22, Iter:2110] Loss: 0.547 | Acc: 71.800% \n",
      "[Epoch:22, Iter:2111] Loss: 0.546 | Acc: 72.182% \n",
      "[Epoch:22, Iter:2112] Loss: 0.548 | Acc: 71.833% \n",
      "[Epoch:22, Iter:2113] Loss: 0.540 | Acc: 72.308% \n",
      "[Epoch:22, Iter:2114] Loss: 0.533 | Acc: 73.000% \n",
      "[Epoch:22, Iter:2115] Loss: 0.546 | Acc: 72.133% \n",
      "[Epoch:22, Iter:2116] Loss: 0.542 | Acc: 72.500% \n",
      "[Epoch:22, Iter:2117] Loss: 0.544 | Acc: 72.353% \n",
      "[Epoch:22, Iter:2118] Loss: 0.550 | Acc: 71.889% \n",
      "[Epoch:22, Iter:2119] Loss: 0.556 | Acc: 71.474% \n",
      "[Epoch:22, Iter:2120] Loss: 0.559 | Acc: 71.200% \n",
      "[Epoch:22, Iter:2121] Loss: 0.558 | Acc: 71.238% \n",
      "[Epoch:22, Iter:2122] Loss: 0.562 | Acc: 70.909% \n",
      "[Epoch:22, Iter:2123] Loss: 0.561 | Acc: 70.696% \n",
      "[Epoch:22, Iter:2124] Loss: 0.558 | Acc: 71.000% \n",
      "[Epoch:22, Iter:2125] Loss: 0.558 | Acc: 70.880% \n",
      "[Epoch:22, Iter:2126] Loss: 0.558 | Acc: 70.923% \n",
      "[Epoch:22, Iter:2127] Loss: 0.561 | Acc: 70.889% \n",
      "[Epoch:22, Iter:2128] Loss: 0.559 | Acc: 71.000% \n",
      "[Epoch:22, Iter:2129] Loss: 0.562 | Acc: 70.552% \n",
      "[Epoch:22, Iter:2130] Loss: 0.565 | Acc: 70.333% \n",
      "[Epoch:22, Iter:2131] Loss: 0.566 | Acc: 70.516% \n",
      "[Epoch:22, Iter:2132] Loss: 0.566 | Acc: 70.500% \n",
      "[Epoch:22, Iter:2133] Loss: 0.568 | Acc: 70.485% \n",
      "[Epoch:22, Iter:2134] Loss: 0.569 | Acc: 70.294% \n",
      "[Epoch:22, Iter:2135] Loss: 0.568 | Acc: 70.343% \n",
      "[Epoch:22, Iter:2136] Loss: 0.565 | Acc: 70.500% \n",
      "[Epoch:22, Iter:2137] Loss: 0.565 | Acc: 70.595% \n",
      "[Epoch:22, Iter:2138] Loss: 0.566 | Acc: 70.421% \n",
      "[Epoch:22, Iter:2139] Loss: 0.566 | Acc: 70.410% \n",
      "[Epoch:22, Iter:2140] Loss: 0.563 | Acc: 70.800% \n",
      "[Epoch:22, Iter:2141] Loss: 0.561 | Acc: 70.878% \n",
      "[Epoch:22, Iter:2142] Loss: 0.561 | Acc: 70.952% \n",
      "[Epoch:22, Iter:2143] Loss: 0.559 | Acc: 71.209% \n",
      "[Epoch:22, Iter:2144] Loss: 0.557 | Acc: 71.273% \n",
      "[Epoch:22, Iter:2145] Loss: 0.556 | Acc: 71.422% \n",
      "[Epoch:22, Iter:2146] Loss: 0.559 | Acc: 71.261% \n",
      "[Epoch:22, Iter:2147] Loss: 0.557 | Acc: 71.489% \n",
      "[Epoch:22, Iter:2148] Loss: 0.554 | Acc: 71.750% \n",
      "[Epoch:22, Iter:2149] Loss: 0.557 | Acc: 71.510% \n",
      "[Epoch:22, Iter:2150] Loss: 0.555 | Acc: 71.600% \n",
      "[Epoch:22, Iter:2151] Loss: 0.555 | Acc: 71.686% \n",
      "[Epoch:22, Iter:2152] Loss: 0.556 | Acc: 71.500% \n",
      "[Epoch:22, Iter:2153] Loss: 0.554 | Acc: 71.660% \n",
      "[Epoch:22, Iter:2154] Loss: 0.554 | Acc: 71.556% \n",
      "[Epoch:22, Iter:2155] Loss: 0.554 | Acc: 71.600% \n",
      "[Epoch:22, Iter:2156] Loss: 0.552 | Acc: 71.750% \n",
      "[Epoch:22, Iter:2157] Loss: 0.550 | Acc: 71.895% \n",
      "[Epoch:22, Iter:2158] Loss: 0.550 | Acc: 71.966% \n",
      "[Epoch:22, Iter:2159] Loss: 0.550 | Acc: 72.068% \n",
      "[Epoch:22, Iter:2160] Loss: 0.551 | Acc: 72.000% \n",
      "[Epoch:22, Iter:2161] Loss: 0.552 | Acc: 71.934% \n",
      "[Epoch:22, Iter:2162] Loss: 0.552 | Acc: 72.097% \n",
      "[Epoch:22, Iter:2163] Loss: 0.551 | Acc: 72.095% \n",
      "[Epoch:22, Iter:2164] Loss: 0.551 | Acc: 72.062% \n",
      "[Epoch:22, Iter:2165] Loss: 0.554 | Acc: 71.846% \n",
      "[Epoch:22, Iter:2166] Loss: 0.555 | Acc: 71.667% \n",
      "[Epoch:22, Iter:2167] Loss: 0.556 | Acc: 71.612% \n",
      "[Epoch:22, Iter:2168] Loss: 0.556 | Acc: 71.618% \n",
      "[Epoch:22, Iter:2169] Loss: 0.556 | Acc: 71.594% \n",
      "[Epoch:22, Iter:2170] Loss: 0.556 | Acc: 71.629% \n",
      "[Epoch:22, Iter:2171] Loss: 0.555 | Acc: 71.690% \n",
      "[Epoch:22, Iter:2172] Loss: 0.554 | Acc: 71.722% \n",
      "[Epoch:22, Iter:2173] Loss: 0.553 | Acc: 71.781% \n",
      "[Epoch:22, Iter:2174] Loss: 0.552 | Acc: 71.919% \n",
      "[Epoch:22, Iter:2175] Loss: 0.554 | Acc: 71.840% \n",
      "[Epoch:22, Iter:2176] Loss: 0.554 | Acc: 71.763% \n",
      "[Epoch:22, Iter:2177] Loss: 0.553 | Acc: 71.844% \n",
      "[Epoch:22, Iter:2178] Loss: 0.553 | Acc: 71.897% \n",
      "[Epoch:22, Iter:2179] Loss: 0.552 | Acc: 71.975% \n",
      "[Epoch:22, Iter:2180] Loss: 0.551 | Acc: 72.100% \n",
      "[Epoch:22, Iter:2181] Loss: 0.550 | Acc: 72.123% \n",
      "[Epoch:22, Iter:2182] Loss: 0.549 | Acc: 72.146% \n",
      "[Epoch:22, Iter:2183] Loss: 0.548 | Acc: 72.193% \n",
      "[Epoch:22, Iter:2184] Loss: 0.549 | Acc: 72.286% \n",
      "[Epoch:22, Iter:2185] Loss: 0.548 | Acc: 72.259% \n",
      "[Epoch:22, Iter:2186] Loss: 0.548 | Acc: 72.279% \n",
      "[Epoch:22, Iter:2187] Loss: 0.549 | Acc: 72.184% \n",
      "[Epoch:22, Iter:2188] Loss: 0.549 | Acc: 72.091% \n",
      "[Epoch:22, Iter:2189] Loss: 0.548 | Acc: 72.135% \n",
      "[Epoch:22, Iter:2190] Loss: 0.548 | Acc: 72.133% \n",
      "[Epoch:22, Iter:2191] Loss: 0.546 | Acc: 72.242% \n",
      "[Epoch:22, Iter:2192] Loss: 0.546 | Acc: 72.239% \n",
      "[Epoch:22, Iter:2193] Loss: 0.545 | Acc: 72.387% \n",
      "[Epoch:22, Iter:2194] Loss: 0.545 | Acc: 72.447% \n",
      "[Epoch:22, Iter:2195] Loss: 0.543 | Acc: 72.526% \n",
      "[Epoch:22, Iter:2196] Loss: 0.542 | Acc: 72.583% \n",
      "[Epoch:22, Iter:2197] Loss: 0.542 | Acc: 72.598% \n",
      "[Epoch:22, Iter:2198] Loss: 0.544 | Acc: 72.449% \n",
      "[Epoch:22, Iter:2199] Loss: 0.545 | Acc: 72.364% \n",
      "[Epoch:22, Iter:2200] Loss: 0.544 | Acc: 72.340% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 72.580%\n",
      "Training set's accuracy (after quantization) is: 72.920%\n",
      "Test set's accuracy (before quantization) is: 71.500%\n",
      "Test set's accuracy (after quantization) is: 73.100%\n",
      "Train Loss: 0.545 | Train Acc: 72.580% | Test Loss: 0.549 | Test Acc: 71.500% \n",
      "Quantized Train Loss: 0.543 | Quantized Train Acc: 72.920% | Quantized Test Loss: 0.545 | Quantized Test Acc: 73.100% \n",
      "\n",
      "Epoch: 23\n",
      "[Epoch:23, Iter:2201] Loss: 0.556 | Acc: 76.000% \n",
      "[Epoch:23, Iter:2202] Loss: 0.558 | Acc: 75.000% \n",
      "[Epoch:23, Iter:2203] Loss: 0.551 | Acc: 76.000% \n",
      "[Epoch:23, Iter:2204] Loss: 0.540 | Acc: 75.500% \n",
      "[Epoch:23, Iter:2205] Loss: 0.525 | Acc: 76.800% \n",
      "[Epoch:23, Iter:2206] Loss: 0.520 | Acc: 77.000% \n",
      "[Epoch:23, Iter:2207] Loss: 0.530 | Acc: 76.286% \n",
      "[Epoch:23, Iter:2208] Loss: 0.524 | Acc: 76.000% \n",
      "[Epoch:23, Iter:2209] Loss: 0.533 | Acc: 74.667% \n",
      "[Epoch:23, Iter:2210] Loss: 0.538 | Acc: 74.200% \n",
      "[Epoch:23, Iter:2211] Loss: 0.534 | Acc: 74.364% \n",
      "[Epoch:23, Iter:2212] Loss: 0.530 | Acc: 74.500% \n",
      "[Epoch:23, Iter:2213] Loss: 0.528 | Acc: 74.615% \n",
      "[Epoch:23, Iter:2214] Loss: 0.530 | Acc: 74.286% \n",
      "[Epoch:23, Iter:2215] Loss: 0.526 | Acc: 74.133% \n",
      "[Epoch:23, Iter:2216] Loss: 0.522 | Acc: 74.250% \n",
      "[Epoch:23, Iter:2217] Loss: 0.524 | Acc: 74.118% \n",
      "[Epoch:23, Iter:2218] Loss: 0.529 | Acc: 74.111% \n",
      "[Epoch:23, Iter:2219] Loss: 0.529 | Acc: 74.316% \n",
      "[Epoch:23, Iter:2220] Loss: 0.529 | Acc: 74.100% \n",
      "[Epoch:23, Iter:2221] Loss: 0.528 | Acc: 73.905% \n",
      "[Epoch:23, Iter:2222] Loss: 0.528 | Acc: 73.727% \n",
      "[Epoch:23, Iter:2223] Loss: 0.535 | Acc: 73.217% \n",
      "[Epoch:23, Iter:2224] Loss: 0.538 | Acc: 72.833% \n",
      "[Epoch:23, Iter:2225] Loss: 0.538 | Acc: 72.800% \n",
      "[Epoch:23, Iter:2226] Loss: 0.540 | Acc: 72.538% \n",
      "[Epoch:23, Iter:2227] Loss: 0.538 | Acc: 72.519% \n",
      "[Epoch:23, Iter:2228] Loss: 0.540 | Acc: 72.571% \n",
      "[Epoch:23, Iter:2229] Loss: 0.537 | Acc: 72.828% \n",
      "[Epoch:23, Iter:2230] Loss: 0.533 | Acc: 73.067% \n",
      "[Epoch:23, Iter:2231] Loss: 0.536 | Acc: 72.968% \n",
      "[Epoch:23, Iter:2232] Loss: 0.536 | Acc: 72.938% \n",
      "[Epoch:23, Iter:2233] Loss: 0.539 | Acc: 72.788% \n",
      "[Epoch:23, Iter:2234] Loss: 0.541 | Acc: 72.647% \n",
      "[Epoch:23, Iter:2235] Loss: 0.539 | Acc: 72.686% \n",
      "[Epoch:23, Iter:2236] Loss: 0.542 | Acc: 72.500% \n",
      "[Epoch:23, Iter:2237] Loss: 0.541 | Acc: 72.595% \n",
      "[Epoch:23, Iter:2238] Loss: 0.541 | Acc: 72.579% \n",
      "[Epoch:23, Iter:2239] Loss: 0.540 | Acc: 72.615% \n",
      "[Epoch:23, Iter:2240] Loss: 0.537 | Acc: 72.800% \n",
      "[Epoch:23, Iter:2241] Loss: 0.541 | Acc: 72.585% \n",
      "[Epoch:23, Iter:2242] Loss: 0.539 | Acc: 72.857% \n",
      "[Epoch:23, Iter:2243] Loss: 0.543 | Acc: 72.651% \n",
      "[Epoch:23, Iter:2244] Loss: 0.546 | Acc: 72.455% \n",
      "[Epoch:23, Iter:2245] Loss: 0.546 | Acc: 72.533% \n",
      "[Epoch:23, Iter:2246] Loss: 0.547 | Acc: 72.565% \n",
      "[Epoch:23, Iter:2247] Loss: 0.548 | Acc: 72.468% \n",
      "[Epoch:23, Iter:2248] Loss: 0.549 | Acc: 72.375% \n",
      "[Epoch:23, Iter:2249] Loss: 0.550 | Acc: 72.408% \n",
      "[Epoch:23, Iter:2250] Loss: 0.549 | Acc: 72.400% \n",
      "[Epoch:23, Iter:2251] Loss: 0.550 | Acc: 72.314% \n",
      "[Epoch:23, Iter:2252] Loss: 0.550 | Acc: 72.385% \n",
      "[Epoch:23, Iter:2253] Loss: 0.550 | Acc: 72.415% \n",
      "[Epoch:23, Iter:2254] Loss: 0.548 | Acc: 72.519% \n",
      "[Epoch:23, Iter:2255] Loss: 0.548 | Acc: 72.618% \n",
      "[Epoch:23, Iter:2256] Loss: 0.549 | Acc: 72.607% \n",
      "[Epoch:23, Iter:2257] Loss: 0.549 | Acc: 72.596% \n",
      "[Epoch:23, Iter:2258] Loss: 0.547 | Acc: 72.621% \n",
      "[Epoch:23, Iter:2259] Loss: 0.547 | Acc: 72.542% \n",
      "[Epoch:23, Iter:2260] Loss: 0.548 | Acc: 72.433% \n",
      "[Epoch:23, Iter:2261] Loss: 0.546 | Acc: 72.590% \n",
      "[Epoch:23, Iter:2262] Loss: 0.547 | Acc: 72.484% \n",
      "[Epoch:23, Iter:2263] Loss: 0.549 | Acc: 72.190% \n",
      "[Epoch:23, Iter:2264] Loss: 0.550 | Acc: 72.125% \n",
      "[Epoch:23, Iter:2265] Loss: 0.550 | Acc: 72.092% \n",
      "[Epoch:23, Iter:2266] Loss: 0.549 | Acc: 72.242% \n",
      "[Epoch:23, Iter:2267] Loss: 0.546 | Acc: 72.418% \n",
      "[Epoch:23, Iter:2268] Loss: 0.543 | Acc: 72.588% \n",
      "[Epoch:23, Iter:2269] Loss: 0.544 | Acc: 72.580% \n",
      "[Epoch:23, Iter:2270] Loss: 0.545 | Acc: 72.514% \n",
      "[Epoch:23, Iter:2271] Loss: 0.546 | Acc: 72.451% \n",
      "[Epoch:23, Iter:2272] Loss: 0.546 | Acc: 72.333% \n",
      "[Epoch:23, Iter:2273] Loss: 0.547 | Acc: 72.247% \n",
      "[Epoch:23, Iter:2274] Loss: 0.547 | Acc: 72.378% \n",
      "[Epoch:23, Iter:2275] Loss: 0.547 | Acc: 72.400% \n",
      "[Epoch:23, Iter:2276] Loss: 0.547 | Acc: 72.447% \n",
      "[Epoch:23, Iter:2277] Loss: 0.545 | Acc: 72.545% \n",
      "[Epoch:23, Iter:2278] Loss: 0.545 | Acc: 72.538% \n",
      "[Epoch:23, Iter:2279] Loss: 0.544 | Acc: 72.557% \n",
      "[Epoch:23, Iter:2280] Loss: 0.545 | Acc: 72.625% \n",
      "[Epoch:23, Iter:2281] Loss: 0.545 | Acc: 72.617% \n",
      "[Epoch:23, Iter:2282] Loss: 0.545 | Acc: 72.756% \n",
      "[Epoch:23, Iter:2283] Loss: 0.544 | Acc: 72.795% \n",
      "[Epoch:23, Iter:2284] Loss: 0.545 | Acc: 72.762% \n",
      "[Epoch:23, Iter:2285] Loss: 0.545 | Acc: 72.800% \n",
      "[Epoch:23, Iter:2286] Loss: 0.545 | Acc: 72.860% \n",
      "[Epoch:23, Iter:2287] Loss: 0.545 | Acc: 72.828% \n",
      "[Epoch:23, Iter:2288] Loss: 0.545 | Acc: 72.841% \n",
      "[Epoch:23, Iter:2289] Loss: 0.546 | Acc: 72.809% \n",
      "[Epoch:23, Iter:2290] Loss: 0.545 | Acc: 72.844% \n",
      "[Epoch:23, Iter:2291] Loss: 0.544 | Acc: 72.879% \n",
      "[Epoch:23, Iter:2292] Loss: 0.543 | Acc: 72.913% \n",
      "[Epoch:23, Iter:2293] Loss: 0.544 | Acc: 72.860% \n",
      "[Epoch:23, Iter:2294] Loss: 0.543 | Acc: 72.894% \n",
      "[Epoch:23, Iter:2295] Loss: 0.544 | Acc: 72.863% \n",
      "[Epoch:23, Iter:2296] Loss: 0.544 | Acc: 72.938% \n",
      "[Epoch:23, Iter:2297] Loss: 0.544 | Acc: 72.845% \n",
      "[Epoch:23, Iter:2298] Loss: 0.545 | Acc: 72.755% \n",
      "[Epoch:23, Iter:2299] Loss: 0.544 | Acc: 72.747% \n",
      "[Epoch:23, Iter:2300] Loss: 0.545 | Acc: 72.720% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.000%\n",
      "Training set's accuracy (after quantization) is: 72.860%\n",
      "Test set's accuracy (before quantization) is: 73.700%\n",
      "Test set's accuracy (after quantization) is: 73.200%\n",
      "Train Loss: 0.540 | Train Acc: 73.000% | Test Loss: 0.543 | Test Acc: 73.700% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.860% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.200% \n",
      "\n",
      "Epoch: 24\n",
      "[Epoch:24, Iter:2301] Loss: 0.526 | Acc: 76.000% \n",
      "[Epoch:24, Iter:2302] Loss: 0.549 | Acc: 70.000% \n",
      "[Epoch:24, Iter:2303] Loss: 0.528 | Acc: 73.333% \n",
      "[Epoch:24, Iter:2304] Loss: 0.541 | Acc: 73.500% \n",
      "[Epoch:24, Iter:2305] Loss: 0.524 | Acc: 75.200% \n",
      "[Epoch:24, Iter:2306] Loss: 0.532 | Acc: 74.667% \n",
      "[Epoch:24, Iter:2307] Loss: 0.516 | Acc: 75.429% \n",
      "[Epoch:24, Iter:2308] Loss: 0.529 | Acc: 74.000% \n",
      "[Epoch:24, Iter:2309] Loss: 0.532 | Acc: 74.222% \n",
      "[Epoch:24, Iter:2310] Loss: 0.527 | Acc: 74.400% \n",
      "[Epoch:24, Iter:2311] Loss: 0.525 | Acc: 74.545% \n",
      "[Epoch:24, Iter:2312] Loss: 0.522 | Acc: 74.833% \n",
      "[Epoch:24, Iter:2313] Loss: 0.520 | Acc: 75.077% \n",
      "[Epoch:24, Iter:2314] Loss: 0.512 | Acc: 75.714% \n",
      "[Epoch:24, Iter:2315] Loss: 0.509 | Acc: 75.600% \n",
      "[Epoch:24, Iter:2316] Loss: 0.509 | Acc: 75.500% \n",
      "[Epoch:24, Iter:2317] Loss: 0.510 | Acc: 75.412% \n",
      "[Epoch:24, Iter:2318] Loss: 0.512 | Acc: 75.222% \n",
      "[Epoch:24, Iter:2319] Loss: 0.523 | Acc: 74.316% \n",
      "[Epoch:24, Iter:2320] Loss: 0.523 | Acc: 74.300% \n",
      "[Epoch:24, Iter:2321] Loss: 0.520 | Acc: 74.381% \n",
      "[Epoch:24, Iter:2322] Loss: 0.527 | Acc: 73.727% \n",
      "[Epoch:24, Iter:2323] Loss: 0.529 | Acc: 73.565% \n",
      "[Epoch:24, Iter:2324] Loss: 0.531 | Acc: 73.167% \n",
      "[Epoch:24, Iter:2325] Loss: 0.534 | Acc: 73.040% \n",
      "[Epoch:24, Iter:2326] Loss: 0.534 | Acc: 73.077% \n",
      "[Epoch:24, Iter:2327] Loss: 0.529 | Acc: 73.556% \n",
      "[Epoch:24, Iter:2328] Loss: 0.532 | Acc: 73.571% \n",
      "[Epoch:24, Iter:2329] Loss: 0.532 | Acc: 73.655% \n",
      "[Epoch:24, Iter:2330] Loss: 0.533 | Acc: 73.667% \n",
      "[Epoch:24, Iter:2331] Loss: 0.533 | Acc: 73.613% \n",
      "[Epoch:24, Iter:2332] Loss: 0.532 | Acc: 73.812% \n",
      "[Epoch:24, Iter:2333] Loss: 0.535 | Acc: 73.515% \n",
      "[Epoch:24, Iter:2334] Loss: 0.539 | Acc: 73.353% \n",
      "[Epoch:24, Iter:2335] Loss: 0.539 | Acc: 73.314% \n",
      "[Epoch:24, Iter:2336] Loss: 0.541 | Acc: 73.222% \n",
      "[Epoch:24, Iter:2337] Loss: 0.543 | Acc: 73.135% \n",
      "[Epoch:24, Iter:2338] Loss: 0.545 | Acc: 72.789% \n",
      "[Epoch:24, Iter:2339] Loss: 0.543 | Acc: 72.974% \n",
      "[Epoch:24, Iter:2340] Loss: 0.543 | Acc: 73.000% \n",
      "[Epoch:24, Iter:2341] Loss: 0.542 | Acc: 73.122% \n",
      "[Epoch:24, Iter:2342] Loss: 0.539 | Acc: 73.333% \n",
      "[Epoch:24, Iter:2343] Loss: 0.539 | Acc: 73.349% \n",
      "[Epoch:24, Iter:2344] Loss: 0.538 | Acc: 73.273% \n",
      "[Epoch:24, Iter:2345] Loss: 0.540 | Acc: 73.067% \n",
      "[Epoch:24, Iter:2346] Loss: 0.539 | Acc: 73.174% \n",
      "[Epoch:24, Iter:2347] Loss: 0.538 | Acc: 73.277% \n",
      "[Epoch:24, Iter:2348] Loss: 0.537 | Acc: 73.333% \n",
      "[Epoch:24, Iter:2349] Loss: 0.536 | Acc: 73.388% \n",
      "[Epoch:24, Iter:2350] Loss: 0.538 | Acc: 73.440% \n",
      "[Epoch:24, Iter:2351] Loss: 0.538 | Acc: 73.451% \n",
      "[Epoch:24, Iter:2352] Loss: 0.538 | Acc: 73.577% \n",
      "[Epoch:24, Iter:2353] Loss: 0.538 | Acc: 73.585% \n",
      "[Epoch:24, Iter:2354] Loss: 0.539 | Acc: 73.593% \n",
      "[Epoch:24, Iter:2355] Loss: 0.541 | Acc: 73.345% \n",
      "[Epoch:24, Iter:2356] Loss: 0.542 | Acc: 73.286% \n",
      "[Epoch:24, Iter:2357] Loss: 0.541 | Acc: 73.368% \n",
      "[Epoch:24, Iter:2358] Loss: 0.543 | Acc: 73.241% \n",
      "[Epoch:24, Iter:2359] Loss: 0.543 | Acc: 73.186% \n",
      "[Epoch:24, Iter:2360] Loss: 0.542 | Acc: 73.300% \n",
      "[Epoch:24, Iter:2361] Loss: 0.543 | Acc: 73.246% \n",
      "[Epoch:24, Iter:2362] Loss: 0.543 | Acc: 73.129% \n",
      "[Epoch:24, Iter:2363] Loss: 0.543 | Acc: 72.984% \n",
      "[Epoch:24, Iter:2364] Loss: 0.543 | Acc: 73.031% \n",
      "[Epoch:24, Iter:2365] Loss: 0.542 | Acc: 73.046% \n",
      "[Epoch:24, Iter:2366] Loss: 0.542 | Acc: 73.030% \n",
      "[Epoch:24, Iter:2367] Loss: 0.541 | Acc: 73.075% \n",
      "[Epoch:24, Iter:2368] Loss: 0.542 | Acc: 72.941% \n",
      "[Epoch:24, Iter:2369] Loss: 0.541 | Acc: 72.986% \n",
      "[Epoch:24, Iter:2370] Loss: 0.543 | Acc: 72.829% \n",
      "[Epoch:24, Iter:2371] Loss: 0.544 | Acc: 72.704% \n",
      "[Epoch:24, Iter:2372] Loss: 0.544 | Acc: 72.750% \n",
      "[Epoch:24, Iter:2373] Loss: 0.546 | Acc: 72.685% \n",
      "[Epoch:24, Iter:2374] Loss: 0.545 | Acc: 72.811% \n",
      "[Epoch:24, Iter:2375] Loss: 0.545 | Acc: 72.800% \n",
      "[Epoch:24, Iter:2376] Loss: 0.545 | Acc: 72.684% \n",
      "[Epoch:24, Iter:2377] Loss: 0.545 | Acc: 72.701% \n",
      "[Epoch:24, Iter:2378] Loss: 0.545 | Acc: 72.667% \n",
      "[Epoch:24, Iter:2379] Loss: 0.544 | Acc: 72.785% \n",
      "[Epoch:24, Iter:2380] Loss: 0.544 | Acc: 72.800% \n",
      "[Epoch:24, Iter:2381] Loss: 0.544 | Acc: 72.765% \n",
      "[Epoch:24, Iter:2382] Loss: 0.544 | Acc: 72.805% \n",
      "[Epoch:24, Iter:2383] Loss: 0.544 | Acc: 72.747% \n",
      "[Epoch:24, Iter:2384] Loss: 0.545 | Acc: 72.714% \n",
      "[Epoch:24, Iter:2385] Loss: 0.544 | Acc: 72.776% \n",
      "[Epoch:24, Iter:2386] Loss: 0.544 | Acc: 72.837% \n",
      "[Epoch:24, Iter:2387] Loss: 0.544 | Acc: 72.874% \n",
      "[Epoch:24, Iter:2388] Loss: 0.544 | Acc: 72.886% \n",
      "[Epoch:24, Iter:2389] Loss: 0.544 | Acc: 72.876% \n",
      "[Epoch:24, Iter:2390] Loss: 0.544 | Acc: 72.867% \n",
      "[Epoch:24, Iter:2391] Loss: 0.545 | Acc: 72.725% \n",
      "[Epoch:24, Iter:2392] Loss: 0.546 | Acc: 72.696% \n",
      "[Epoch:24, Iter:2393] Loss: 0.545 | Acc: 72.753% \n",
      "[Epoch:24, Iter:2394] Loss: 0.545 | Acc: 72.745% \n",
      "[Epoch:24, Iter:2395] Loss: 0.546 | Acc: 72.695% \n",
      "[Epoch:24, Iter:2396] Loss: 0.545 | Acc: 72.729% \n",
      "[Epoch:24, Iter:2397] Loss: 0.546 | Acc: 72.639% \n",
      "[Epoch:24, Iter:2398] Loss: 0.546 | Acc: 72.592% \n",
      "[Epoch:24, Iter:2399] Loss: 0.546 | Acc: 72.566% \n",
      "[Epoch:24, Iter:2400] Loss: 0.546 | Acc: 72.580% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.120%\n",
      "Training set's accuracy (after quantization) is: 72.940%\n",
      "Test set's accuracy (before quantization) is: 73.400%\n",
      "Test set's accuracy (after quantization) is: 73.300%\n",
      "Train Loss: 0.540 | Train Acc: 73.120% | Test Loss: 0.543 | Test Acc: 73.400% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.940% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.300% \n",
      "\n",
      "Epoch: 25\n",
      "[Epoch:25, Iter:2401] Loss: 0.601 | Acc: 72.000% \n",
      "[Epoch:25, Iter:2402] Loss: 0.600 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2403] Loss: 0.597 | Acc: 67.333% \n",
      "[Epoch:25, Iter:2404] Loss: 0.597 | Acc: 68.000% \n",
      "[Epoch:25, Iter:2405] Loss: 0.599 | Acc: 66.800% \n",
      "[Epoch:25, Iter:2406] Loss: 0.593 | Acc: 68.667% \n",
      "[Epoch:25, Iter:2407] Loss: 0.588 | Acc: 68.286% \n",
      "[Epoch:25, Iter:2408] Loss: 0.584 | Acc: 68.500% \n",
      "[Epoch:25, Iter:2409] Loss: 0.575 | Acc: 69.111% \n",
      "[Epoch:25, Iter:2410] Loss: 0.566 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2411] Loss: 0.569 | Acc: 69.818% \n",
      "[Epoch:25, Iter:2412] Loss: 0.570 | Acc: 69.333% \n",
      "[Epoch:25, Iter:2413] Loss: 0.570 | Acc: 69.846% \n",
      "[Epoch:25, Iter:2414] Loss: 0.566 | Acc: 70.286% \n",
      "[Epoch:25, Iter:2415] Loss: 0.569 | Acc: 70.133% \n",
      "[Epoch:25, Iter:2416] Loss: 0.564 | Acc: 70.250% \n",
      "[Epoch:25, Iter:2417] Loss: 0.560 | Acc: 70.471% \n",
      "[Epoch:25, Iter:2418] Loss: 0.563 | Acc: 69.889% \n",
      "[Epoch:25, Iter:2419] Loss: 0.559 | Acc: 70.211% \n",
      "[Epoch:25, Iter:2420] Loss: 0.558 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2421] Loss: 0.555 | Acc: 70.190% \n",
      "[Epoch:25, Iter:2422] Loss: 0.553 | Acc: 70.545% \n",
      "[Epoch:25, Iter:2423] Loss: 0.553 | Acc: 70.522% \n",
      "[Epoch:25, Iter:2424] Loss: 0.559 | Acc: 70.333% \n",
      "[Epoch:25, Iter:2425] Loss: 0.558 | Acc: 70.160% \n",
      "[Epoch:25, Iter:2426] Loss: 0.561 | Acc: 70.077% \n",
      "[Epoch:25, Iter:2427] Loss: 0.562 | Acc: 69.852% \n",
      "[Epoch:25, Iter:2428] Loss: 0.562 | Acc: 69.571% \n",
      "[Epoch:25, Iter:2429] Loss: 0.558 | Acc: 70.138% \n",
      "[Epoch:25, Iter:2430] Loss: 0.562 | Acc: 69.867% \n",
      "[Epoch:25, Iter:2431] Loss: 0.562 | Acc: 69.935% \n",
      "[Epoch:25, Iter:2432] Loss: 0.563 | Acc: 69.812% \n",
      "[Epoch:25, Iter:2433] Loss: 0.563 | Acc: 69.879% \n",
      "[Epoch:25, Iter:2434] Loss: 0.564 | Acc: 69.824% \n",
      "[Epoch:25, Iter:2435] Loss: 0.561 | Acc: 70.000% \n",
      "[Epoch:25, Iter:2436] Loss: 0.560 | Acc: 70.111% \n",
      "[Epoch:25, Iter:2437] Loss: 0.558 | Acc: 70.270% \n",
      "[Epoch:25, Iter:2438] Loss: 0.556 | Acc: 70.632% \n",
      "[Epoch:25, Iter:2439] Loss: 0.556 | Acc: 70.615% \n",
      "[Epoch:25, Iter:2440] Loss: 0.556 | Acc: 70.700% \n",
      "[Epoch:25, Iter:2441] Loss: 0.555 | Acc: 70.878% \n",
      "[Epoch:25, Iter:2442] Loss: 0.554 | Acc: 70.952% \n",
      "[Epoch:25, Iter:2443] Loss: 0.555 | Acc: 70.837% \n",
      "[Epoch:25, Iter:2444] Loss: 0.554 | Acc: 70.864% \n",
      "[Epoch:25, Iter:2445] Loss: 0.553 | Acc: 71.111% \n",
      "[Epoch:25, Iter:2446] Loss: 0.553 | Acc: 71.217% \n",
      "[Epoch:25, Iter:2447] Loss: 0.554 | Acc: 71.319% \n",
      "[Epoch:25, Iter:2448] Loss: 0.553 | Acc: 71.167% \n",
      "[Epoch:25, Iter:2449] Loss: 0.552 | Acc: 71.265% \n",
      "[Epoch:25, Iter:2450] Loss: 0.551 | Acc: 71.400% \n",
      "[Epoch:25, Iter:2451] Loss: 0.551 | Acc: 71.451% \n",
      "[Epoch:25, Iter:2452] Loss: 0.550 | Acc: 71.538% \n",
      "[Epoch:25, Iter:2453] Loss: 0.550 | Acc: 71.736% \n",
      "[Epoch:25, Iter:2454] Loss: 0.553 | Acc: 71.593% \n",
      "[Epoch:25, Iter:2455] Loss: 0.552 | Acc: 71.600% \n",
      "[Epoch:25, Iter:2456] Loss: 0.553 | Acc: 71.429% \n",
      "[Epoch:25, Iter:2457] Loss: 0.552 | Acc: 71.509% \n",
      "[Epoch:25, Iter:2458] Loss: 0.552 | Acc: 71.517% \n",
      "[Epoch:25, Iter:2459] Loss: 0.550 | Acc: 71.763% \n",
      "[Epoch:25, Iter:2460] Loss: 0.552 | Acc: 71.667% \n",
      "[Epoch:25, Iter:2461] Loss: 0.551 | Acc: 71.902% \n",
      "[Epoch:25, Iter:2462] Loss: 0.553 | Acc: 71.742% \n",
      "[Epoch:25, Iter:2463] Loss: 0.553 | Acc: 71.810% \n",
      "[Epoch:25, Iter:2464] Loss: 0.552 | Acc: 71.844% \n",
      "[Epoch:25, Iter:2465] Loss: 0.551 | Acc: 71.908% \n",
      "[Epoch:25, Iter:2466] Loss: 0.550 | Acc: 72.000% \n",
      "[Epoch:25, Iter:2467] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:25, Iter:2468] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:25, Iter:2469] Loss: 0.549 | Acc: 72.000% \n",
      "[Epoch:25, Iter:2470] Loss: 0.550 | Acc: 71.914% \n",
      "[Epoch:25, Iter:2471] Loss: 0.551 | Acc: 71.859% \n",
      "[Epoch:25, Iter:2472] Loss: 0.550 | Acc: 71.944% \n",
      "[Epoch:25, Iter:2473] Loss: 0.550 | Acc: 72.027% \n",
      "[Epoch:25, Iter:2474] Loss: 0.550 | Acc: 71.973% \n",
      "[Epoch:25, Iter:2475] Loss: 0.551 | Acc: 71.947% \n",
      "[Epoch:25, Iter:2476] Loss: 0.550 | Acc: 72.026% \n",
      "[Epoch:25, Iter:2477] Loss: 0.549 | Acc: 72.130% \n",
      "[Epoch:25, Iter:2478] Loss: 0.548 | Acc: 72.154% \n",
      "[Epoch:25, Iter:2479] Loss: 0.549 | Acc: 72.025% \n",
      "[Epoch:25, Iter:2480] Loss: 0.549 | Acc: 71.925% \n",
      "[Epoch:25, Iter:2481] Loss: 0.549 | Acc: 71.901% \n",
      "[Epoch:25, Iter:2482] Loss: 0.549 | Acc: 71.927% \n",
      "[Epoch:25, Iter:2483] Loss: 0.549 | Acc: 71.976% \n",
      "[Epoch:25, Iter:2484] Loss: 0.549 | Acc: 71.976% \n",
      "[Epoch:25, Iter:2485] Loss: 0.549 | Acc: 71.906% \n",
      "[Epoch:25, Iter:2486] Loss: 0.550 | Acc: 71.884% \n",
      "[Epoch:25, Iter:2487] Loss: 0.550 | Acc: 71.908% \n",
      "[Epoch:25, Iter:2488] Loss: 0.550 | Acc: 71.977% \n",
      "[Epoch:25, Iter:2489] Loss: 0.548 | Acc: 72.112% \n",
      "[Epoch:25, Iter:2490] Loss: 0.550 | Acc: 72.089% \n",
      "[Epoch:25, Iter:2491] Loss: 0.549 | Acc: 72.066% \n",
      "[Epoch:25, Iter:2492] Loss: 0.549 | Acc: 72.043% \n",
      "[Epoch:25, Iter:2493] Loss: 0.549 | Acc: 72.065% \n",
      "[Epoch:25, Iter:2494] Loss: 0.548 | Acc: 72.106% \n",
      "[Epoch:25, Iter:2495] Loss: 0.548 | Acc: 72.084% \n",
      "[Epoch:25, Iter:2496] Loss: 0.547 | Acc: 72.167% \n",
      "[Epoch:25, Iter:2497] Loss: 0.547 | Acc: 72.206% \n",
      "[Epoch:25, Iter:2498] Loss: 0.546 | Acc: 72.245% \n",
      "[Epoch:25, Iter:2499] Loss: 0.546 | Acc: 72.283% \n",
      "[Epoch:25, Iter:2500] Loss: 0.546 | Acc: 72.340% \n",
      "Waiting Test...\n",
      "Training set's accuracy (before quantization) is: 73.020%\n",
      "Training set's accuracy (after quantization) is: 72.900%\n",
      "Test set's accuracy (before quantization) is: 73.200%\n",
      "Test set's accuracy (after quantization) is: 73.300%\n",
      "Train Loss: 0.540 | Train Acc: 73.020% | Test Loss: 0.542 | Test Acc: 73.200% \n",
      "Quantized Train Loss: 0.541 | Quantized Train Acc: 72.900% | Quantized Test Loss: 0.543 | Quantized Test Acc: 73.300% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945ebebfccb64cd4abb22ddeb7889c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.135 MB uploaded\\r'), FloatProgress(value=0.00983987330635228, max=1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td></td></tr><tr><td>batch_gradient</td><td></td></tr><tr><td>quantized_accuracy</td><td></td></tr><tr><td>quantized_test_accuracy</td><td></td></tr><tr><td>quantized_test_loss</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>weight_distance</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>73.02</td></tr><tr><td>batch_gradient</td><td>0.00731</td></tr><tr><td>quantized_accuracy</td><td>72.9</td></tr><tr><td>quantized_test_accuracy</td><td>73.3</td></tr><tr><td>quantized_test_loss</td><td>0.54311</td></tr><tr><td>test_accuracy</td><td>73.2</td></tr><tr><td>test_loss</td><td>0.54236</td></tr><tr><td>weight_distance</td><td>0.12398</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ProxQuant</strong> at: <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/jf7xtj0v' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary/runs/jf7xtj0v</a><br/> View project at: <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/LogisticRegression_binary</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250711_035508-jf7xtj0v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ProxQuant\n",
    "net, optimizer = init(project_name=\"LogisticRegression_binary\", opt_name=\"ProxQuant\", batch_size=BATCH_SIZE, architecture=\"MLP\", dataset_name=\"LogisticRegression\", lr=LR)\n",
    "\n",
    "model_copy = copy.deepcopy(net)\n",
    "\n",
    "lr_decay_epochs = [30]\n",
    "lr=LR/10\n",
    "for decay_epoch in lr_decay_epochs:\n",
    "    if pre_epoch > decay_epoch:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] *= 0.5\n",
    "            lr*=0.5\n",
    "\n",
    "# Train\n",
    "it=-1\n",
    "total_it=(EPOCH-ANNEAL_EPOCH_PQ)*len(trainloader)\n",
    "\n",
    "for epoch in range(pre_epoch, EPOCH):\n",
    "    print(\"\\nEpoch: %d\" % (epoch + 1))\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    if epoch in lr_decay_epochs:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] *= 0.5\n",
    "            lr*=0.5\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # prepare dataset\n",
    "        if epoch < ANNEAL_EPOCH_PQ:\n",
    "            epsilon=0\n",
    "        else:\n",
    "            it+=1\n",
    "            epsilon = reg_lambda*it/total_it\n",
    "        net.train()\n",
    "        length = len(trainloader)\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # forward & backward\n",
    "        outputs = net(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            for name, param in net.named_parameters():\n",
    "                if not name.endswith(\".bias\"):\n",
    "                    # Prox Step\n",
    "                    param.data=(param.data+epsilon*torch.sign(param.data))/(1+epsilon)\n",
    "        optimizer.zero_grad()\n",
    "        sum_loss += loss.item()\n",
    "        predicted = torch.where(outputs.data>0.5, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        print(\n",
    "            \"[Epoch:%d, Iter:%d] Loss: %.03f | Acc: %.3f%% \"\n",
    "            % (\n",
    "                epoch + 1,\n",
    "                (i + 1 + (epoch) * length),\n",
    "                sum_loss / (i + 1),\n",
    "                100.0 * correct / total,\n",
    "            )\n",
    "        )\n",
    "    print(\"Waiting Test...\")\n",
    "    # calculate weight_dist and batch_gradient\n",
    "    model_copy=copy.deepcopy(net)\n",
    "    weight_dist=torch.norm(w_star-model_copy.fc1.weight.cpu()).item()\n",
    "    model_copy.to(device)\n",
    "    model_copy.train()\n",
    "    weights = [p for name, p in net.named_parameters() if 'bias' not in name]\n",
    "    bias = [p for name, p in net.named_parameters() if 'bias' in name]\n",
    "    parameters = [{\"params\": weights, \"tag\": \"weights\"}, {\"params\": bias, \"tag\": \"bias\"}]\n",
    "    optimizerx = optim.SGD(parameters, lr=LR)\n",
    "    grad=torch.zeros(model_copy.fc1.weight.shape, device=device)\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_copy(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        grad += model_copy.fc1.weight.grad\n",
    "        optimizerx.step()\n",
    "        model_copy.zero_grad()\n",
    "    grad = grad / len(trainloader)\n",
    "    batch_gradient = torch.norm(grad).item()\n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_acc = net.evaluate(trainloader, criterion, device, eval=False, qt=False)\n",
    "        qtrain_loss, qtrain_acc = net.evaluate(trainloader, criterion, device, eval=False, qt=True)\n",
    "        test_loss, test_acc=net.evaluate(testloader, criterion, device, eval=True, qt=False)\n",
    "        qtest_loss, qtest_acc=net.evaluate(testloader, criterion, device, eval=True, qt=True)\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"test_loss\": test_loss,\n",
    "                \"quantized_test_loss\": qtest_loss,\n",
    "                \"accuracy\": train_acc,\n",
    "                \"quantized_accuracy\": qtrain_acc,\n",
    "                \"test_accuracy\": test_acc,\n",
    "                \"quantized_test_accuracy\": qtest_acc,\n",
    "                \"weight_distance\": weight_dist,\n",
    "                \"batch_gradient\": batch_gradient,\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            \"Train Loss: %.03f | Train Acc: %.3f%% | Test Loss: %.03f | Test Acc: %.3f%% \"\n",
    "            % (\n",
    "                train_loss,\n",
    "                train_acc,\n",
    "                test_loss,\n",
    "                test_acc,\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Quantized Train Loss: %.03f | Quantized Train Acc: %.3f%% | Quantized Test Loss: %.03f | Quantized Test Acc: %.3f%% \"\n",
    "            % (\n",
    "                qtrain_loss,\n",
    "                qtrain_acc,\n",
    "                qtest_loss,\n",
    "                qtest_acc,\n",
    "            )\n",
    "        )\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0510, -1.0311,  0.9678, -1.0093, -0.9910,  1.0555, -1.0553, -1.0067,\n",
      "          0.9820,  0.9360]], device='cuda:0', requires_grad=True)\n",
      "tensor([-1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "print(model_copy.fc1.weight)\n",
    "print(w_star)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
